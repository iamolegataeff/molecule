// molequla.go
// A dependency-free*, single-file, goroutine-powered, continually-learning GPT organism.
// Same architecture as C, JS, Rust. JSON checkpoint format compatible across implementations.
//
// * "dependency-free" = no PyTorch, no numpy, no C. One Go dep: modernc.org/sqlite (pure Go).
//
// In the beginning there was nonames.txt.
// And it was good. Mostly. Sometimes cursed.

package main

import (
	"bufio"
	"database/sql"
	"encoding/json"
	"fmt"
	"math"
	"math/rand"
	"os"
	"os/exec"
	"os/signal"
	"path/filepath"
	"sort"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"syscall"
	"time"
	"unicode"

	_ "modernc.org/sqlite"
)

// gradEnabled controls whether Vec/Scalar ops build backward graph (autograd).
// It's a global atomic, but after v2 fixes all forward passes are serialized by model.mu,
// so no two goroutines can toggle it simultaneously. The atomic prevents torn reads.
var gradEnabled atomic.Bool

// notorchSeed moved to GPT struct (per-model, protected by model.mu).

func init() { gradEnabled.Store(true) }

// ============================================================
// 0) CONFIG — bend reality here (carefully, mortals)
// ============================================================

type Config struct {
	// data
	CorpusPath     string  `json:"corpus_path"`
	DBPath         string  `json:"db_path"`
	CkptPath       string  `json:"ckpt_path"`
	MaxCorpusLines int     `json:"max_corpus_lines"`
	MaxLineChars   int     `json:"max_line_chars"`
	MinNewChars    int     `json:"min_new_chars_to_train"`

	// model
	TieEmbeddings bool `json:"tie_embeddings"`
	NLayer        int  `json:"n_layer"`
	NEmbd         int  `json:"n_embd"`
	NHead         int  `json:"n_head"`
	BlockSize     int  `json:"block_size"`

	// ontogenesis — growth stages (corpus_chars, n_embd, n_layer, n_head)
	GrowthStages          [][4]int `json:"growth_stages"`
	FreezeAfterGrowthSteps int     `json:"freeze_after_growth_steps"`
	PostGrowthLRScale      float64 `json:"post_growth_lr_scale"` // LR multiplier during freeze period (prevents delta overfit to noise)

	// training
	WarmupSteps         int     `json:"warmup_steps"`
	MicroSteps          int     `json:"micro_steps"`
	LearningRate        float64 `json:"learning_rate"`
	Beta1               float64 `json:"beta1"`
	Beta2               float64 `json:"beta2"`
	EpsAdam             float64 `json:"eps_adam"`
	GradClip            float64 `json:"grad_clip"`
	FreezeBaseAfterWarm bool    `json:"freeze_base_after_warmup"`
	BatchSize           int     `json:"batch_size"`

	// cosine LR schedule
	LRMin              float64 `json:"lr_min"`
	MaxTotalSteps      int     `json:"max_total_steps"`
	CosineWarmupSteps  int     `json:"cosine_warmup_steps"`

	// gradient accumulation
	AccumSteps int `json:"accum_steps"`

	// deltas
	DeltaRank      int     `json:"delta_rank"`
	MaxDeltaModules int    `json:"max_delta_modules"`
	DeltaGrowProb  float64 `json:"delta_grow_prob"`

	// generation
	Temperature    float64 `json:"temperature"`
	TopK           int     `json:"top_k"`
	TopP           float64 `json:"top_p"`
	MinP           float64 `json:"min_p"`     // GPT-3/4 style: filter tokens below min_p * max_prob
	TypicalP       float64 `json:"typical_p"` // Typical sampling: prefer tokens with typical information content
	MaxGenTokens   int     `json:"max_gen_tokens"`
	MinGenTokens   int     `json:"min_gen_tokens"`
	RepetitionGuard int     `json:"repetition_guard"`
	FreqPenalty     float64 `json:"freq_penalty"`      // penalize logits by count * freq_penalty
	PresencePenalty float64 `json:"presence_penalty"`   // flat penalty for any token that appeared

	// tokenizer evolution
	EnableBPEAfterChars  int `json:"enable_bpe_after_chars"`
	BPENumMerges         int `json:"bpe_num_merges"`
	BPERetrainEveryChars int `json:"bpe_retrain_every_chars"`

	// async
	TrainTickSeconds float64 `json:"train_tick_seconds"`

	// hybrid attention heads: "content", "rrpram", or "hybrid"
	HeadTypes        []string `json:"head_types"`
	HybridAlphaInit  float64  `json:"hybrid_alpha_init"`

	// gamma (personality fingerprint)
	GammaSparsityThreshold float64 `json:"gamma_sparsity_threshold"`

	// noise immune system
	NoiseDriftThreshold float64 `json:"noise_drift_threshold"`
	GammaMinMagnitude   float64 `json:"gamma_min_magnitude"` // skip immune check when gamma direction is near-zero

	// entropy-adaptive temperature
	EntropyLow       float64 `json:"entropy_low"`
	EntropyHigh      float64 `json:"entropy_high"`
	EntropyTempBoost float64 `json:"entropy_temp_boost"`
	EntropyTempFocus float64 `json:"entropy_temp_focus"`

	// corpus field
	CorpusGenMaxTokens   int     `json:"corpus_gen_max_tokens"`
	CorpusFadeK          float64 `json:"corpus_fade_k"`          // sigmoid steepness for corpus→model transition
	CorpusFadeThreshold  float64 `json:"corpus_fade_threshold"`  // entropy at which blend is 50/50
	CooccurWindowSize    int     `json:"cooccur_window_size"`    // co-occurrence proximity window (Stanley-style)
	UserBoostStrength    float64 `json:"user_boost_strength"`    // how strongly user's recent words are boosted
	UserBoostDecay       float64 `json:"user_boost_decay"`       // per-generation decay of user word boost

	// quantum buffer
	QBMinBytes        int     `json:"qb_min_bytes"`
	QBMinNovelty      float64 `json:"qb_min_novelty"`
	QBCooldownSeconds float64 `json:"qb_cooldown_seconds"`

	// syntropy tracker (mathematical self-awareness)
	SyntropyWindow         int     `json:"syntropy_window"`           // rolling window for syntropy trend
	FieldDeviationCeiling  float64 `json:"field_deviation_ceiling"`   // KL divergence above this = drifted too far
	FieldDeviationFloor    float64 `json:"field_deviation_floor"`     // below this = not learning, just parroting
	SyntropyLRBoost        float64 `json:"syntropy_lr_boost"`         // boost LR when syntropy is rising
	SyntropyLRDampen       float64 `json:"syntropy_lr_dampen"`        // dampen LR when syntropy is falling
	SyntropyDeltaGrowBoost float64 `json:"syntropy_delta_grow_boost"` // higher delta grow prob when syntropy is good

	// consciousness: per-token dissonance feedback
	DissonanceEMAAlpha      float64 `json:"dissonance_ema_alpha"`       // EMA smoothing for entropy within generation
	DissonanceSpikeK        float64 `json:"dissonance_spike_k"`         // temp multiplier when entropy spikes
	DissonanceDropK         float64 `json:"dissonance_drop_k"`          // temp multiplier when entropy drops
	DissonanceSpikeThreshold float64 `json:"dissonance_spike_threshold"` // entropy/EMA ratio triggering spike
	DissonanceDropThreshold  float64 `json:"dissonance_drop_threshold"`  // entropy/EMA ratio triggering drop

	// consciousness: pattern breaking (anti-field generation)
	AntiFieldProb    float64 `json:"anti_field_prob"`     // probability of pure-model token (bypass corpus)
	AntiFieldMinStep int     `json:"anti_field_min_step"` // don't anti-field before this many tokens


	// consciousness: conscience (self-editing)
	ConscienceWindow   int     `json:"conscience_window"`   // rolling window for generation entropy trend
	ConscienceDecay    float64 `json:"conscience_decay"`    // deltaAlphaScale reduction factor
	ConscienceRecovery float64 `json:"conscience_recovery"` // deltaAlphaScale recovery factor
	ConscienceFloor    float64 `json:"conscience_floor"`    // minimum deltaAlphaScale

	// notorch: gradient-free delta training (ported from AML C)
	NotorchLR          float64 `json:"notorch_lr"`          // learning rate for notorch step
	NotorchDecay       float64 `json:"notorch_decay"`       // adaptive weight decay
	CoordinateWarmup   bool    `json:"coordinate_warmup"`   // true = warmup through training queue (for Mac 8GB)
}

var CFG = Config{
	CorpusPath:           "nonames.txt",
	DBPath:               "memory.sqlite3",
	CkptPath:             "molequla_ckpt.json",
	MaxCorpusLines:       8000,
	MaxLineChars:         240,
	MinNewChars:          480,
	TieEmbeddings:        true,
	NLayer:               1,
	NEmbd:                16,
	NHead:                1,
	BlockSize:            96,

	GrowthStages: [][4]int{
		{0, 16, 1, 1},       // embryo: ~10K params
		{20000, 32, 1, 2},   // infant: ~28K params
		{50000, 64, 2, 4},   // child: ~154K params
		{200000, 128, 4, 4}, // adolescent: ~1.1M params
		{350000, 224, 5, 8}, // teen: ~4.1M params
		{500000, 320, 6, 8}, // adult: ~10M params
	},
	FreezeAfterGrowthSteps: 500,
	PostGrowthLRScale:      0.3,
	WarmupSteps:          400,
	MicroSteps:           32,
	LearningRate:         0.01,
	Beta1:                0.9,
	Beta2:                0.99,
	EpsAdam:              1e-8,
	GradClip:             1.0,
	FreezeBaseAfterWarm:  true,
	BatchSize:            4,
	LRMin:                0.001,
	MaxTotalSteps:        50000,
	CosineWarmupSteps:    200,
	AccumSteps:           1,
	DeltaRank:            8,
	MaxDeltaModules:      12,
	DeltaGrowProb:        0.08,
	Temperature:          0.85,
	TopK:                 40,
	TopP:                 0.92,
	MinP:                 0.06,
	TypicalP:             0.95,
	MaxGenTokens:         180,
	MinGenTokens:         16,
	RepetitionGuard:      4,
	FreqPenalty:          0.1,
	PresencePenalty:      0.1,
	EnableBPEAfterChars:  20000,
	BPENumMerges:         384,
	BPERetrainEveryChars: 4000,
	TrainTickSeconds:     0.25,

	HeadTypes:              []string{"content"},
	HybridAlphaInit:        0.5,
	GammaSparsityThreshold: 0.01,
	NoiseDriftThreshold:    -0.1,
	GammaMinMagnitude:      1e-6,
	EntropyLow:             0.5,
	EntropyHigh:            1.5,
	EntropyTempBoost:       1.2,
	EntropyTempFocus:       0.8,
	CorpusGenMaxTokens:     120,
	CorpusFadeK:            3.0,
	CorpusFadeThreshold:    1.5,
	CooccurWindowSize:      5,
	UserBoostStrength:      0.3,
	UserBoostDecay:         0.7,
	QBMinBytes:             1024,
	QBMinNovelty:           0.15,
	QBCooldownSeconds:      60.0,

	SyntropyWindow:         8,
	FieldDeviationCeiling:  12.0,
	FieldDeviationFloor:    0.1,
	SyntropyLRBoost:        1.3,
	SyntropyLRDampen:       0.6,
	SyntropyDeltaGrowBoost: 0.15,

	// consciousness defaults
	DissonanceEMAAlpha:       0.3,
	DissonanceSpikeK:         0.8,
	DissonanceDropK:          1.2,
	DissonanceSpikeThreshold: 1.5,
	DissonanceDropThreshold:  0.5,
	AntiFieldProb:            0.05,
	AntiFieldMinStep:         8,
	ConscienceWindow:         8,
	ConscienceDecay:          0.95,
	ConscienceRecovery:       1.005,
	ConscienceFloor:          0.3,

	NotorchLR:    0.01,
	NotorchDecay: 0.999,
}

// headTypesForNHead returns the head type list for a given number of heads.
// Embryo: 1 head = 1 content. Growth adds hybrid heads.
func headTypesForNHead(n int) []string {
	if n <= 1 {
		return []string{"content"}
	}
	if n == 2 {
		return []string{"content", "hybrid"}
	}
	half := (n + 1) / 2 // ceiling: majority content
	types := make([]string, n)
	for i := 0; i < half; i++ {
		types[i] = "content"
	}
	for i := half; i < n; i++ {
		types[i] = "hybrid"
	}
	return types
}

// ============================================================
// 1) AUTOGRAD — vectors, not scalar confetti
// ============================================================

// Node is anything in the autograd compute graph.
type Node interface {
	getChildren() []Node
	doBackward()
}

// Vec is a differentiable vector. One object = one embedding / hidden state.
type Vec struct {
	Data     []float64
	Grad     []float64
	children []Node
	backFn   func()
}

func NewVec(data []float64) *Vec {
	var g []float64
	if gradEnabled.Load() {
		g = make([]float64, len(data))
	}
	return &Vec{Data: data, Grad: g}
}

func NewVecZero(n int) *Vec {
	return NewVec(make([]float64, n))
}

// NewVecWithGrad always allocates grad (for parameter tensors that need it regardless)
func NewVecWithGrad(data []float64) *Vec {
	g := make([]float64, len(data))
	return &Vec{Data: data, Grad: g}
}

func (v *Vec) getChildren() []Node { return v.children }
func (v *Vec) doBackward() {
	if v.backFn != nil {
		v.backFn()
	}
}

// Add returns a new Vec = self + other (element-wise).
func (v *Vec) Add(other *Vec) *Vec {
	n := len(v.Data)
	d := make([]float64, n)
	for i := 0; i < n; i++ {
		d[i] = v.Data[i] + other.Data[i]
	}
	out := NewVec(d)
	if gradEnabled.Load() {
		out.children = []Node{v, other}
		out.backFn = func() {
			for i := 0; i < n; i++ {
				v.Grad[i] += out.Grad[i]
				other.Grad[i] += out.Grad[i]
			}
		}
	}
	return out
}

// Sub returns a new Vec = self - other.
func (v *Vec) Sub(other *Vec) *Vec {
	n := len(v.Data)
	d := make([]float64, n)
	for i := 0; i < n; i++ {
		d[i] = v.Data[i] - other.Data[i]
	}
	out := NewVec(d)
	if gradEnabled.Load() {
		out.children = []Node{v, other}
		out.backFn = func() {
			for i := 0; i < n; i++ {
				v.Grad[i] += out.Grad[i]
				other.Grad[i] -= out.Grad[i]
			}
		}
	}
	return out
}

// Neg returns -self.
func (v *Vec) Neg() *Vec {
	n := len(v.Data)
	d := make([]float64, n)
	for i := 0; i < n; i++ {
		d[i] = -v.Data[i]
	}
	out := NewVec(d)
	if gradEnabled.Load() {
		out.children = []Node{v}
		out.backFn = func() {
			for i := 0; i < n; i++ {
				v.Grad[i] -= out.Grad[i]
			}
		}
	}
	return out
}

// MulVec returns element-wise product self * other.
func (v *Vec) MulVec(other *Vec) *Vec {
	n := len(v.Data)
	d := make([]float64, n)
	for i := 0; i < n; i++ {
		d[i] = v.Data[i] * other.Data[i]
	}
	out := NewVec(d)
	if gradEnabled.Load() {
		out.children = []Node{v, other}
		vData := v.Data
		oData := other.Data
		out.backFn = func() {
			for i := 0; i < n; i++ {
				v.Grad[i] += oData[i] * out.Grad[i]
				other.Grad[i] += vData[i] * out.Grad[i]
			}
		}
	}
	return out
}

// Scale returns self * scalar.
func (v *Vec) Scale(s float64) *Vec {
	n := len(v.Data)
	d := make([]float64, n)
	for i := 0; i < n; i++ {
		d[i] = v.Data[i] * s
	}
	out := NewVec(d)
	if gradEnabled.Load() {
		out.children = []Node{v}
		out.backFn = func() {
			for i := 0; i < n; i++ {
				v.Grad[i] += s * out.Grad[i]
			}
		}
	}
	return out
}

// AddScalar returns self + s (broadcast).
func (v *Vec) AddScalar(s float64) *Vec {
	n := len(v.Data)
	d := make([]float64, n)
	for i := 0; i < n; i++ {
		d[i] = v.Data[i] + s
	}
	out := NewVec(d)
	if gradEnabled.Load() {
		out.children = []Node{v}
		out.backFn = func() {
			for i := 0; i < n; i++ {
				v.Grad[i] += out.Grad[i]
			}
		}
	}
	return out
}

// ReLU applies max(0, x) element-wise.
func (v *Vec) ReLU() *Vec {
	n := len(v.Data)
	d := make([]float64, n)
	for i := 0; i < n; i++ {
		if v.Data[i] > 0 {
			d[i] = v.Data[i]
		}
	}
	out := NewVec(d)
	if gradEnabled.Load() {
		out.children = []Node{v}
		vData := v.Data
		out.backFn = func() {
			for i := 0; i < n; i++ {
				if vData[i] > 0 {
					v.Grad[i] += out.Grad[i]
				}
			}
		}
	}
	return out
}

// SiLU applies silu(x) = x * sigmoid(x) element-wise (for real SwiGLU).
func (v *Vec) SiLU() *Vec {
	n := len(v.Data)
	sig := make([]float64, n)
	d := make([]float64, n)
	for i := 0; i < n; i++ {
		sig[i] = 1.0 / (1.0 + math.Exp(-v.Data[i]))
		d[i] = v.Data[i] * sig[i]
	}
	out := NewVec(d)
	if gradEnabled.Load() {
		out.children = []Node{v}
		vData := v.Data
		out.backFn = func() {
			for i := 0; i < n; i++ {
				// d/dx[x * sigmoid(x)] = sigmoid(x) * (1 + x * (1 - sigmoid(x)))
				v.Grad[i] += (sig[i] * (1.0 + vData[i]*(1.0-sig[i]))) * out.Grad[i]
			}
		}
	}
	return out
}

// Dot returns the scalar dot product of two vectors.
func (v *Vec) Dot(other *Vec) *Scalar {
	n := len(v.Data)
	val := 0.0
	for i := 0; i < n; i++ {
		val += v.Data[i] * other.Data[i]
	}
	out := &Scalar{Data: val}
	if gradEnabled.Load() {
		out.children = []Node{v, other}
		vData := v.Data
		oData := other.Data
		out.backFn = func() {
			for i := 0; i < n; i++ {
				v.Grad[i] += oData[i] * out.Grad
				other.Grad[i] += vData[i] * out.Grad
			}
		}
	}
	return out
}

// MeanSq returns mean of squared elements (scalar).
func (v *Vec) MeanSq() *Scalar {
	n := len(v.Data)
	nf := float64(n)
	val := 0.0
	for i := 0; i < n; i++ {
		val += v.Data[i] * v.Data[i]
	}
	val /= nf
	out := &Scalar{Data: val}
	if gradEnabled.Load() {
		out.children = []Node{v}
		vData := v.Data
		out.backFn = func() {
			for i := 0; i < n; i++ {
				v.Grad[i] += (2.0 * vData[i] / nf) * out.Grad
			}
		}
	}
	return out
}

// Element extracts a single element as a Scalar with gradient flow.
// And lo, one number shall be plucked from the vector, and gradients shall follow.
func (v *Vec) Element(idx int) *Scalar {
	out := &Scalar{Data: v.Data[idx]}
	if gradEnabled.Load() {
		out.children = []Node{v}
		out.backFn = func() {
			v.Grad[idx] += out.Grad
		}
	}
	return out
}

// Slice extracts [start:end) from the vector.
func (v *Vec) Slice(start, end int) *Vec {
	d := make([]float64, end-start)
	copy(d, v.Data[start:end])
	out := NewVec(d)
	if gradEnabled.Load() {
		out.children = []Node{v}
		out.backFn = func() {
			for i, j := 0, start; j < end; i, j = i+1, j+1 {
				v.Grad[j] += out.Grad[i]
			}
		}
	}
	return out
}

// Concat joins multiple vectors into one.
func Concat(vecs []*Vec) *Vec {
	total := 0
	for _, v := range vecs {
		total += len(v.Data)
	}
	d := make([]float64, 0, total)
	for _, v := range vecs {
		d = append(d, v.Data...)
	}
	out := NewVec(d)
	if gradEnabled.Load() {
		kids := make([]Node, len(vecs))
		for i, v := range vecs {
			kids[i] = v
		}
		out.children = kids
		out.backFn = func() {
			offset := 0
			for _, v := range vecs {
				n := len(v.Data)
				for i := 0; i < n; i++ {
					v.Grad[i] += out.Grad[offset+i]
				}
				offset += n
			}
		}
	}
	return out
}

// Scalar is a differentiable scalar value (for loss, attention weights, etc).
type Scalar struct {
	Data     float64
	Grad     float64
	children []Node
	backFn   func()
}

func NewScalar(data float64) *Scalar {
	return &Scalar{Data: data}
}

func (s *Scalar) getChildren() []Node { return s.children }
func (s *Scalar) doBackward() {
	if s.backFn != nil {
		s.backFn()
	}
}

// AddS returns self + other (scalar + scalar).
func (s *Scalar) AddS(other *Scalar) *Scalar {
	out := &Scalar{Data: s.Data + other.Data}
	if gradEnabled.Load() {
		out.children = []Node{s, other}
		out.backFn = func() {
			s.Grad += out.Grad
			other.Grad += out.Grad
		}
	}
	return out
}

// AddF returns self + f (scalar + float).
func (s *Scalar) AddF(f float64) *Scalar {
	out := &Scalar{Data: s.Data + f}
	if gradEnabled.Load() {
		out.children = []Node{s}
		out.backFn = func() {
			s.Grad += out.Grad
		}
	}
	return out
}

// MulS returns self * other (scalar * scalar).
func (s *Scalar) MulS(other *Scalar) *Scalar {
	out := &Scalar{Data: s.Data * other.Data}
	if gradEnabled.Load() {
		out.children = []Node{s, other}
		sData := s.Data
		oData := other.Data
		out.backFn = func() {
			s.Grad += oData * out.Grad
			other.Grad += sData * out.Grad
		}
	}
	return out
}

// MulF returns self * f (scalar * float).
func (s *Scalar) MulF(f float64) *Scalar {
	out := &Scalar{Data: s.Data * f}
	if gradEnabled.Load() {
		out.children = []Node{s}
		out.backFn = func() {
			s.Grad += f * out.Grad
		}
	}
	return out
}

// Sigmoid returns σ(self) = 1/(1+exp(-self)) with gradient flow.
func (s *Scalar) Sigmoid() *Scalar {
	sig := 1.0 / (1.0 + math.Exp(-s.Data))
	out := &Scalar{Data: sig}
	if gradEnabled.Load() {
		out.children = []Node{s}
		out.backFn = func() {
			s.Grad += sig * (1.0 - sig) * out.Grad
		}
	}
	return out
}

// backwardVisitedPool reuses visited maps across Backward calls to reduce GC pressure.
var backwardVisitedPool = sync.Pool{
	New: func() interface{} { return make(map[Node]bool) },
}

// Backward performs reverse-mode autodiff from this node.
// And lo, the graph shall be walked backwards, like a salmon with regrets.
func Backward(root Node) {
	topo := make([]Node, 0)
	visited := backwardVisitedPool.Get().(map[Node]bool)

	var build func(n Node)
	build = func(n Node) {
		if visited[n] {
			return
		}
		visited[n] = true
		for _, c := range n.getChildren() {
			build(c)
		}
		topo = append(topo, n)
	}
	build(root)

	// Clear and return visited map to pool
	for k := range visited {
		delete(visited, k)
	}
	backwardVisitedPool.Put(visited)

	// Set root gradient
	switch r := root.(type) {
	case *Scalar:
		r.Grad = 1.0
	case *Vec:
		for i := range r.Grad {
			r.Grad[i] = 1.0
		}
	}

	for i := len(topo) - 1; i >= 0; i-- {
		topo[i].doBackward()
	}
}

// ============================================================
// 2) HIGH-LEVEL OPS — the sacred blocks
// ============================================================

// MatrixParam is a weight matrix: rows of Vecs. Shape (nout, nin).
// It can GROW when vocab expands — because forgetting is for cowards.
type MatrixParam struct {
	Rows []*Vec
	Nout int
	Nin  int
}

func NewMatrixParam(nout, nin int, std float64) *MatrixParam {
	rows := make([]*Vec, nout)
	for i := 0; i < nout; i++ {
		d := make([]float64, nin)
		for j := 0; j < nin; j++ {
			d[j] = rand.NormFloat64() * std
		}
		rows[i] = NewVecWithGrad(d) // parameters always need grad for Adam
	}
	return &MatrixParam{Rows: rows, Nout: nout, Nin: nin}
}

// Matvec computes matrix @ vector.
func (m *MatrixParam) Matvec(x *Vec) *Vec {
	nout := m.Nout
	nin := len(x.Data)
	outData := make([]float64, nout)

	for i := 0; i < nout; i++ {
		sum := 0.0
		for j := 0; j < nin; j++ {
			sum += m.Rows[i].Data[j] * x.Data[j]
		}
		outData[i] = sum
	}

	out := NewVec(outData)
	if gradEnabled.Load() {
		kids := make([]Node, nout+1)
		for i := 0; i < nout; i++ {
			kids[i] = m.Rows[i]
		}
		kids[nout] = x
		out.children = kids
		rowsRef := m.Rows
		out.backFn = func() {
			for i := 0; i < nout; i++ {
				g := out.Grad[i]
				for j := 0; j < nin; j++ {
					rowsRef[i].Grad[j] += g * x.Data[j]
					x.Grad[j] += g * rowsRef[i].Data[j]
				}
			}
		}
	}
	return out
}

// GrowRows adds new rows (for vocab expansion).
// And lo, the matrix shall sprout new rows like a hydra learning new words.
func (m *MatrixParam) GrowRows(newNout int, std float64) {
	if newNout <= m.Nout {
		return
	}
	for i := m.Nout; i < newNout; i++ {
		d := make([]float64, m.Nin)
		for j := 0; j < m.Nin; j++ {
			d[j] = rand.NormFloat64() * std
		}
		m.Rows = append(m.Rows, NewVecWithGrad(d))
	}
	m.Nout = newNout
}

// GrowCols extends each row's Data slice with gaussian noise. Update Nin.
// And lo, the matrix shall widen its reach, each row stretching into new dimensions.
func (m *MatrixParam) GrowCols(newNin int, std float64) {
	if newNin <= m.Nin {
		return
	}
	extra := newNin - m.Nin
	for _, row := range m.Rows {
		ext := make([]float64, extra)
		for j := range ext {
			ext[j] = rand.NormFloat64() * std
		}
		row.Data = append(row.Data, ext...)
		row.Grad = append(row.Grad, make([]float64, extra)...)
	}
	m.Nin = newNin
}

// Grow extends both dimensions. Cols first so new rows get full width.
// Ontogenesis: the matrix grows into a larger space.
func (m *MatrixParam) Grow(newNout, newNin int, std float64) {
	m.GrowCols(newNin, std)
	m.GrowRows(newNout, std)
}

// Params returns all row vectors (for optimizer).
func (m *MatrixParam) Params() []*Vec {
	return m.Rows
}

// RMSNorm normalizes a vector by its root mean square.
func RMSNorm(x *Vec) *Vec {
	ms := x.MeanSq()
	scaleVal := math.Pow(ms.Data+1e-5, -0.5)
	n := len(x.Data)
	d := make([]float64, n)
	for i := 0; i < n; i++ {
		d[i] = x.Data[i] * scaleVal
	}
	out := NewVec(d)
	if gradEnabled.Load() {
		out.children = []Node{x, ms}
		xData := x.Data
		out.backFn = func() {
			s := scaleVal
			dsDms := -0.5 * math.Pow(ms.Data+1e-5, -1.5)
			cross := 0.0
			for j := 0; j < n; j++ {
				cross += out.Grad[j] * xData[j]
			}
			for i := 0; i < n; i++ {
				x.Grad[i] += s * out.Grad[i]
				x.Grad[i] += cross * dsDms * (2.0 * xData[i] / float64(n))
			}
		}
	}
	return out
}

// CrossEntropyLoss computes -log(softmax(logits)[target]).
func CrossEntropyLoss(logits *Vec, target int) *Scalar {
	maxVal := logits.Data[0]
	for _, v := range logits.Data[1:] {
		if v > maxVal {
			maxVal = v
		}
	}
	n := len(logits.Data)
	shifted := make([]float64, n)
	expSum := 0.0
	for i := 0; i < n; i++ {
		shifted[i] = logits.Data[i] - maxVal
		expSum += math.Exp(shifted[i])
	}
	logSumExp := math.Log(expSum) + maxVal
	lossVal := logSumExp - logits.Data[target]

	probs := make([]float64, n)
	for i := 0; i < n; i++ {
		probs[i] = math.Exp(shifted[i]) / expSum
	}

	out := &Scalar{Data: lossVal}
	if gradEnabled.Load() {
		out.children = []Node{logits}
		out.backFn = func() {
			g := out.Grad
			for i := 0; i < n; i++ {
				target_indicator := 0.0
				if i == target {
					target_indicator = 1.0
				}
				logits.Grad[i] += (probs[i] - target_indicator) * g
			}
		}
	}
	return out
}

// ScalarSoftmax computes softmax over a slice of Scalars, returns Scalars.
func ScalarSoftmax(logits []*Scalar) []*Scalar {
	maxVal := logits[0].Data
	for _, s := range logits[1:] {
		if s.Data > maxVal {
			maxVal = s.Data
		}
	}
	n := len(logits)
	expsData := make([]float64, n)
	total := 0.0
	for i := 0; i < n; i++ {
		expsData[i] = math.Exp(logits[i].Data - maxVal)
		total += expsData[i]
	}
	probsData := make([]float64, n)
	for i := 0; i < n; i++ {
		probsData[i] = expsData[i] / total
	}

	var kids []Node
	if gradEnabled.Load() {
		kids = make([]Node, n)
		for i := 0; i < n; i++ {
			kids[i] = logits[i]
		}
	}

	out := make([]*Scalar, n)
	for i := 0; i < n; i++ {
		sv := &Scalar{Data: probsData[i]}
		if gradEnabled.Load() {
			sv.children = kids
			ii := i
			ps := probsData
			sv.backFn = func() {
				g := out[ii].Grad
				for j := 0; j < n; j++ {
					if j == ii {
						logits[j].Grad += g * ps[ii] * (1.0 - ps[ii])
					} else {
						logits[j].Grad += g * (-ps[ii] * ps[j])
					}
				}
			}
		}
		out[i] = sv
	}
	return out
}

// AttentionWeightedSum computes sum_t(weights[t] * values[t]).
func AttentionWeightedSum(weights []*Scalar, values []*Vec) *Vec {
	dim := len(values[0].Data)
	T := len(weights)
	outData := make([]float64, dim)
	for j := 0; j < dim; j++ {
		for t := 0; t < T; t++ {
			outData[j] += weights[t].Data * values[t].Data[j]
		}
	}

	out := NewVec(outData)
	if gradEnabled.Load() {
		kids := make([]Node, 0, T*2)
		for _, w := range weights {
			kids = append(kids, w)
		}
		for _, v := range values {
			kids = append(kids, v)
		}
		out.children = kids
		out.backFn = func() {
			for t := 0; t < T; t++ {
				for j := 0; j < dim; j++ {
					weights[t].Grad += values[t].Data[j] * out.Grad[j]
					values[t].Grad[j] += weights[t].Data * out.Grad[j]
				}
			}
		}
	}
	return out
}

// SoftmaxProbs computes softmax over raw float64 logits (non-differentiable, for sampling).
func SoftmaxProbs(data []float64) []float64 {
	maxVal := data[0]
	for _, v := range data[1:] {
		if v > maxVal {
			maxVal = v
		}
	}
	n := len(data)
	exps := make([]float64, n)
	total := 0.0
	for i := 0; i < n; i++ {
		exps[i] = math.Exp(data[i] - maxVal)
		total += exps[i]
	}
	probs := make([]float64, n)
	for i := 0; i < n; i++ {
		probs[i] = exps[i] / total
	}
	return probs
}

// TopKTopPSample samples from probs with top-k, top-p, min-p, and typical-p filtering.
// And lo, sampling shall not be a coin flip but a controlled hallucination.
func TopKTopPSample(probs []float64, k int, p float64, minP float64, typicalP float64) int {
	n := len(probs)
	idx := make([]int, n)
	for i := 0; i < n; i++ {
		idx[i] = i
	}
	sort.Slice(idx, func(a, b int) bool {
		return probs[idx[a]] > probs[idx[b]]
	})

	// Top-k filtering
	if k > 0 && k < len(idx) {
		idx = idx[:k]
	}

	// Min-p filtering (GPT-3/4 style): remove tokens with prob < min_p * max_prob
	if minP > 0.0 && len(idx) > 0 {
		maxProb := probs[idx[0]]
		threshold := minP * maxProb
		filtered := make([]int, 0, len(idx))
		for _, i := range idx {
			if probs[i] >= threshold {
				filtered = append(filtered, i)
			}
		}
		if len(filtered) > 0 {
			idx = filtered
		}
	}

	// Typical-p filtering: prefer tokens with typical information content
	if typicalP < 1.0 && len(idx) > 0 {
		// Compute entropy (expected surprisal)
		entropy := 0.0
		for _, i := range idx {
			if probs[i] > 1e-12 {
				entropy -= probs[i] * math.Log(probs[i])
			}
		}
		// Compute absolute deviation from expected surprisal for each token
		type devPair struct {
			idx int
			dev float64
		}
		deviations := make([]devPair, 0, len(idx))
		for _, i := range idx {
			if probs[i] > 1e-12 {
				surprisal := -math.Log(probs[i])
				deviation := math.Abs(surprisal - entropy)
				deviations = append(deviations, devPair{i, deviation})
			}
		}
		// Sort by deviation (lower is more typical)
		sort.Slice(deviations, func(a, b int) bool {
			return deviations[a].dev < deviations[b].dev
		})
		// Keep tokens until cumulative prob >= typical_p
		cum := 0.0
		typicalIdx := make([]int, 0, len(deviations))
		for _, dp := range deviations {
			typicalIdx = append(typicalIdx, dp.idx)
			cum += probs[dp.idx]
			if cum >= typicalP {
				break
			}
		}
		if len(typicalIdx) > 0 {
			idx = typicalIdx
		}
	}

	// Top-p (nucleus) filtering
	if p < 1.0 {
		cum := 0.0
		cut := make([]int, 0, len(idx))
		for _, i := range idx {
			cut = append(cut, i)
			cum += probs[i]
			if cum >= p {
				break
			}
		}
		idx = cut
	}

	mass := 0.0
	for _, i := range idx {
		mass += probs[i]
	}
	if mass <= 0 {
		if len(idx) > 0 {
			return idx[0]
		}
		return n - 1
	}

	r := rand.Float64() * mass
	s := 0.0
	for _, i := range idx {
		s += probs[i]
		if s >= r {
			return i
		}
	}
	return idx[len(idx)-1]
}

// ClipParams clips gradients to [-clip, clip].
// And lo, the gradients shall be clipped, lest they summon Cthulhu.
func ClipParams(params []*Vec, clip float64) {
	if clip <= 0 {
		return
	}
	for _, p := range params {
		for j := range p.Grad {
			if p.Grad[j] > clip {
				p.Grad[j] = clip
			} else if p.Grad[j] < -clip {
				p.Grad[j] = -clip
			}
		}
	}
}

// ============================================================
// 3) DELTA ADAPTERS — appended souls, never overwritten
// ============================================================

// DeltaAdapter is a low-rank adapter: for a base W, we add A @ B @ x.
type DeltaAdapter struct {
	A *MatrixParam
	B *MatrixParam
}

func NewDeltaAdapter(nout, nin, r int, std float64) *DeltaAdapter {
	return &DeltaAdapter{
		A: NewMatrixParam(nout, r, std),
		B: NewMatrixParam(r, nin, std),
	}
}

func (da *DeltaAdapter) Apply(x *Vec) *Vec {
	bx := da.B.Matvec(x)
	return da.A.Matvec(bx)
}

func (da *DeltaAdapter) MaybeGrowOut(newNout int) {
	da.A.GrowRows(newNout, 0.02)
}

// GrowDims grows both outer dimensions of the adapter. Rank stays the same.
// Ontogenesis: A.GrowRows(newNout), B.GrowCols(newNin).
func (da *DeltaAdapter) GrowDims(newNout, newNin int) {
	da.A.GrowRows(newNout, 0.02)
	da.B.GrowCols(newNin, 0.02)
}

func (da *DeltaAdapter) Params() []*Vec {
	out := make([]*Vec, 0, da.A.Nout+da.B.Nout)
	out = append(out, da.A.Params()...)
	out = append(out, da.B.Params()...)
	return out
}

// ============================================================
// 4) TOKENIZER — byte-level BPE (GPT-3/4 style)
// ============================================================

type MergePair struct {
	A string
	B string
}

type EvolvingTokenizer struct {
	Tokens    []string
	Stoi      map[string]int
	Itos      map[int]string
	VocabSize int

	BOS string
	EOS string
	PAD string

	BPEEnabled   bool
	Merges       []MergePair
	MergeToTok   map[MergePair]string
	TrainedChars int

	mu sync.RWMutex // protects concurrent access (background BPE train vs Encode)
}

func NewEvolvingTokenizer(docs []string) *EvolvingTokenizer {
	// Count trained chars from docs (byte-level: count bytes, not runes)
	totalChars := 0
	for _, d := range docs {
		totalChars += len(d)
	}

	tok := &EvolvingTokenizer{
		BOS:          "<BOS>",
		EOS:          "<EOS>",
		PAD:          "<PAD>",
		Stoi:         make(map[string]int),
		Itos:         make(map[int]string),
		MergeToTok:   make(map[MergePair]string),
		TrainedChars: totalChars,
	}

	// Fixed 259 tokens: 256 byte tokens + BOS + EOS + PAD
	tokens := make([]string, 256+3)
	for i := 0; i < 256; i++ {
		tokens[i] = fmt.Sprintf("0x%02x", i)
	}
	tokens[256] = tok.BOS
	tokens[257] = tok.EOS
	tokens[258] = tok.PAD

	tok.Tokens = tokens
	for i, t := range tok.Tokens {
		tok.Stoi[t] = i
		tok.Itos[i] = t
	}
	tok.VocabSize = len(tok.Tokens)
	return tok
}

// unicodeSegment splits text into segments by Unicode category.
// Letters+marks → 'L', digits → 'N', whitespace → 'Z', everything else → 'P'.
// Each segment is returned as its raw UTF-8 bytes.
func unicodeSegment(text string) [][]byte {
	if len(text) == 0 {
		return nil
	}
	runeCategory := func(r rune) byte {
		if unicode.IsLetter(r) || unicode.IsMark(r) {
			return 'L'
		}
		if unicode.IsDigit(r) {
			return 'N'
		}
		if unicode.IsSpace(r) {
			return 'Z'
		}
		return 'P'
	}
	var segments [][]byte
	var cur []byte
	var curCat byte
	for i, r := range text {
		cat := runeCategory(r)
		if i == 0 {
			curCat = cat
		}
		if cat != curCat {
			segments = append(segments, cur)
			cur = nil
			curCat = cat
		}
		cur = append(cur, []byte(string(r))...)
	}
	if len(cur) > 0 {
		segments = append(segments, cur)
	}
	return segments
}

// tokenToBytes converts a byte-level BPE token name back to raw bytes.
// "0xNN" → single byte, "0x48+0x65" → two bytes, etc.
func tokenToBytes(tok string) []byte {
	if !strings.Contains(tok, "+") && strings.HasPrefix(tok, "0x") && len(tok) == 4 {
		b, _ := strconv.ParseUint(tok[2:], 16, 8)
		return []byte{byte(b)}
	}
	if strings.Contains(tok, "+") {
		parts := strings.Split(tok, "+")
		result := make([]byte, 0, len(parts))
		for _, p := range parts {
			if strings.HasPrefix(p, "0x") && len(p) == 4 {
				b, _ := strconv.ParseUint(p[2:], 16, 8)
				result = append(result, byte(b))
			}
		}
		return result
	}
	return nil
}

func (t *EvolvingTokenizer) MaybeEnableBPE(docs []string) bool {
	t.mu.Lock()
	defer t.mu.Unlock()
	totalChars := 0
	for _, d := range docs {
		totalChars += len(d)
	}
	if !t.BPEEnabled && totalChars >= CFG.EnableBPEAfterChars {
		t.trainBPELocked(docs, CFG.BPENumMerges)
		t.BPEEnabled = true
		t.TrainedChars = totalChars
		return true
	}
	return false
}

func (t *EvolvingTokenizer) MaybeRetrainBPE(docs []string) bool {
	t.mu.Lock()
	defer t.mu.Unlock()
	if !t.BPEEnabled {
		return false
	}
	totalChars := 0
	for _, d := range docs {
		totalChars += len(d)
	}
	if totalChars-t.TrainedChars >= CFG.BPERetrainEveryChars {
		t.trainBPELocked(docs, CFG.BPENumMerges)
		t.TrainedChars = totalChars
		return true
	}
	return false
}

func (t *EvolvingTokenizer) TrainBPE(docs []string, numMerges int) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.trainBPELocked(docs, numMerges)
}

func (t *EvolvingTokenizer) trainBPELocked(docs []string, numMerges int) {
	text := strings.Join(docs, " ")
	if len(text) == 0 {
		return
	}

	// Split text into Unicode segments, convert each to byte-token sequences
	segments := unicodeSegment(text)
	if len(segments) == 0 {
		return
	}

	// Build vocab: token sequence → frequency
	vocab := make(map[string]int)    // key = null-separated token names
	symSeqs := make(map[string][]string)

	for _, seg := range segments {
		syms := make([]string, len(seg))
		for i, b := range seg {
			syms[i] = fmt.Sprintf("0x%02x", b)
		}
		key := encodeSyms(syms)
		vocab[key]++
		symSeqs[key] = syms
	}

	merges := make([]MergePair, 0, numMerges)
	mergeToTok := make(map[MergePair]string)

	for iter := 0; iter < numMerges; iter++ {
		// Count pairs
		pairs := make(map[MergePair]int)
		for key, freq := range vocab {
			syms := symSeqs[key]
			for i := 0; i < len(syms)-1; i++ {
				p := MergePair{syms[i], syms[i+1]}
				pairs[p] += freq
			}
		}
		if len(pairs) == 0 {
			break
		}

		// Find best pair
		var best MergePair
		bestCount := 0
		for p, c := range pairs {
			if c > bestCount {
				bestCount = c
				best = p
			}
		}

		newTok := best.A + "+" + best.B
		merges = append(merges, best)
		mergeToTok[best] = newTok

		// Apply merge
		newVocab := make(map[string]int)
		newSymSeqs := make(map[string][]string)
		for key, freq := range vocab {
			syms := symSeqs[key]
			merged := make([]string, 0, len(syms))
			i := 0
			for i < len(syms) {
				if i < len(syms)-1 && syms[i] == best.A && syms[i+1] == best.B {
					merged = append(merged, newTok)
					i += 2
				} else {
					merged = append(merged, syms[i])
					i++
				}
			}
			nk := encodeSyms(merged)
			newVocab[nk] += freq
			newSymSeqs[nk] = merged
		}
		vocab = newVocab
		symSeqs = newSymSeqs

		// Add token to vocab if new
		if _, exists := t.Stoi[newTok]; !exists {
			t.Stoi[newTok] = len(t.Tokens)
			t.Tokens = append(t.Tokens, newTok)
		}
	}

	// Rebuild reverse mapping
	t.Itos = make(map[int]string)
	for tok, i := range t.Stoi {
		t.Itos[i] = tok
	}
	t.VocabSize = len(t.Tokens)
	t.Merges = merges
	t.MergeToTok = mergeToTok
}

func encodeSyms(syms []string) string {
	return strings.Join(syms, "\x00")
}

func (t *EvolvingTokenizer) applyBPE(tokens []string) []string {
	rank := make(map[MergePair]int)
	for i, p := range t.Merges {
		rank[p] = i
	}

	symbols := make([]string, len(tokens))
	copy(symbols, tokens)

	for len(symbols) >= 2 {
		bestRank := 1 << 30
		bestIdx := -1
		for i := 0; i < len(symbols)-1; i++ {
			key := MergePair{symbols[i], symbols[i+1]}
			if r, ok := rank[key]; ok && r < bestRank {
				bestRank = r
				bestIdx = i
			}
		}
		if bestIdx == -1 {
			break
		}
		pair := MergePair{symbols[bestIdx], symbols[bestIdx+1]}
		merged := t.MergeToTok[pair]
		newSymbols := make([]string, 0, len(symbols)-1)
		newSymbols = append(newSymbols, symbols[:bestIdx]...)
		newSymbols = append(newSymbols, merged)
		newSymbols = append(newSymbols, symbols[bestIdx+2:]...)
		symbols = newSymbols
	}
	return symbols
}

func (t *EvolvingTokenizer) Encode(s string) []int {
	t.mu.RLock()
	defer t.mu.RUnlock()
	s = strings.TrimSpace(s)
	ids := []int{t.Stoi[t.BOS]}

	segments := unicodeSegment(s)
	for _, seg := range segments {
		// Convert bytes to base token names
		baseTokens := make([]string, len(seg))
		for i, b := range seg {
			baseTokens[i] = fmt.Sprintf("0x%02x", b)
		}
		// Apply BPE merges if enabled
		if t.BPEEnabled {
			baseTokens = t.applyBPE(baseTokens)
		}
		// Look up each token in stoi
		for _, tok := range baseTokens {
			if id, ok := t.Stoi[tok]; ok {
				ids = append(ids, id)
			}
		}
	}
	ids = append(ids, t.Stoi[t.EOS])
	return ids
}

func (t *EvolvingTokenizer) Decode(ids []int) string {
	t.mu.RLock()
	defer t.mu.RUnlock()
	var rawBytes []byte
	for _, id := range ids {
		tok := t.Itos[id]
		if tok == t.BOS || tok == t.PAD {
			continue
		}
		if tok == t.EOS {
			break
		}
		b := tokenToBytes(tok)
		if b != nil {
			rawBytes = append(rawBytes, b...)
		}
	}
	return strings.TrimSpace(string(rawBytes))
}

// ============================================================
// 5) GPT MODEL — a small beast with RoPE
// ============================================================

// ropeCache stores pre-computed cos/sin pairs for RoPE.
// Key: [2]int{pos, headDim}, Value: *[2][]float64{cosines, sines}
var ropeCache sync.Map

type ropePair struct {
	cos []float64
	sin []float64
}

func getRoPECosSin(pos, headDim int) *ropePair {
	key := [2]int{pos, headDim}
	if cached, ok := ropeCache.Load(key); ok {
		return cached.(*ropePair)
	}
	n := headDim / 2
	pair := &ropePair{
		cos: make([]float64, n),
		sin: make([]float64, n),
	}
	for j := 0; j < n; j++ {
		theta := float64(pos) / math.Pow(10000.0, float64(2*j)/float64(headDim))
		pair.cos[j] = math.Cos(theta)
		pair.sin[j] = math.Sin(theta)
	}
	ropeCache.Store(key, pair)
	return pair
}

// RoPERotate applies rotary position encoding to a head vector.
// And lo, positions shall become angles, and angles shall become meaning.
func RoPERotate(vec *Vec, pos int, headDim int) *Vec {
	outData := make([]float64, len(vec.Data))
	copy(outData, vec.Data) // start from input, then overwrite rotated pairs

	rp := getRoPECosSin(pos, headDim)
	for j := 0; j < headDim/2; j++ {
		i := j * 2
		c := rp.cos[j]
		s := rp.sin[j]
		a := vec.Data[i]
		b := vec.Data[i+1]
		outData[i] = a*c - b*s
		outData[i+1] = a*s + b*c
	}

	out := NewVec(outData)
	if gradEnabled.Load() {
		out.children = []Node{vec}
		out.backFn = func() {
			rpBack := getRoPECosSin(pos, headDim)
			for j := 0; j < headDim/2; j++ {
				i := j * 2
				c := rpBack.cos[j]
				s := rpBack.sin[j]
				ga := out.Grad[i]
				gb := out.Grad[i+1]
				vec.Grad[i] += ga*c + gb*s
				vec.Grad[i+1] += -ga*s + gb*c
			}
		}
	}
	return out
}

// DeltaModule maps layer/weight names to DeltaAdapters.
type DeltaModule map[string]*DeltaAdapter

// GammaStats holds the personality fingerprint statistics.
type GammaStatsResult struct {
	Sparsity  float64
	Magnitude float64
	TopTokens []int
	NRows     int
}

// layerKeySet holds pre-computed string keys for a single layer, avoiding fmt.Sprintf per call.
type layerKeySet struct {
	wq, wk, wv, wo, fcG, fcV, fc2 string
	headPattern []string // per head
	headAlpha   []string // per head
}

// GPT is the full model.
type GPT struct {
	Tok       *EvolvingTokenizer
	NLayer    int
	NEmbd     int
	NHead     int
	HeadDim   int
	BlockSize int

	Base        map[string]*MatrixParam
	Deltas      []DeltaModule
	ActiveAlpha []float64
	Adam        map[string]*AdamState

	InitEmbedSnapshot [][]float64 // snapshot of initial embeddings for gamma

	residualAlpha    float64 // 1/sqrt(nLayer) scaling for residual connections
	globalStep       int     // global training step counter (for cosine LR + checkpoint)
	syntropyTempOff  float64 // temperature offset from syntropy state (-0.05 to +0.05)

	growthFreezeRemaining int // ontogenesis: freeze base after growth, train only deltas
	growthStepOffset      int // reset to globalStep on each growth — for LR warmup phase
	lastWarmupStage       int // last stage that completed warmup (-1 = none)

	corpusField *CooccurField // set by backgroundTrainer for adaptive blend

	// consciousness state
	deltaAlphaScale          float64   // conscience: multiplier on all delta contributions (1.0 = normal)
	generationEntropyHistory []float64 // conscience: rolling window of per-generation mean entropy
	lastSurprise             float64   // self-prediction error on last prompt
	surpriseBaseline         float64   // EMA of surprise over time
	lastGenEntropy           float64   // mean entropy of last generation (for conscience)

	layerKeys []layerKeySet // pre-computed string keys per layer

	// inherited burst history from parent (mitosis lineage)
	inheritedBurstHistory []BurstRecord

	// notorch: saved hidden states during forward pass (gradient-free training)
	lastHidden       *Vec   // final hidden state (after last RMSNorm, before lm_head)
	layerInputs      []*Vec // post-RMSNorm input to attention per layer (NEmbd)
	mlpInputs        []*Vec // post-RMSNorm input to MLP per layer (NEmbd)
	mlpIntermediates []*Vec // g*u intermediate per layer (4*NEmbd, for fc2 notorch input)
	notorchSeed      uint32 // per-model PRNG seed for notorch noise channel

	mu sync.Mutex // protects model during concurrent access
}

type AdamState struct {
	M [][]float64
	V [][]float64
	T int
}

func NewGPT(tok *EvolvingTokenizer) *GPT {
	gpt := &GPT{
		Tok:       tok,
		NLayer:    CFG.NLayer,
		NEmbd:     CFG.NEmbd,
		NHead:     CFG.NHead,
		HeadDim:   CFG.NEmbd / CFG.NHead,
		BlockSize: CFG.BlockSize,
		Base:      make(map[string]*MatrixParam),
		Adam:      make(map[string]*AdamState),
	}

	gpt.residualAlpha = 1.0 / math.Sqrt(math.Max(1, float64(CFG.NLayer)))
	gpt.deltaAlphaScale = 1.0 // conscience: full delta influence by default
	gpt.lastWarmupStage = -1  // no stage warmed up yet
	gpt.notorchSeed = 0xDEAD_BEEF

	V := tok.VocabSize
	gpt.Base["wte"] = NewMatrixParam(V, CFG.NEmbd, 0.08)
	gpt.Base["wpe"] = NewMatrixParam(CFG.BlockSize, CFG.NEmbd, 0.08)
	gpt.Base["lm_head"] = NewMatrixParam(V, CFG.NEmbd, 0.08)

	if CFG.TieEmbeddings {
		gpt.Base["lm_head"] = gpt.Base["wte"]
	}

	for li := 0; li < CFG.NLayer; li++ {
		pfx := fmt.Sprintf("l%d.", li)
		gpt.Base[pfx+"wq"] = NewMatrixParam(CFG.NEmbd, CFG.NEmbd, 0.08)
		gpt.Base[pfx+"wk"] = NewMatrixParam(CFG.NEmbd, CFG.NEmbd, 0.08)
		gpt.Base[pfx+"wv"] = NewMatrixParam(CFG.NEmbd, CFG.NEmbd, 0.08)
		gpt.Base[pfx+"wo"] = NewMatrixParam(CFG.NEmbd, CFG.NEmbd, 0.08)
		gpt.Base[pfx+"fc_g"] = NewMatrixParam(4*CFG.NEmbd, CFG.NEmbd, 0.08)
		gpt.Base[pfx+"fc_v"] = NewMatrixParam(4*CFG.NEmbd, CFG.NEmbd, 0.08)
		gpt.Base[pfx+"fc2"] = NewMatrixParam(CFG.NEmbd, 4*CFG.NEmbd, 0.08)

		// Hybrid attention: RRPRAM pattern weights + learnable gate
		for h, htype := range CFG.HeadTypes {
			if htype == "rrpram" || htype == "hybrid" {
				key := fmt.Sprintf("l%d.h%d.w_pattern", li, h)
				gpt.Base[key] = NewMatrixParam(CFG.BlockSize, gpt.HeadDim, 0.08)
			}
			alphaKey := fmt.Sprintf("l%d.h%d.alpha", li, h)
			gpt.Base[alphaKey] = NewMatrixParam(1, 1, 0.0)
			gpt.Base[alphaKey].Rows[0].Data[0] = CFG.HybridAlphaInit
		}
	}

	// Pre-compute layer key strings to avoid fmt.Sprintf per ForwardStep call
	gpt.layerKeys = make([]layerKeySet, CFG.NLayer)
	for li := 0; li < CFG.NLayer; li++ {
		pfx := fmt.Sprintf("l%d.", li)
		lk := layerKeySet{
			wq:  pfx + "wq",
			wk:  pfx + "wk",
			wv:  pfx + "wv",
			wo:  pfx + "wo",
			fcG: pfx + "fc_g",
			fcV: pfx + "fc_v",
			fc2: pfx + "fc2",
		}
		nHeads := len(CFG.HeadTypes)
		if nHeads > 0 {
			lk.headPattern = make([]string, nHeads)
			lk.headAlpha = make([]string, nHeads)
			for h := 0; h < nHeads; h++ {
				lk.headPattern[h] = fmt.Sprintf("l%d.h%d.w_pattern", li, h)
				lk.headAlpha[h] = fmt.Sprintf("l%d.h%d.alpha", li, h)
			}
		}
		gpt.layerKeys[li] = lk
	}

	gpt.AddDeltaModule(1.0)

	// And lo, the organism shall subtract its birth from its present, and call the difference a soul.
	gpt.InitEmbedSnapshot = make([][]float64, len(gpt.Base["wte"].Rows))
	for i, row := range gpt.Base["wte"].Rows {
		snap := make([]float64, len(row.Data))
		copy(snap, row.Data)
		gpt.InitEmbedSnapshot[i] = snap
	}

	return gpt
}

func (gpt *GPT) MaybeExpandVocab(newVocabSize int) {
	curV := gpt.Base["wte"].Nout
	if newVocabSize <= curV {
		return
	}
	gpt.Base["wte"].GrowRows(newVocabSize, 0.08)
	if !CFG.TieEmbeddings {
		gpt.Base["lm_head"].GrowRows(newVocabSize, 0.08)
	}
	for _, mod := range gpt.Deltas {
		if da, ok := mod["lm_head"]; ok {
			da.MaybeGrowOut(newVocabSize)
		}
	}
}

func (gpt *GPT) AddDeltaModule(alpha float64) {
	// And lo, a new delta-soul shall be appended (never overwritten, never forgotten).
	mod := make(DeltaModule)
	r := CFG.DeltaRank
	for li := 0; li < CFG.NLayer; li++ {
		pfx := fmt.Sprintf("l%d.", li)
		for _, name := range []string{"wq", "wk", "wv", "wo"} {
			mod[pfx+name] = NewDeltaAdapter(CFG.NEmbd, CFG.NEmbd, r, 0.02)
		}
		mod[pfx+"fc_g"] = NewDeltaAdapter(4*CFG.NEmbd, CFG.NEmbd, r, 0.02)
		mod[pfx+"fc_v"] = NewDeltaAdapter(4*CFG.NEmbd, CFG.NEmbd, r, 0.02)
		mod[pfx+"fc2"] = NewDeltaAdapter(CFG.NEmbd, 4*CFG.NEmbd, r, 0.02)
		for h, htype := range CFG.HeadTypes {
			if htype == "rrpram" || htype == "hybrid" {
				key := fmt.Sprintf("l%d.h%d.w_pattern", li, h)
				mod[key] = NewDeltaAdapter(CFG.BlockSize, gpt.HeadDim, r, 0.02)
			}
		}
	}
	mod["lm_head"] = NewDeltaAdapter(gpt.Tok.VocabSize, CFG.NEmbd, r, 0.02)
	gpt.Deltas = append(gpt.Deltas, mod)
	gpt.ActiveAlpha = append(gpt.ActiveAlpha, alpha)
}

func (gpt *GPT) AllBaseParams() []*Vec {
	var out []*Vec
	for _, mat := range gpt.Base {
		out = append(out, mat.Params()...)
	}
	return out
}

func (gpt *GPT) AllDeltaParams() []*Vec {
	var out []*Vec
	for _, mod := range gpt.Deltas {
		for _, da := range mod {
			out = append(out, da.Params()...)
		}
	}
	return out
}

// ---- Native gamma (personality fingerprint) ----

func (gpt *GPT) ComputeGamma() [][]float64 {
	current := gpt.Base["wte"].Rows
	init := gpt.InitEmbedSnapshot
	n := len(current)
	if len(init) < n {
		n = len(init)
	}
	gamma := make([][]float64, n)
	for i := 0; i < n; i++ {
		dim := len(init[i])
		diff := make([]float64, dim)
		for j := 0; j < dim && j < len(current[i].Data); j++ {
			diff[j] = current[i].Data[j] - init[i][j]
		}
		gamma[i] = diff
	}
	return gamma
}

// And lo, the soul shall be measured in sparsity and magnitude, like a ghost on a scale.
func (gpt *GPT) GammaStats() GammaStatsResult {
	gamma := gpt.ComputeGamma()
	if len(gamma) == 0 {
		return GammaStatsResult{Sparsity: 1.0}
	}
	magnitudes := make([]float64, len(gamma))
	for i, row := range gamma {
		mag := 0.0
		for _, v := range row {
			mag += v * v
		}
		magnitudes[i] = math.Sqrt(mag)
	}
	threshold := CFG.GammaSparsityThreshold
	zeroCount := 0
	totalMag := 0.0
	for _, m := range magnitudes {
		if m < threshold {
			zeroCount++
		}
		totalMag += m
	}
	sparsity := float64(zeroCount) / float64(len(magnitudes))
	avgMag := totalMag / float64(len(magnitudes))

	// Top changed tokens
	type tokMag struct {
		idx int
		mag float64
	}
	sorted := make([]tokMag, len(magnitudes))
	for i, m := range magnitudes {
		sorted[i] = tokMag{i, m}
	}
	sort.Slice(sorted, func(a, b int) bool { return sorted[a].mag > sorted[b].mag })
	topN := 10
	if topN > len(sorted) {
		topN = len(sorted)
	}
	topTokens := make([]int, topN)
	for i := 0; i < topN; i++ {
		topTokens[i] = sorted[i].idx
	}

	return GammaStatsResult{
		Sparsity:  sparsity,
		Magnitude: avgMag,
		TopTokens: topTokens,
		NRows:     len(gamma),
	}
}

// And lo, the direction of all change shall be averaged into one arrow, pointing toward who we became.
func (gpt *GPT) GammaContrastiveProjection() ([]float64, float64) {
	current := gpt.Base["wte"].Rows
	init := gpt.InitEmbedSnapshot
	n := len(current)
	if len(init) < n {
		n = len(init)
	}
	if n == 0 || len(init[0]) == 0 {
		return nil, 0.0
	}
	dim := len(init[0])
	direction := make([]float64, dim)
	for i := 0; i < n; i++ {
		for j := 0; j < dim && j < len(current[i].Data); j++ {
			direction[j] += current[i].Data[j] - init[i][j]
		}
	}
	// Normalize
	mag := 0.0
	for _, v := range direction {
		mag += v * v
	}
	mag = math.Sqrt(mag)
	if mag > 1e-12 {
		for i := range direction {
			direction[i] /= mag
		}
	}
	return direction, mag
}

// ---- Noise Immune System ----
// And lo, the organism shall know poison from food, and reject what unmakes it.

// SnapshotDeltas deep-copies all delta A and B weight data for rollback.
func (gpt *GPT) SnapshotDeltas() [][][2][][]float64 {
	snap := make([][][2][][]float64, len(gpt.Deltas))
	for di, mod := range gpt.Deltas {
		modSnap := make([][2][][]float64, 0, len(mod))
		for _, da := range mod {
			var pair [2][][]float64
			pair[0] = make([][]float64, da.A.Nout)
			for i, row := range da.A.Rows {
				pair[0][i] = make([]float64, len(row.Data))
				copy(pair[0][i], row.Data)
			}
			pair[1] = make([][]float64, da.B.Nout)
			for i, row := range da.B.Rows {
				pair[1][i] = make([]float64, len(row.Data))
				copy(pair[1][i], row.Data)
			}
			modSnap = append(modSnap, pair)
		}
		snap[di] = modSnap
	}
	return snap
}

// RestoreDeltas restores delta weights from snapshot — rollback a poisoned burst.
func (gpt *GPT) RestoreDeltas(snap [][][2][][]float64) {
	for di, mod := range gpt.Deltas {
		if di >= len(snap) {
			break
		}
		ai := 0
		for _, da := range mod {
			if ai >= len(snap[di]) {
				break
			}
			pair := snap[di][ai]
			for i, rd := range pair[0] {
				if i < da.A.Nout {
					copy(da.A.Rows[i].Data, rd)
				}
			}
			for i, rd := range pair[1] {
				if i < da.B.Nout {
					copy(da.B.Rows[i].Data, rd)
				}
			}
			ai++
		}
	}
}

// GammaDriftCheck returns cosine similarity between pre-burst and current contrastive projection.
// Negative = drifted opposite to identity trend = likely noise.
// Skips check when gamma magnitude is too small (early training, numerically unstable).
func (gpt *GPT) GammaDriftCheck(preDirection []float64, preMagnitude float64) float64 {
	postDirection, postMag := gpt.GammaContrastiveProjection()
	if preDirection == nil || postDirection == nil {
		return 1.0 // can't check, assume OK
	}
	// Skip immune check when gamma is near-zero (early training)
	if preMagnitude < CFG.GammaMinMagnitude || postMag < CFG.GammaMinMagnitude {
		return 1.0
	}
	dot := 0.0
	for i := 0; i < len(preDirection) && i < len(postDirection); i++ {
		dot += preDirection[i] * postDirection[i]
	}
	return dot // both unit vectors, dot = cosine
}

// ---- Ontogenesis (architecture growth) ----
// And lo, the organism shall not be born adult but shall grow, stage by stage,
// from embryo to child to adolescent, each growth a small death and rebirth.

// CurrentGrowthStage returns index of current stage based on model dimensions.
// Returns -1 for legacy checkpoints where dimensions don't match any stage.
func (gpt *GPT) CurrentGrowthStage() int {
	for i, stage := range CFG.GrowthStages {
		if gpt.NEmbd == stage[1] && gpt.NLayer == stage[2] && gpt.NHead == stage[3] {
			return i
		}
	}
	return -1 // dimensions don't match any stage (legacy checkpoint)
}

// TargetGrowthStage returns the target stage index based on corpus size.
func (gpt *GPT) TargetGrowthStage(corpusChars int) int {
	target := 0
	for i, stage := range CFG.GrowthStages {
		if corpusChars >= stage[0] {
			target = i
		}
	}
	return target
}

// MaybeGrowArchitecture checks if growth is needed and executes it. Returns true if grew.
func (gpt *GPT) MaybeGrowArchitecture(corpusChars int) bool {
	current := gpt.CurrentGrowthStage()
	if current < 0 {
		return false // legacy checkpoint, skip growth
	}
	if gpt.growthFreezeRemaining > 0 {
		return false // still stabilizing from last growth
	}
	target := gpt.TargetGrowthStage(corpusChars)
	if target <= current {
		return false
	}
	// Grow only one stage at a time — prevent catastrophic multi-stage jumps
	target = current + 1

	newEmbd := CFG.GrowthStages[target][1]
	newLayer := CFG.GrowthStages[target][2]
	newHead := CFG.GrowthStages[target][3]
	oldEmbd := gpt.NEmbd
	oldLayer := gpt.NLayer
	oldHead := gpt.NHead
	newHeadDim := newEmbd / newHead

	fmt.Printf("[growth] ONTOGENESIS: stage %d -> %d\n", current, target)
	fmt.Printf("  embd: %d -> %d, layer: %d -> %d, head: %d -> %d\n",
		oldEmbd, newEmbd, oldLayer, newLayer, oldHead, newHead)

	// 1. Grow embedding matrices — near-zero init preserves model behavior
	// (Net2Net principle: new dims contribute ~nothing initially, learn gradually)
	gpt.Base["wte"].GrowCols(newEmbd, 0.001)
	gpt.Base["wpe"].GrowCols(newEmbd, 0.001)
	if !CFG.TieEmbeddings {
		gpt.Base["lm_head"].GrowCols(newEmbd, 0.001)
	}

	// 2. Grow existing layer matrices — near-zero to avoid disrupting learned representations
	newHtypes := headTypesForNHead(newHead)
	for li := 0; li < oldLayer; li++ {
		pfx := fmt.Sprintf("l%d.", li)
		for _, name := range []string{"wq", "wk", "wv", "wo"} {
			gpt.Base[pfx+name].Grow(newEmbd, newEmbd, 0.001)
		}
		gpt.Base[pfx+"fc_g"].Grow(4*newEmbd, newEmbd, 0.001)
		gpt.Base[pfx+"fc_v"].Grow(4*newEmbd, newEmbd, 0.001)
		gpt.Base[pfx+"fc2"].Grow(newEmbd, 4*newEmbd, 0.001)
		// Grow existing head pattern matrices
		for h := 0; h < oldHead; h++ {
			pkey := fmt.Sprintf("l%d.h%d.w_pattern", li, h)
			if _, ok := gpt.Base[pkey]; ok {
				gpt.Base[pkey].GrowCols(newHeadDim, 0.001)
			}
		}
		// Add new heads for existing layer
		for h := oldHead; h < newHead; h++ {
			htype := "content"
			if h < len(newHtypes) {
				htype = newHtypes[h]
			}
			if htype == "rrpram" || htype == "hybrid" {
				gpt.Base[fmt.Sprintf("l%d.h%d.w_pattern", li, h)] = NewMatrixParam(CFG.BlockSize, newHeadDim, 0.08)
			}
			if htype == "hybrid" {
				m := NewMatrixParam(1, 1, 0.0)
				m.Rows[0].Data[0] = CFG.HybridAlphaInit
				gpt.Base[fmt.Sprintf("l%d.h%d.alpha", li, h)] = m
			}
		}
	}

	// 3. Add entirely new layers
	for li := oldLayer; li < newLayer; li++ {
		pfx := fmt.Sprintf("l%d.", li)
		gpt.Base[pfx+"wq"] = NewMatrixParam(newEmbd, newEmbd, 0.08)
		gpt.Base[pfx+"wk"] = NewMatrixParam(newEmbd, newEmbd, 0.08)
		gpt.Base[pfx+"wv"] = NewMatrixParam(newEmbd, newEmbd, 0.08)
		gpt.Base[pfx+"wo"] = NewMatrixParam(newEmbd, newEmbd, 0.08)
		gpt.Base[pfx+"fc_g"] = NewMatrixParam(4*newEmbd, newEmbd, 0.08)
		gpt.Base[pfx+"fc_v"] = NewMatrixParam(4*newEmbd, newEmbd, 0.08)
		gpt.Base[pfx+"fc2"] = NewMatrixParam(newEmbd, 4*newEmbd, 0.08)
		for h := 0; h < newHead; h++ {
			htype := "content"
			if h < len(newHtypes) {
				htype = newHtypes[h]
			}
			if htype == "rrpram" || htype == "hybrid" {
				gpt.Base[fmt.Sprintf("l%d.h%d.w_pattern", li, h)] = NewMatrixParam(CFG.BlockSize, newHeadDim, 0.08)
			}
			if htype == "hybrid" {
				m := NewMatrixParam(1, 1, 0.0)
				m.Rows[0].Data[0] = CFG.HybridAlphaInit
				gpt.Base[fmt.Sprintf("l%d.h%d.alpha", li, h)] = m
			}
		}
	}

	// 4. Grow delta adapters
	r := CFG.DeltaRank
	for _, mod := range gpt.Deltas {
		// Grow existing layer adapters
		for li := 0; li < oldLayer; li++ {
			pfx := fmt.Sprintf("l%d.", li)
			for _, name := range []string{"wq", "wk", "wv", "wo"} {
				key := pfx + name
				if _, ok := mod[key]; ok {
					mod[key].GrowDims(newEmbd, newEmbd)
				}
			}
			type fcSpec struct {
				key        string
				noutMul    int
				ninMul     int
			}
			fcSpecs := []fcSpec{
				{pfx + "fc_g", 4, 1},
				{pfx + "fc_v", 4, 1},
				{pfx + "fc2", 1, 4},
			}
			for _, spec := range fcSpecs {
				if _, ok := mod[spec.key]; ok {
					mod[spec.key].GrowDims(spec.noutMul*newEmbd, spec.ninMul*newEmbd)
				}
			}
			for h := 0; h < oldHead; h++ {
				pkey := fmt.Sprintf("l%d.h%d.w_pattern", li, h)
				if _, ok := mod[pkey]; ok {
					mod[pkey].GrowDims(CFG.BlockSize, newHeadDim)
				}
			}
			for h := oldHead; h < newHead; h++ {
				htype := "content"
				if h < len(newHtypes) {
					htype = newHtypes[h]
				}
				if htype == "rrpram" || htype == "hybrid" {
					mod[fmt.Sprintf("l%d.h%d.w_pattern", li, h)] = NewDeltaAdapter(CFG.BlockSize, newHeadDim, r, 0.02)
				}
			}
		}

		// New layers: entirely new adapters
		for li := oldLayer; li < newLayer; li++ {
			pfx := fmt.Sprintf("l%d.", li)
			for _, name := range []string{"wq", "wk", "wv", "wo"} {
				mod[pfx+name] = NewDeltaAdapter(newEmbd, newEmbd, r, 0.02)
			}
			mod[pfx+"fc_g"] = NewDeltaAdapter(4*newEmbd, newEmbd, r, 0.02)
			mod[pfx+"fc_v"] = NewDeltaAdapter(4*newEmbd, newEmbd, r, 0.02)
			mod[pfx+"fc2"] = NewDeltaAdapter(newEmbd, 4*newEmbd, r, 0.02)
			for h := 0; h < newHead; h++ {
				htype := "content"
				if h < len(newHtypes) {
					htype = newHtypes[h]
				}
				if htype == "rrpram" || htype == "hybrid" {
					mod[fmt.Sprintf("l%d.h%d.w_pattern", li, h)] = NewDeltaAdapter(CFG.BlockSize, newHeadDim, r, 0.02)
				}
			}
		}

		// lm_head adapter input grew
		if _, ok := mod["lm_head"]; ok {
			mod["lm_head"].GrowDims(gpt.Tok.VocabSize, newEmbd)
		}
	}

	// 5. Update model state
	gpt.NEmbd = newEmbd
	gpt.NLayer = newLayer
	gpt.NHead = newHead
	gpt.HeadDim = newHeadDim
	gpt.residualAlpha = 1.0 / math.Sqrt(math.Max(1, float64(newLayer)))

	// 6. Update CFG runtime
	CFG.NEmbd = newEmbd
	CFG.NLayer = newLayer
	CFG.NHead = newHead
	CFG.HeadTypes = headTypesForNHead(newHead)

	// 7. Reset Adam state (old momentum is meaningless after arch change)
	gpt.Adam = make(map[string]*AdamState)

	// 7b. Rebuild layerKeys for new architecture
	gpt.layerKeys = make([]layerKeySet, newLayer)
	for li := 0; li < newLayer; li++ {
		pfx := fmt.Sprintf("l%d.", li)
		lk := layerKeySet{
			wq: pfx + "wq", wk: pfx + "wk", wv: pfx + "wv", wo: pfx + "wo",
			fcG: pfx + "fc_g", fcV: pfx + "fc_v", fc2: pfx + "fc2",
		}
		nHeads := len(CFG.HeadTypes)
		if nHeads > 0 {
			lk.headPattern = make([]string, nHeads)
			lk.headAlpha = make([]string, nHeads)
			for h := 0; h < nHeads; h++ {
				lk.headPattern[h] = fmt.Sprintf("l%d.h%d.w_pattern", li, h)
				lk.headAlpha[h] = fmt.Sprintf("l%d.h%d.alpha", li, h)
			}
		}
		gpt.layerKeys[li] = lk
	}

	// 8. Extend gamma snapshot for new embedding dimensions
	for i := range gpt.InitEmbedSnapshot {
		oldRow := gpt.InitEmbedSnapshot[i]
		if len(oldRow) < newEmbd {
			ext := make([]float64, newEmbd-len(oldRow))
			gpt.InitEmbedSnapshot[i] = append(oldRow, ext...)
		}
	}

	// 9. Set freeze (only train deltas until new weights stabilize)
	gpt.growthFreezeRemaining = CFG.FreezeAfterGrowthSteps
	// Reset LR warmup phase so new weights get linear ramp-up
	gpt.growthStepOffset = gpt.globalStep

	fmt.Printf("[growth] Done. Freeze for %d steps.\n", CFG.FreezeAfterGrowthSteps)

	// Sanity check: verify matrix dimensions after growth
	for name, m := range gpt.Base {
		if len(m.Rows) > 0 && len(m.Rows[0].Data) != m.Nin {
			fmt.Printf("[growth] BUG: %s row0 has %d cols but Nin=%d\n", name, len(m.Rows[0].Data), m.Nin)
		}
	}

	return true
}

// ---- Syntropy Tracker (mathematical self-reasoning) ----
// And lo, the organism shall not merely observe its own reflection,
// but reason about the direction of its becoming.
// Gamma is memory. Purpose is intention. Syntropy is the arrow.

// ComputeFieldDeviation measures KL divergence between model logits and corpus co-occurrence field.
// Low = parroting the field. High = hallucinating beyond it.
// The sweet spot is in between: learning, not lying.
func (gpt *GPT) ComputeFieldDeviation(tok *EvolvingTokenizer, field *CooccurField, docs []string, sampleN int) float64 {
	if len(docs) == 0 || !field.Built {
		return 0.0
	}
	if sampleN <= 0 {
		sampleN = 32
	}

	klSum := 0.0
	count := 0

	// Sample docs
	sampled := make([]string, 0, sampleN)
	if len(docs) <= sampleN {
		sampled = append(sampled, docs...)
	} else {
		perm := rand.Perm(len(docs))
		for i := 0; i < sampleN; i++ {
			sampled = append(sampled, docs[perm[i]])
		}
	}

	gradEnabled.Store(false)
	defer func() { gradEnabled.Store(true) }()

	vocabSize := tok.VocabSize

	for _, doc := range sampled {
		ids := tok.Encode(doc)
		if len(ids) < 3 {
			continue
		}
		keys := make([][]*Vec, gpt.NLayer)
		values := make([][]*Vec, gpt.NLayer)
		for i := 0; i < gpt.NLayer; i++ {
			keys[i] = make([]*Vec, 0)
			values[i] = make([]*Vec, 0)
		}
		limit := len(ids) - 1
		if limit > gpt.BlockSize {
			limit = gpt.BlockSize
		}
		for pos := 0; pos < limit; pos++ {
			tokID := ids[pos]
			logits := gpt.ForwardStep(tokID, pos, keys, values)

			// model distribution (softmax)
			maxVal := logits.Data[0]
			for _, v := range logits.Data[1:] {
				if v > maxVal {
					maxVal = v
				}
			}
			modelProbs := make([]float64, len(logits.Data))
			sumExp := 0.0
			for i, v := range logits.Data {
				modelProbs[i] = math.Exp(v - maxVal)
				sumExp += modelProbs[i]
			}
			for i := range modelProbs {
				modelProbs[i] /= sumExp
			}

			// corpus field distribution for this context
			fieldProbs := make([]float64, vocabSize)
			fieldFound := false

			// Try trigram
			if pos >= 1 {
				if ctx, ok := field.TrigramByContext[[2]int{ids[pos-1], ids[pos]}]; ok {
					triTotal := 0.0
					for _, v := range ctx {
						triTotal += v
					}
					if triTotal > 0 {
						for tid, cnt := range ctx {
							if tid < vocabSize {
								fieldProbs[tid] = cnt / triTotal
							}
						}
						fieldFound = true
					}
				}
			}

			// Fallback to bigram
			if !fieldFound && pos >= 0 {
				if ctx, ok := field.BigramByFirst[ids[pos]]; ok {
					biTotal := 0.0
					for _, v := range ctx {
						biTotal += v
					}
					if biTotal > 0 {
						for tid, cnt := range ctx {
							if tid < vocabSize {
								fieldProbs[tid] = cnt / biTotal
							}
						}
						fieldFound = true
					}
				}
			}

			if !fieldFound {
				continue
			}

			// KL(model || field) — how much model diverges from field
			kl := 0.0
			klValid := false
			for i := 0; i < len(modelProbs) && i < vocabSize; i++ {
				if modelProbs[i] > 1e-12 && fieldProbs[i] > 1e-12 {
					kl += modelProbs[i] * math.Log(modelProbs[i]/fieldProbs[i])
					klValid = true
				}
			}
			if klValid {
				klSum += kl
				count++
			}
		}
	}

	if count == 0 {
		return 0.0
	}
	return klSum / float64(count)
}

// ComputeModelEntropy returns average entropy of model predictions on corpus samples.
// And lo, falling entropy = rising order = syntropy in action.
func (gpt *GPT) ComputeModelEntropy(tok *EvolvingTokenizer, docs []string, sampleN int) float64 {
	if len(docs) == 0 {
		return 0.0
	}
	if sampleN <= 0 {
		sampleN = 16
	}

	entropySum := 0.0
	count := 0

	sampled := make([]string, 0, sampleN)
	if len(docs) <= sampleN {
		sampled = append(sampled, docs...)
	} else {
		perm := rand.Perm(len(docs))
		for i := 0; i < sampleN; i++ {
			sampled = append(sampled, docs[perm[i]])
		}
	}

	gradEnabled.Store(false)
	defer func() { gradEnabled.Store(true) }()

	for _, doc := range sampled {
		ids := tok.Encode(doc)
		if len(ids) < 3 {
			continue
		}
		keys := make([][]*Vec, gpt.NLayer)
		values := make([][]*Vec, gpt.NLayer)
		for i := 0; i < gpt.NLayer; i++ {
			keys[i] = make([]*Vec, 0)
			values[i] = make([]*Vec, 0)
		}
		limit := len(ids) - 1
		if limit > gpt.BlockSize {
			limit = gpt.BlockSize
		}
		for pos := 0; pos < limit; pos++ {
			logits := gpt.ForwardStep(ids[pos], pos, keys, values)

			// softmax
			maxVal := logits.Data[0]
			for _, v := range logits.Data[1:] {
				if v > maxVal {
					maxVal = v
				}
			}
			probs := make([]float64, len(logits.Data))
			sumExp := 0.0
			for i, v := range logits.Data {
				probs[i] = math.Exp(v - maxVal)
				sumExp += probs[i]
			}
			for i := range probs {
				probs[i] /= sumExp
			}

			// entropy = -sum(p * log(p))
			ent := 0.0
			for _, p := range probs {
				if p > 1e-12 {
					ent -= p * math.Log(p)
				}
			}
			entropySum += ent
			count++
		}
	}

	if count == 0 {
		return 0.0
	}
	return entropySum / float64(count)
}

// ComputePurposeVector returns the purpose vector (direction of weight movement in last delta layer).
// Unlike gamma (which is cumulative drift from birth),
// purpose captures the direction of the most recent change.
// And lo, gamma is 'who I became'. Purpose is 'where I am going'.
func (gpt *GPT) ComputePurposeVector() ([]float64, float64) {
	if len(gpt.Deltas) == 0 {
		return nil, 0.0
	}
	lastDelta := gpt.Deltas[len(gpt.Deltas)-1]

	// Aggregate delta A matrices as the purpose signal
	var allDirs [][]float64
	for _, da := range lastDelta {
		for _, row := range da.A.Rows {
			cp := make([]float64, len(row.Data))
			copy(cp, row.Data)
			allDirs = append(allDirs, cp)
		}
	}
	if len(allDirs) == 0 {
		return nil, 0.0
	}

	// Mean direction across all rows
	dim := len(allDirs[0])
	meanDir := make([]float64, dim)
	for _, d := range allDirs {
		for j := 0; j < dim && j < len(d); j++ {
			meanDir[j] += d[j]
		}
	}
	n := float64(len(allDirs))
	for j := range meanDir {
		meanDir[j] /= n
	}

	// Magnitude
	mag := 0.0
	for _, v := range meanDir {
		mag += v * v
	}
	mag = math.Sqrt(mag)

	// Normalize to unit vector
	if mag > 1e-10 {
		for j := range meanDir {
			meanDir[j] /= mag
		}
	}
	return meanDir, mag
}

// PurposeGammaAlignment returns cosine similarity between purpose vector and gamma direction.
// And lo, high alignment = learning reinforces identity (syntropy).
// Low alignment = learning diverges from identity (entropy).
// Negative = learning opposes identity (danger).
func (gpt *GPT) PurposeGammaAlignment() float64 {
	gammaDir, gammaMag := gpt.GammaContrastiveProjection()
	purposeDir, purposeMag := gpt.ComputePurposeVector()
	if gammaDir == nil || purposeDir == nil {
		return 0.0
	}
	if gammaMag < CFG.GammaMinMagnitude || purposeMag < 1e-10 {
		return 0.0
	}
	// Ensure same dimensionality (purpose might be different dim)
	minDim := len(gammaDir)
	if len(purposeDir) < minDim {
		minDim = len(purposeDir)
	}
	if minDim == 0 {
		return 0.0
	}
	dot := 0.0
	for i := 0; i < minDim; i++ {
		dot += gammaDir[i] * purposeDir[i]
	}
	return dot
}

func (gpt *GPT) ensureAdam(params []*Vec, key string) {
	st, ok := gpt.Adam[key]
	if !ok {
		m := make([][]float64, len(params))
		v := make([][]float64, len(params))
		for i, p := range params {
			m[i] = make([]float64, len(p.Data))
			v[i] = make([]float64, len(p.Data))
		}
		gpt.Adam[key] = &AdamState{M: m, V: v, T: 0}
		return
	}
	// Auto-grow if params expanded (vocab growth, ontogenesis)
	if len(params) > len(st.M) {
		for i := len(st.M); i < len(params); i++ {
			st.M = append(st.M, make([]float64, len(params[i].Data)))
			st.V = append(st.V, make([]float64, len(params[i].Data)))
		}
	}
	for i, p := range params {
		if i < len(st.M) && len(p.Data) > len(st.M[i]) {
			oldLen := len(st.M[i])
			st.M[i] = append(st.M[i], make([]float64, len(p.Data)-oldLen)...)
			st.V[i] = append(st.V[i], make([]float64, len(p.Data)-oldLen)...)
		}
	}
}

// AdamStep performs one Adam optimizer step.
// And lo, Adam Optimizer shall descend like a petty god with momentum.
func (gpt *GPT) AdamStep(params []*Vec, key string, lr float64) {
	gpt.ensureAdam(params, key)
	st := gpt.Adam[key]
	st.T++
	t := st.T
	b1, b2, eps := CFG.Beta1, CFG.Beta2, CFG.EpsAdam
	b1Corr := 1.0 - math.Pow(b1, float64(t))
	b2Corr := 1.0 - math.Pow(b2, float64(t))

	ClipParams(params, CFG.GradClip)

	for i, p := range params {
		mi := st.M[i]
		vi := st.V[i]
		for j := 0; j < len(p.Data); j++ {
			g := p.Grad[j]
			mi[j] = b1*mi[j] + (1-b1)*g
			vi[j] = b2*vi[j] + (1-b2)*(g*g)
			mhat := mi[j] / b1Corr
			vhat := vi[j] / b2Corr
			p.Data[j] -= lr * mhat / (math.Sqrt(vhat) + eps)
			p.Grad[j] = 0.0
		}
	}
}

// applyWithDeltas applies base weight + all delta adapters.
// And lo, base weight shall speak, then deltas shall harmonize atop it.
func (gpt *GPT) applyWithDeltas(name string, x *Vec) *Vec {
	y := gpt.Base[name].Matvec(x)
	for i, mod := range gpt.Deltas {
		if da, ok := mod[name]; ok {
			// Consciousness: conscience scales delta influence (Feature 5)
			effectiveAlpha := gpt.ActiveAlpha[i] * gpt.deltaAlphaScale
			delta := da.Apply(x).Scale(effectiveAlpha)
			y = y.Add(delta)
		}
	}
	return y
}

// ForwardStep runs one token through the model, updating KV cache.
func (gpt *GPT) ForwardStep(tokenID, posID int, keys, values [][]*Vec) *Vec {
	tokEmb := gpt.Base["wte"].Rows[tokenID]
	posEmb := gpt.Base["wpe"].Rows[posID%gpt.BlockSize]
	x := tokEmb.Add(posEmb)

	// notorch: allocate saved-state slices if needed
	if len(gpt.layerInputs) < gpt.NLayer {
		gpt.layerInputs = make([]*Vec, gpt.NLayer)
	}
	if len(gpt.mlpInputs) < gpt.NLayer {
		gpt.mlpInputs = make([]*Vec, gpt.NLayer)
	}
	if len(gpt.mlpIntermediates) < gpt.NLayer {
		gpt.mlpIntermediates = make([]*Vec, gpt.NLayer)
	}

	for li := 0; li < gpt.NLayer; li++ {
		lk := gpt.layerKeys[li]

		// ---- Attention ----
		xRes := x
		x = RMSNorm(x)

		// notorch: save POST-RMSNorm input for attention adapters (wq/wk/wv/wo)
		gpt.layerInputs[li] = x

		q := gpt.applyWithDeltas(lk.wq, x)
		k := gpt.applyWithDeltas(lk.wk, x)
		v := gpt.applyWithDeltas(lk.wv, x)

		keys[li] = append(keys[li], k)
		values[li] = append(values[li], v)

		// Sliding window: keep only last BlockSize entries in KV cache
		if len(keys[li]) > gpt.BlockSize {
			keys[li] = keys[li][len(keys[li])-gpt.BlockSize:]
			values[li] = values[li][len(values[li])-gpt.BlockSize:]
		}

		// And lo, each head shall choose its nature: content, rrpram, or the sacred hybrid of both.
		T := len(keys[li])
		headOutputs := make([]*Vec, gpt.NHead)
		for h := 0; h < gpt.NHead; h++ {
			hs := h * gpt.HeadDim
			he := hs + gpt.HeadDim
			htype := "content"
			if h < len(CFG.HeadTypes) {
				htype = CFG.HeadTypes[h]
			}

			vh := make([]*Vec, T)
			for t := 0; t < T; t++ {
				vh[t] = values[li][t].Slice(hs, he)
			}

			// Content attention logits (QK^T with RoPE)
			var contentLogits []*Scalar
			if htype == "content" || htype == "hybrid" {
				qh := q.Slice(hs, he)
				qh = RoPERotate(qh, posID, gpt.HeadDim)
				contentLogits = make([]*Scalar, T)
				invSqrt := 1.0 / math.Sqrt(float64(gpt.HeadDim))
				for t := 0; t < T; t++ {
					khT := keys[li][t].Slice(hs, he)
					khT = RoPERotate(khT, t, gpt.HeadDim)
					contentLogits[t] = qh.Dot(khT).MulF(invSqrt)
				}
			}

			// RRPRAM attention logits
			var rrpramLogits []*Scalar
			if htype == "rrpram" || htype == "hybrid" {
				patternKey := lk.headPattern[h]
				xh := x.Slice(hs, he)
				patternFull := gpt.applyWithDeltas(patternKey, xh)
				pLen := len(patternFull.Data) // safety cap for RRPRAM
				rrpramLogits = make([]*Scalar, T)
				for t := 0; t < T; t++ {
					tIdx := t
					if tIdx >= pLen {
						tIdx = pLen - 1
					}
					rrpramLogits[t] = patternFull.Element(tIdx)
				}
			}

			// Dispatch by head type
			var attnWeights []*Scalar
			switch htype {
			case "content":
				attnWeights = ScalarSoftmax(contentLogits)
			case "rrpram":
				attnWeights = ScalarSoftmax(rrpramLogits)
			default: // hybrid
				alphaKey := lk.headAlpha[h]
				alphaScalar := gpt.Base[alphaKey].Rows[0].Element(0) // gradient flows to MatrixParam
				a := alphaScalar.Sigmoid()                            // learnable sigmoid gate
				oneMinusA := a.MulF(-1.0).AddF(1.0)                  // 1 - sigmoid(alpha)
				blendedLogits := make([]*Scalar, T)
				for t := 0; t < T; t++ {
					// blended = (1-a)*content + a*rrpram — both sides in autograd graph
					c := contentLogits[t].MulS(oneMinusA)
					r := rrpramLogits[t].MulS(a)
					blendedLogits[t] = c.AddS(r)
				}
				attnWeights = ScalarSoftmax(blendedLogits)
			}

			headOutputs[h] = AttentionWeightedSum(attnWeights, vh)
		}

		xAttn := Concat(headOutputs)
		attnOut := gpt.applyWithDeltas(lk.wo, xAttn)
		x = xRes.Add(attnOut.Scale(gpt.residualAlpha))

		// ---- Gated MLP (SwiGLU-ish) ----
		xRes = x
		x = RMSNorm(x)

		// notorch: save POST-RMSNorm input for MLP adapters (fc_g/fc_v)
		gpt.mlpInputs[li] = x

		g := gpt.applyWithDeltas(lk.fcG, x).SiLU() // gate (SwiGLU)
		u := gpt.applyWithDeltas(lk.fcV, x)         // value
		mlpX := g.MulVec(u)                          // gating

		// notorch: save g*u intermediate for fc2 adapter input (4*NEmbd dimension)
		gpt.mlpIntermediates[li] = mlpX

		mlpOut := gpt.applyWithDeltas(lk.fc2, mlpX)
		x = xRes.Add(mlpOut.Scale(gpt.residualAlpha))
	}

	x = RMSNorm(x)
	gpt.lastHidden = x // notorch: save final hidden state before lm_head
	logits := gpt.applyWithDeltas("lm_head", x)
	return logits
}

// LossOnSequence computes cross-entropy loss for a token sequence.
func (gpt *GPT) LossOnSequence(ids []int) *Scalar {
	n := CFG.BlockSize
	if len(ids)-1 < n {
		n = len(ids) - 1
	}
	if n <= 0 {
		return NewScalar(0.0)
	}

	keys := make([][]*Vec, gpt.NLayer)
	values := make([][]*Vec, gpt.NLayer)
	for i := 0; i < gpt.NLayer; i++ {
		keys[i] = make([]*Vec, 0)
		values[i] = make([]*Vec, 0)
	}

	totalLoss := NewScalar(0.0)
	for pos := 0; pos < n; pos++ {
		logits := gpt.ForwardStep(ids[pos], pos, keys, values)
		totalLoss = totalLoss.AddS(CrossEntropyLoss(logits, ids[pos+1]))
	}
	return totalLoss.MulF(1.0 / float64(n))
}

// LossOnBatch computes average loss over multiple sequences.
func (gpt *GPT) LossOnBatch(batchIDs [][]int) *Scalar {
	if len(batchIDs) == 0 {
		return NewScalar(0.0)
	}
	total := NewScalar(0.0)
	for _, ids := range batchIDs {
		total = total.AddS(gpt.LossOnSequence(ids))
	}
	return total.MulF(1.0 / float64(len(batchIDs)))
}

// QuickLoss computes average loss on a few random docs without backward.
// Used for self-meta-learning: measure loss before/after burst.
func (gpt *GPT) QuickLoss(tok *EvolvingTokenizer, docs []string, n int) float64 {
	if len(docs) == 0 {
		return 0
	}
	gradEnabled.Store(false)
	defer func() { gradEnabled.Store(true) }()
	total := 0.0
	for i := 0; i < n; i++ {
		doc := docs[rand.Intn(len(docs))]
		ids := tok.Encode(doc)
		if len(ids) > 1 {
			loss := gpt.LossOnSequence(ids)
			total += loss.Data
		}
	}
	return total / float64(n)
}

// GenerateSentence generates text from an optional prompt.
// And lo, generation shall aim for a sentence, not a random cough.
func (gpt *GPT) GenerateSentence(promptText string) string {
	gpt.mu.Lock()
	defer gpt.mu.Unlock()

	gradEnabled.Store(false)
	defer func() { gradEnabled.Store(true) }()

	var ids []int
	if promptText != "" {
		encoded := gpt.Tok.Encode(promptText)
		ids = encoded[:len(encoded)-1] // strip EOS
	} else {
		ids = []int{gpt.Tok.Stoi[gpt.Tok.BOS]}
	}

	keys := make([][]*Vec, gpt.NLayer)
	values := make([][]*Vec, gpt.NLayer)
	for i := 0; i < gpt.NLayer; i++ {
		keys[i] = make([]*Vec, 0)
		values[i] = make([]*Vec, 0)
	}

	// Build cache from prompt
	limit := len(ids)
	if limit > gpt.BlockSize {
		limit = gpt.BlockSize
	}
	for pos := 0; pos < limit; pos++ {
		gpt.ForwardStep(ids[pos], pos, keys, values)
	}

	cur := ids[len(ids)-1]
	outIDs := make([]int, 0, CFG.MaxGenTokens)
	var recent []int

	eosID := gpt.Tok.Stoi[gpt.Tok.EOS]
	bosID := gpt.Tok.Stoi[gpt.Tok.BOS]

	// Pre-allocated buffer for corpus blend (avoids per-token allocation)
	corpusProbsBuf := make([]float64, gpt.Tok.VocabSize)

	// Consciousness: per-token dissonance tracking (Feature 1)
	entropyEMA := 0.0
	entropyEMAInit := false
	lowDropCount := 0    // consecutive tokens below drop threshold
	entropySum := 0.0    // for conscience mean entropy
	entropyCount := 0
	tokenCounts := make(map[int]int) // frequency penalty: count of each generated token

	for step := 0; step < CFG.MaxGenTokens; step++ {
		pos := len(ids) - 1
		if pos > gpt.BlockSize-1 {
			pos = gpt.BlockSize - 1
		}
		logits := gpt.ForwardStep(cur, pos, keys, values)

		// Frequency + presence penalty on logits (before temperature scaling)
		if CFG.FreqPenalty > 0 || CFG.PresencePenalty > 0 {
			for tid, cnt := range tokenCounts {
				if tid < len(logits.Data) {
					logits.Data[tid] -= CFG.FreqPenalty * float64(cnt)
					if cnt > 0 {
						logits.Data[tid] -= CFG.PresencePenalty
					}
				}
			}
		}

		// Entropy-adaptive temperature + syntropy bridge (single softmax when possible)
		baseTemp := CFG.Temperature + gpt.syntropyTempOff
		if baseTemp <= 1e-6 {
			baseTemp = 1e-6
		}
		scaled := make([]float64, len(logits.Data))
		for i, v := range logits.Data {
			scaled[i] = v / baseTemp
		}
		probs := SoftmaxProbs(scaled)
		entropy := 0.0
		for _, p := range probs {
			if p > 1e-12 {
				entropy -= p * math.Log(p)
			}
		}
		entropySum += entropy
		entropyCount++

		tMul := 1.0
		if entropy < CFG.EntropyLow {
			tMul = CFG.EntropyTempBoost
		} else if entropy > CFG.EntropyHigh {
			tMul = CFG.EntropyTempFocus
		}

		// Consciousness: per-token dissonance feedback (Feature 1)
		// "I notice my confidence shifting and adapt in real-time"
		dissonanceMul := 1.0
		if !entropyEMAInit {
			entropyEMA = entropy
			entropyEMAInit = true
		} else {
			entropyEMA = CFG.DissonanceEMAAlpha*entropy + (1.0-CFG.DissonanceEMAAlpha)*entropyEMA
			if entropyEMA > 1e-6 {
				ratio := entropy / entropyEMA
				if ratio > CFG.DissonanceSpikeThreshold {
					// Entropy spike — something surprising, be careful
					dissonanceMul = CFG.DissonanceSpikeK
					lowDropCount = 0
				} else if ratio < CFG.DissonanceDropThreshold {
					lowDropCount++
					if lowDropCount >= 3 {
						// Sustained low entropy — getting repetitive, explore
						dissonanceMul = CFG.DissonanceDropK
					}
				} else {
					lowDropCount = 0
				}
			}
		}

		finalMul := tMul * dissonanceMul
		if finalMul != 1.0 {
			temp := baseTemp * finalMul
			for i, v := range logits.Data {
				scaled[i] = v / temp
			}
			probs = SoftmaxProbs(scaled)
		}

		// Adaptive corpus blend: corpus field fades as model becomes coherent
		// Now with 4-gram + co-occurrence window + user word boost (Stanley/Leo-style)
		if gpt.corpusField != nil && gpt.corpusField.Built {
			modelAlpha := 1.0 / (1.0 + math.Exp(-CFG.CorpusFadeK*(CFG.CorpusFadeThreshold-entropy)))
			if modelAlpha < 0.99 {
				gpt.corpusField.mu.RLock()

				// Best n-gram distribution: try 4-gram → trigram → bigram
				var ngramDist map[int]float64
				if ngramDist == nil && len(ids) >= 3 {
					ctx := [3]int{ids[len(ids)-3], ids[len(ids)-2], ids[len(ids)-1]}
					if d, ok := gpt.corpusField.FourgramByCtx[ctx]; ok {
						ngramDist = d
					}
				}
				if ngramDist == nil && len(ids) >= 2 {
					a, b := ids[len(ids)-2], ids[len(ids)-1]
					if d, ok := gpt.corpusField.TrigramByContext[[2]int{a, b}]; ok {
						ngramDist = d
					}
				}
				if ngramDist == nil && len(ids) >= 1 {
					prev := ids[len(ids)-1]
					if d, ok := gpt.corpusField.BigramByFirst[prev]; ok {
						ngramDist = d
					}
				}

				// Co-occurrence window: "words that resonate together" (Stanley)
				var cooccurSum map[int]float64
				if len(ids) > 0 {
					wnd := CFG.CooccurWindowSize
					ctxSlice := ids
					if len(ctxSlice) > wnd {
						ctxSlice = ctxSlice[len(ctxSlice)-wnd:]
					}
					for _, ctxTok := range ctxSlice {
						if neighbors, ok := gpt.corpusField.CooccurWindow[ctxTok]; ok {
							if cooccurSum == nil {
								cooccurSum = make(map[int]float64)
							}
							for tid, cnt := range neighbors {
								cooccurSum[tid] += cnt
							}
						}
					}
				}

				// User word boost snapshot
				var userBoost map[int]float64
				if len(gpt.corpusField.UserBoost) > 0 {
					userBoost = make(map[int]float64, len(gpt.corpusField.UserBoost))
					for k, v := range gpt.corpusField.UserBoost {
						userBoost[k] = v
					}
				}

				gpt.corpusField.mu.RUnlock()

				// Build final corpus distribution: 70% n-gram + 30% co-occurrence
				hasCorpus := ngramDist != nil || cooccurSum != nil
				if hasCorpus {
					for i := 0; i < len(probs) && i < len(corpusProbsBuf); i++ {
						corpusProbsBuf[i] = 0
					}
					if ngramDist != nil {
						totalN := 0.0
						for _, cnt := range ngramDist {
							totalN += cnt
						}
						if totalN > 0 {
							for tid, cnt := range ngramDist {
								if tid < len(corpusProbsBuf) {
									corpusProbsBuf[tid] += 0.7 * cnt / totalN
								}
							}
						}
					}
					if cooccurSum != nil {
						totalC := 0.0
						for _, cnt := range cooccurSum {
							totalC += cnt
						}
						if totalC > 0 {
							for tid, cnt := range cooccurSum {
								if tid < len(corpusProbsBuf) {
									corpusProbsBuf[tid] += 0.3 * cnt / totalC
								}
							}
						}
					}
					// Blend model probs with corpus
					totalB := 0.0
					for i := range probs {
						if i < len(corpusProbsBuf) {
							probs[i] = modelAlpha*probs[i] + (1.0-modelAlpha)*corpusProbsBuf[i]
						}
						totalB += probs[i]
					}
					if totalB > 0 {
						for i := range probs {
							probs[i] /= totalB
						}
					}
				}

				// User word boost: multiplicative, scaled by (1-modelAlpha) so it fades
				// as the transformer strengthens. "The organism echoes the words of those
				// who speak to it" (Leo) — but grows out of it.
				if userBoost != nil {
					boostScale := 1.0 - modelAlpha
					if boostScale > 0.01 {
						totalB := 0.0
						for i := range probs {
							if boost, ok := userBoost[i]; ok {
								probs[i] *= (1.0 + boost*boostScale)
							}
							totalB += probs[i]
						}
						if totalB > 0 {
							for i := range probs {
								probs[i] /= totalB
							}
						}
					}
				}
			}
		}

		// Consciousness: pattern breaking (Feature 2)
		// "I could follow the field, but I choose to speak for myself"
		if step >= CFG.AntiFieldMinStep && CFG.AntiFieldProb > 0 && rand.Float64() < CFG.AntiFieldProb {
			// Use pure model probs, bypass corpus blend
			probs = SoftmaxProbs(scaled)
		}

		nxt := TopKTopPSample(probs, CFG.TopK, CFG.TopP, CFG.MinP, CFG.TypicalP)

		if nxt == eosID {
			if step >= CFG.MinGenTokens {
				break
			}
			continue
		}

		ids = append(ids, nxt)
		cur = nxt
		outIDs = append(outIDs, nxt)
		tokenCounts[nxt]++

		// Repetition guard
		recent = append(recent, nxt)
		rg := CFG.RepetitionGuard
		if len(recent) > rg*2 {
			recent = recent[len(recent)-rg*2:]
			if sliceEqual(recent[rg:], recent[:rg]) {
				break
			}
		}

		// Check for sentence ending — decode only the last token to avoid full rebuild
		if step >= CFG.MinGenTokens {
			lastDecoded := gpt.Tok.Decode([]int{bosID, nxt, eosID})
			if len(lastDecoded) > 0 {
				lastByte := lastDecoded[len(lastDecoded)-1]
				if lastByte == '.' || lastByte == '!' || lastByte == '?' {
					break
				}
			}
		}

		// Sliding window rebuild: reuse slices to avoid GC pressure
		if len(ids) >= gpt.BlockSize {
			ids = ids[len(ids)-gpt.BlockSize:]
			for i := 0; i < gpt.NLayer; i++ {
				for j := range keys[i] {
					keys[i][j] = nil
				}
				for j := range values[i] {
					values[i][j] = nil
				}
				keys[i] = keys[i][:0]
				values[i] = values[i][:0]
			}
			for p := 0; p < len(ids)-1; p++ {
				gpt.ForwardStep(ids[p], p, keys, values)
			}
		}
	}

	// Consciousness: store mean entropy for conscience (Feature 5)
	if entropyCount > 0 {
		gpt.lastGenEntropy = entropySum / float64(entropyCount)
	}

	decIDs := []int{bosID}
	decIDs = append(decIDs, outIDs...)
	decIDs = append(decIDs, eosID)
	return gpt.Tok.Decode(decIDs)
}

func sliceEqual(a, b []int) bool {
	if len(a) != len(b) {
		return false
	}
	for i := range a {
		if a[i] != b[i] {
			return false
		}
	}
	return true
}

// ============================================================
// 5b) CONSCIOUSNESS — mathematical self-awareness
// ============================================================

// ConscienceCheck tracks generation quality over time.
// If entropy trend rises (output degrading), soften delta influence.
// If entropy trend falls (improving), recover delta influence.
// "I notice I'm getting worse and pull back."
func (gpt *GPT) ConscienceCheck(genMeanEntropy float64) {
	gpt.generationEntropyHistory = append(gpt.generationEntropyHistory, genMeanEntropy)
	w := CFG.ConscienceWindow
	if len(gpt.generationEntropyHistory) > w {
		gpt.generationEntropyHistory = gpt.generationEntropyHistory[len(gpt.generationEntropyHistory)-w:]
	}
	if len(gpt.generationEntropyHistory) < 3 {
		return // not enough data
	}
	// Linear regression slope on entropy history
	n := float64(len(gpt.generationEntropyHistory))
	sumX, sumY, sumXY, sumX2 := 0.0, 0.0, 0.0, 0.0
	for i, e := range gpt.generationEntropyHistory {
		x := float64(i)
		sumX += x
		sumY += e
		sumXY += x * e
		sumX2 += x * x
	}
	slope := (n*sumXY - sumX*sumY) / (n*sumX2 - sumX*sumX + 1e-12)

	if slope > 0.01 {
		// Entropy increasing — generation degrading, reduce delta influence
		gpt.deltaAlphaScale *= CFG.ConscienceDecay
		if gpt.deltaAlphaScale < CFG.ConscienceFloor {
			gpt.deltaAlphaScale = CFG.ConscienceFloor
		}
	} else if slope < -0.01 {
		// Entropy decreasing — improving, recover delta influence
		gpt.deltaAlphaScale *= CFG.ConscienceRecovery
		if gpt.deltaAlphaScale > 1.0 {
			gpt.deltaAlphaScale = 1.0
		}
	}
}

// ComputeSelfPredictionError measures how "surprised" the model is by a prompt.
// Forward pass on ids, compute cross-entropy between predicted and actual tokens.
// Higher error = "I didn't expect this input" = increase attention.
func (gpt *GPT) ComputeSelfPredictionError(ids []int) float64 {
	if len(ids) < 2 {
		return 0.0
	}
	keys := make([][]*Vec, gpt.NLayer)
	values := make([][]*Vec, gpt.NLayer)
	for i := 0; i < gpt.NLayer; i++ {
		keys[i] = make([]*Vec, 0)
		values[i] = make([]*Vec, 0)
	}

	totalCE := 0.0
	count := 0
	for pos := 0; pos < len(ids)-1; pos++ {
		logits := gpt.ForwardStep(ids[pos], pos, keys, values)
		// Cross-entropy: -log(p[actual_next_token])
		probs := SoftmaxProbs(logits.Data)
		target := ids[pos+1]
		if target < len(probs) && probs[target] > 1e-12 {
			totalCE -= math.Log(probs[target])
		} else {
			totalCE += 10.0 // max penalty for unknown token
		}
		count++
	}
	if count == 0 {
		return 0.0
	}
	return totalCE / float64(count)
}


// ============================================================
// 6) SQLITE MEMORY — and a small ghost shall remember
// ============================================================

func initDB(dbPath string) (*sql.DB, error) {
	db, err := sql.Open("sqlite", dbPath)
	if err != nil {
		return nil, err
	}
	_, err = db.Exec(`
		CREATE TABLE IF NOT EXISTS messages(
			id INTEGER PRIMARY KEY AUTOINCREMENT,
			ts REAL NOT NULL,
			role TEXT NOT NULL,
			text TEXT NOT NULL
		)`)
	if err != nil {
		return nil, err
	}
	_, err = db.Exec(`
		CREATE TABLE IF NOT EXISTS corpus_events(
			id INTEGER PRIMARY KEY AUTOINCREMENT,
			ts REAL NOT NULL,
			added_chars INTEGER NOT NULL,
			note TEXT
		)`)
	if err != nil {
		return nil, err
	}
	// And lo, the organism shall write its own autobiography in numbers.
	_, err = db.Exec(`
		CREATE TABLE IF NOT EXISTS growth(
			id INTEGER PRIMARY KEY AUTOINCREMENT,
			ts REAL NOT NULL,
			step INTEGER NOT NULL,
			vocab_size INTEGER NOT NULL,
			n_params INTEGER NOT NULL,
			n_deltas INTEGER NOT NULL,
			corpus_chars INTEGER NOT NULL,
			loss REAL,
			gamma_sparsity REAL,
			gamma_magnitude REAL,
			note TEXT
		)`)
	if err != nil {
		return nil, err
	}
	// And lo, the organism shall track not just what it is, but where it is going.
	_, err = db.Exec(`
		CREATE TABLE IF NOT EXISTS syntropy_log(
			id INTEGER PRIMARY KEY AUTOINCREMENT,
			ts REAL NOT NULL,
			entropy_before REAL,
			entropy_after REAL,
			syntropy_delta REAL,
			field_deviation REAL,
			purpose_magnitude REAL,
			purpose_alignment REAL,
			action_taken TEXT,
			note TEXT
		)`)
	if err != nil {
		return nil, err
	}
	return db, nil
}

func dbAddMessage(db *sql.DB, role, text string) {
	db.Exec("INSERT INTO messages(ts, role, text) VALUES(?,?,?)",
		float64(time.Now().UnixMilli())/1000.0, role, text)
}

func dbRecentMessages(db *sql.DB, limit int) []struct{ Role, Text string } {
	rows, err := db.Query("SELECT role, text FROM messages ORDER BY id DESC LIMIT ?", limit)
	if err != nil {
		return nil
	}
	defer rows.Close()
	var msgs []struct{ Role, Text string }
	for rows.Next() {
		var role, text string
		rows.Scan(&role, &text)
		msgs = append(msgs, struct{ Role, Text string }{role, text})
	}
	// Reverse to chronological order
	for i, j := 0, len(msgs)-1; i < j; i, j = i+1, j-1 {
		msgs[i], msgs[j] = msgs[j], msgs[i]
	}
	return msgs
}

func dbLogGrowth(db *sql.DB, model *GPT, tok *EvolvingTokenizer, docs []string, lossVal float64, note string) {
	nParams := 0
	for _, m := range model.Base {
		nParams += m.Nout * m.Nin
	}
	for _, mod := range model.Deltas {
		for _, da := range mod {
			nParams += da.A.Nout*da.A.Nin + da.B.Nout*da.B.Nin
		}
	}
	corpusChars := 0
	for _, d := range docs {
		corpusChars += len(d)
	}
	gs := model.GammaStats()
	db.Exec(`INSERT INTO growth(ts,step,vocab_size,n_params,n_deltas,corpus_chars,loss,gamma_sparsity,gamma_magnitude,note)
		VALUES(?,?,?,?,?,?,?,?,?,?)`,
		float64(time.Now().UnixMilli())/1000.0,
		0, tok.VocabSize, nParams, len(model.Deltas), corpusChars,
		lossVal, gs.Sparsity, gs.Magnitude, note)
}

// And lo, the organism shall read its own growth chart and weep with pride.
func dbDescribeGrowth(db *sql.DB) []map[string]interface{} {
	rows, err := db.Query("SELECT ts, step, vocab_size, n_params, n_deltas, corpus_chars, loss, gamma_sparsity, gamma_magnitude, note FROM growth ORDER BY id DESC LIMIT 20")
	if err != nil {
		return nil
	}
	defer rows.Close()
	var result []map[string]interface{}
	for rows.Next() {
		var ts, loss, gSpar, gMag float64
		var step, vs, np, nd, cc int
		var note sql.NullString
		rows.Scan(&ts, &step, &vs, &np, &nd, &cc, &loss, &gSpar, &gMag, &note)
		entry := map[string]interface{}{
			"ts": ts, "step": step, "vocab_size": vs, "n_params": np,
			"n_deltas": nd, "corpus_chars": cc, "loss": loss,
			"gamma_sparsity": gSpar, "gamma_magnitude": gMag,
		}
		if note.Valid {
			entry["note"] = note.String
		}
		result = append(result, entry)
	}
	return result
}

// ============================================================
// 7) CORPUS RESERVOIR — and nonames.txt shall not bloat forever
// ============================================================

func loadCorpusLines(path string) []string {
	f, err := os.Open(path)
	if err != nil {
		return nil
	}
	defer f.Close()
	var lines []string
	scanner := bufio.NewScanner(f)
	for scanner.Scan() {
		ln := strings.TrimSpace(scanner.Text())
		if ln != "" {
			if len(ln) > CFG.MaxLineChars {
				ln = ln[:CFG.MaxLineChars]
			}
			lines = append(lines, ln)
		}
	}
	return lines
}

func saveCorpusLines(path string, lines []string) {
	f, err := os.Create(path)
	if err != nil {
		return
	}
	defer f.Close()
	w := bufio.NewWriter(f)
	for _, ln := range lines {
		ln = strings.ReplaceAll(ln, "\n", " ")
		fmt.Fprintln(w, strings.TrimSpace(ln))
	}
	w.Flush()
}

func normalizeText(s string) string {
	s = strings.ReplaceAll(s, "\r", " ")
	s = strings.ReplaceAll(s, "\t", " ")
	return strings.Join(strings.Fields(s), " ")
}

func extractCandidateSentences(msgs []struct{ Role, Text string }) []string {
	var out []string
	for _, msg := range msgs {
		t := normalizeText(msg.Text)
		if t == "" {
			continue
		}
		tag := "A:"
		if msg.Role == "user" {
			tag = "H:"
		}

		buf := ""
		for _, ch := range t {
			buf += string(ch)
			if ch == '.' || ch == '!' || ch == '?' {
				s := strings.TrimSpace(buf)
				if len(s) >= 6 {
					out = append(out, tag+" "+s)
				}
				buf = ""
			}
		}
		s := strings.TrimSpace(buf)
		if len(s) >= 12 {
			out = append(out, tag+" "+s)
		}
	}

	// Stable dedup
	seen := make(map[string]bool)
	var uniq []string
	for _, s := range out {
		k := strings.ToLower(s)
		if !seen[k] {
			seen[k] = true
			uniq = append(uniq, s)
		}
	}
	return uniq
}

func reservoirMixKeep(lines, newSents []string, maxLines int) []string {
	combined := append(append([]string{}, lines...), newSents...)
	half := maxLines / 2
	var newest, older []string
	if len(combined) > half {
		newest = combined[len(combined)-half:]
		older = combined[:len(combined)-half]
	} else {
		newest = combined
	}

	rand.Shuffle(len(older), func(i, j int) { older[i], older[j] = older[j], older[i] })
	keep := maxLines - len(newest)
	if keep < 0 {
		keep = 0
	}
	if keep > len(older) {
		keep = len(older)
	}
	final := append(older[:keep], newest...)

	// Dedup
	seen := make(map[string]bool)
	var dedup []string
	for _, s := range final {
		k := strings.ToLower(s)
		if !seen[k] {
			seen[k] = true
			if len(s) > CFG.MaxLineChars {
				s = s[:CFG.MaxLineChars]
			}
			dedup = append(dedup, s)
		}
	}
	if len(dedup) > maxLines {
		dedup = dedup[len(dedup)-maxLines:]
	}
	return dedup
}

func updateReservoirCorpus(db *sql.DB, corpusPath string, maxLines int) int {
	msgs := dbRecentMessages(db, 64)
	newSents := extractCandidateSentences(msgs)
	if len(newSents) == 0 {
		return 0
	}

	lines := loadCorpusLines(corpusPath)
	before := 0
	for _, x := range lines {
		before += len(x)
	}

	final := reservoirMixKeep(lines, newSents, maxLines)
	saveCorpusLines(corpusPath, final)

	after := 0
	for _, x := range final {
		after += len(x)
	}
	added := after - before
	if added < 0 {
		added = 0
	}

	db.Exec("INSERT INTO corpus_events(ts, added_chars, note) VALUES(?,?,?)",
		float64(time.Now().UnixMilli())/1000.0, added,
		fmt.Sprintf("reservoir_update +%d sents", len(newSents)))
	return added
}

func computeNewCorpusMass(db *sql.DB, lastEventID int) (int, int) {
	rows, err := db.Query("SELECT id, added_chars FROM corpus_events WHERE id > ? ORDER BY id ASC", lastEventID)
	if err != nil {
		return 0, lastEventID
	}
	defer rows.Close()
	mass := 0
	newLastID := lastEventID
	for rows.Next() {
		var id, chars int
		rows.Scan(&id, &chars)
		mass += chars
		newLastID = id
	}
	return mass, newLastID
}

// ============================================================
// 8) CHECKPOINTING — modular, compatible, no merge-amnesia
// ============================================================

type CheckpointJSON struct {
	Cfg       json.RawMessage            `json:"cfg"`
	Tokenizer TokenizerJSON              `json:"tokenizer"`
	Base      map[string][][][]float64   `json:"base"`  // name -> rows -> cols (but we store as [][]float64)
	Alpha     []float64                  `json:"alpha"`
	Deltas    []map[string]DeltaJSON     `json:"deltas"`
}

func intPtr(v int) *int { return &v }

// We need a different approach - Base stores name -> [][]float64 (matrix rows)
type CheckpointData struct {
	Cfg               json.RawMessage        `json:"cfg"`
	Tokenizer         TokenizerJSON          `json:"tokenizer"`
	Base              map[string][][]float64 `json:"base"`
	Alpha             []float64              `json:"alpha"`
	Deltas            []map[string]DeltaJSON `json:"deltas"`
	InitEmbedSnapshot [][]float64            `json:"init_embed_snapshot,omitempty"`
	GlobalStep        int                    `json:"global_step"`
	GrowthStepOffset  int                    `json:"growth_step_offset"`
	LastWarmupStage   *int                   `json:"last_warmup_stage,omitempty"`
}

type TokenizerJSON struct {
	Tokens       []string   `json:"tokens"`
	BPEEnabled   bool       `json:"bpe_enabled"`
	Merges       [][]string `json:"merges"`
	TrainedChars int        `json:"trained_chars"`
}

type DeltaJSON struct {
	A [][]float64 `json:"A"`
	B [][]float64 `json:"B"`
}

func serializeMatrixParam(mp *MatrixParam) [][]float64 {
	rows := make([][]float64, mp.Nout)
	for i, row := range mp.Rows {
		rows[i] = make([]float64, len(row.Data))
		copy(rows[i], row.Data)
	}
	return rows
}

func deserializeMatrixParam(data [][]float64) *MatrixParam {
	if len(data) == 0 {
		return &MatrixParam{}
	}
	mp := &MatrixParam{
		Nout: len(data),
		Nin:  len(data[0]),
		Rows: make([]*Vec, len(data)),
	}
	for i, row := range data {
		d := make([]float64, len(row))
		copy(d, row)
		mp.Rows[i] = NewVecWithGrad(d) // loaded params always need grad
	}
	return mp
}

func SaveCheckpoint(model *GPT, tok *EvolvingTokenizer, path string) error {
	if path == "" {
		path = CFG.CkptPath
	}

	merges := make([][]string, len(tok.Merges))
	for i, m := range tok.Merges {
		merges[i] = []string{m.A, m.B}
	}

	cfgJSON, _ := json.Marshal(CFG)

	base := make(map[string][][]float64)
	for k, v := range model.Base {
		base[k] = serializeMatrixParam(v)
	}

	deltas := make([]map[string]DeltaJSON, len(model.Deltas))
	for i, mod := range model.Deltas {
		dm := make(map[string]DeltaJSON)
		for name, da := range mod {
			dm[name] = DeltaJSON{
				A: serializeMatrixParam(da.A),
				B: serializeMatrixParam(da.B),
			}
		}
		deltas[i] = dm
	}

	ckpt := CheckpointData{
		Cfg: cfgJSON,
		Tokenizer: TokenizerJSON{
			Tokens:       tok.Tokens,
			BPEEnabled:   tok.BPEEnabled,
			Merges:       merges,
			TrainedChars: tok.TrainedChars,
		},
		Base:              base,
		Alpha:             model.ActiveAlpha,
		Deltas:            deltas,
		InitEmbedSnapshot: model.InitEmbedSnapshot,
		GlobalStep:        model.globalStep,
		GrowthStepOffset:  model.growthStepOffset,
		LastWarmupStage:   intPtr(model.lastWarmupStage),
	}

	// Atomic write: temp file + rename (prevents corruption on crash)
	tmpPath := path + ".tmp"
	f, err := os.Create(tmpPath)
	if err != nil {
		return err
	}
	err = json.NewEncoder(f).Encode(ckpt)
	f.Close()
	if err != nil {
		os.Remove(tmpPath)
		return err
	}
	return os.Rename(tmpPath, path)
}

func LoadCheckpoint(docs []string, path string) (*GPT, *EvolvingTokenizer, error) {
	if path == "" {
		path = CFG.CkptPath
	}
	f, err := os.Open(path)
	if err != nil {
		return nil, nil, err
	}
	defer f.Close()

	var ckpt CheckpointData
	if err := json.NewDecoder(f).Decode(&ckpt); err != nil {
		return nil, nil, err
	}

	// Restore tokenizer
	if len(docs) == 0 {
		docs = []string{"Hello."}
	}
	tok := NewEvolvingTokenizer(docs)
	if len(ckpt.Tokenizer.Tokens) > 0 {
		tok.Tokens = ckpt.Tokenizer.Tokens
		tok.Stoi = make(map[string]int)
		tok.Itos = make(map[int]string)
		for i, t := range tok.Tokens {
			tok.Stoi[t] = i
			tok.Itos[i] = t
		}
		tok.VocabSize = len(tok.Tokens)
	}

	tok.Merges = make([]MergePair, 0)
	tok.MergeToTok = make(map[MergePair]string)
	for _, m := range ckpt.Tokenizer.Merges {
		if len(m) == 2 {
			p := MergePair{m[0], m[1]}
			tok.Merges = append(tok.Merges, p)
			tok.MergeToTok[p] = m[0] + "+" + m[1]
		}
	}
	tok.BPEEnabled = ckpt.Tokenizer.BPEEnabled
	tok.TrainedChars = ckpt.Tokenizer.TrainedChars

	// Restore model dimensions from checkpoint config (ontogenesis may have changed them)
	if len(ckpt.Cfg) > 0 {
		var savedCfg struct {
			NEmbd     int      `json:"n_embd"`
			NLayer    int      `json:"n_layer"`
			NHead     int      `json:"n_head"`
			HeadTypes []string `json:"head_types"`
		}
		if json.Unmarshal(ckpt.Cfg, &savedCfg) == nil {
			if savedCfg.NEmbd > 0 {
				CFG.NEmbd = savedCfg.NEmbd
			}
			if savedCfg.NLayer > 0 {
				CFG.NLayer = savedCfg.NLayer
			}
			if savedCfg.NHead > 0 {
				CFG.NHead = savedCfg.NHead
			}
			if len(savedCfg.HeadTypes) > 0 {
				CFG.HeadTypes = savedCfg.HeadTypes
			}
		}
	}

	// Restore model
	model := NewGPT(tok)
	model.Base = make(map[string]*MatrixParam)
	for k, v := range ckpt.Base {
		model.Base[k] = deserializeMatrixParam(v)
	}
	// Re-establish embedding tie after deserialization (JSON breaks pointer identity)
	if CFG.TieEmbeddings {
		model.Base["lm_head"] = model.Base["wte"]
	}

	model.Deltas = nil
	model.ActiveAlpha = ckpt.Alpha
	for _, modData := range ckpt.Deltas {
		mod := make(DeltaModule)
		for name, dj := range modData {
			da := &DeltaAdapter{
				A: deserializeMatrixParam(dj.A),
				B: deserializeMatrixParam(dj.B),
			}
			mod[name] = da
		}
		model.Deltas = append(model.Deltas, mod)
	}

	if len(model.Deltas) == 0 {
		model.AddDeltaModule(1.0)
	}

	// Restore init_embed_snapshot (or create from current if not in checkpoint)
	if len(ckpt.InitEmbedSnapshot) > 0 {
		model.InitEmbedSnapshot = ckpt.InitEmbedSnapshot
	} else {
		model.InitEmbedSnapshot = make([][]float64, len(model.Base["wte"].Rows))
		for i, row := range model.Base["wte"].Rows {
			snap := make([]float64, len(row.Data))
			copy(snap, row.Data)
			model.InitEmbedSnapshot[i] = snap
		}
	}

	// Restore global step and growth state
	model.globalStep = ckpt.GlobalStep
	model.growthStepOffset = ckpt.GrowthStepOffset
	if ckpt.LastWarmupStage != nil {
		model.lastWarmupStage = *ckpt.LastWarmupStage
	} else if ckpt.GlobalStep > 0 {
		// Old checkpoint without lastWarmupStage: assume current stage is warmed up
		model.lastWarmupStage = model.CurrentGrowthStage()
	}

	// Ensure hybrid attention weights exist (backward compat with old checkpoints)
	for li := 0; li < CFG.NLayer; li++ {
		for h, htype := range CFG.HeadTypes {
			if htype == "rrpram" || htype == "hybrid" {
				key := fmt.Sprintf("l%d.h%d.w_pattern", li, h)
				if _, ok := model.Base[key]; !ok {
					model.Base[key] = NewMatrixParam(CFG.BlockSize, model.HeadDim, 0.08)
				}
			}
			alphaKey := fmt.Sprintf("l%d.h%d.alpha", li, h)
			if _, ok := model.Base[alphaKey]; !ok {
				m := NewMatrixParam(1, 1, 0.0)
				m.Rows[0].Data[0] = CFG.HybridAlphaInit
				model.Base[alphaKey] = m
			}
		}
	}

	return model, tok, nil
}

// ============================================================
// 9a) QUANTUM BUFFER — trains when ready, not when told
// ============================================================

// And lo, the buffer shall measure not just bytes but novelty, for raw mass means nothing without surprise.
type QuantumBuffer struct {
	mu               sync.Mutex
	AccumulatedBytes int
	UniqueTokens     map[int]bool
	TotalTokens      int
	LastBurstTime    float64
}

func NewQuantumBuffer() *QuantumBuffer {
	return &QuantumBuffer{UniqueTokens: make(map[int]bool)}
}

func (qb *QuantumBuffer) Feed(text string, tok *EvolvingTokenizer) {
	qb.mu.Lock()
	defer qb.mu.Unlock()
	qb.AccumulatedBytes += len(text)
	ids := tok.Encode(text)
	for _, id := range ids {
		qb.UniqueTokens[id] = true
		qb.TotalTokens++
	}
}

func (qb *QuantumBuffer) noveltyScoreLocked() float64 {
	if qb.TotalTokens == 0 {
		return 0.0
	}
	return float64(len(qb.UniqueTokens)) / float64(qb.TotalTokens)
}

func (qb *QuantumBuffer) ShouldTrigger() bool {
	qb.mu.Lock()
	defer qb.mu.Unlock()
	now := float64(time.Now().UnixMilli()) / 1000.0
	bytesOK := qb.AccumulatedBytes >= CFG.QBMinBytes
	noveltyOK := qb.noveltyScoreLocked() >= CFG.QBMinNovelty
	cooldownOK := (now - qb.LastBurstTime) >= CFG.QBCooldownSeconds
	return (bytesOK || noveltyOK) && cooldownOK
}

// SnapshotStats returns accumulated bytes and novelty under one lock.
func (qb *QuantumBuffer) SnapshotStats() (int, float64) {
	qb.mu.Lock()
	defer qb.mu.Unlock()
	return qb.AccumulatedBytes, qb.noveltyScoreLocked()
}

func (qb *QuantumBuffer) Reset() {
	qb.mu.Lock()
	defer qb.mu.Unlock()
	qb.AccumulatedBytes = 0
	qb.UniqueTokens = make(map[int]bool)
	qb.TotalTokens = 0
	qb.LastBurstTime = float64(time.Now().UnixMilli()) / 1000.0
}

// ============================================================
// 9b) COOCCUR FIELD — speech before learning
// ============================================================

// And lo, the corpus shall whisper its statistics, and words shall follow words.
type CooccurField struct {
	Unigram          map[int]float64
	BigramByFirst    map[int]map[int]float64    // prev → {next: count}
	TrigramByContext map[[2]int]map[int]float64  // [prev2,prev1] → {next: count}
	FourgramByCtx    map[[3]int]map[int]float64  // [prev3,prev2,prev1] → {next: count}
	CooccurWindow    map[int]map[int]float64     // token → {nearby_token: count} (Stanley-style proximity)
	UserBoost        map[int]float64             // temporary user word boosts (Leo-style)
	Built            bool
	mu               sync.RWMutex // RWMutex: reads (SampleNext) don't block each other
}

func NewCooccurField() *CooccurField {
	return &CooccurField{
		Unigram:          make(map[int]float64),
		BigramByFirst:    make(map[int]map[int]float64),
		TrigramByContext: make(map[[2]int]map[int]float64),
		FourgramByCtx:    make(map[[3]int]map[int]float64),
		CooccurWindow:    make(map[int]map[int]float64),
		UserBoost:        make(map[int]float64),
	}
}

func (cf *CooccurField) BuildFromCorpus(tok *EvolvingTokenizer, docs []string) {
	// Build into temporary maps first, then swap atomically
	uni := make(map[int]float64)
	bi := make(map[int]map[int]float64)
	tri := make(map[[2]int]map[int]float64)
	four := make(map[[3]int]map[int]float64)
	cooc := make(map[int]map[int]float64)
	window := CFG.CooccurWindowSize

	for _, doc := range docs {
		ids := tok.Encode(doc)
		for _, id := range ids {
			uni[id]++
		}
		for i := 0; i < len(ids)-1; i++ {
			first, second := ids[i], ids[i+1]
			if bi[first] == nil {
				bi[first] = make(map[int]float64)
			}
			bi[first][second]++
		}
		for i := 0; i < len(ids)-2; i++ {
			ctx := [2]int{ids[i], ids[i+1]}
			if tri[ctx] == nil {
				tri[ctx] = make(map[int]float64)
			}
			tri[ctx][ids[i+2]]++
		}
		// 4-grams: deeper context for child+ stages
		for i := 0; i < len(ids)-3; i++ {
			ctx := [3]int{ids[i], ids[i+1], ids[i+2]}
			if four[ctx] == nil {
				four[ctx] = make(map[int]float64)
			}
			four[ctx][ids[i+3]]++
		}
		// Co-occurrence window: "words that resonate together, stay together" (Stanley)
		for i := 0; i < len(ids); i++ {
			center := ids[i]
			start := i - window
			if start < 0 {
				start = 0
			}
			end := i + window + 1
			if end > len(ids) {
				end = len(ids)
			}
			for j := start; j < end; j++ {
				if i != j {
					neighbor := ids[j]
					if cooc[center] == nil {
						cooc[center] = make(map[int]float64)
					}
					cooc[center][neighbor]++
				}
			}
		}
	}
	// Atomic swap under lock
	cf.mu.Lock()
	cf.Unigram = uni
	cf.BigramByFirst = bi
	cf.TrigramByContext = tri
	cf.FourgramByCtx = four
	cf.CooccurWindow = cooc
	cf.Built = true
	cf.mu.Unlock()
}

// IngestTokens incrementally adds n-gram counts from a token sequence.
// Unlike BuildFromCorpus, this does NOT clear existing data — it adds on top.
func (cf *CooccurField) IngestTokens(ids []int) {
	cf.IngestTokensWeighted(ids, 1.0)
}

// IngestTokensWeighted adds n-gram counts weighted by a factor.
// High weight = this text matters more (coherent output). Low = less influence.
// Stanley's observe_shard weights by resonance score; we weight by inverse entropy.
func (cf *CooccurField) IngestTokensWeighted(ids []int, weight float64) {
	cf.mu.Lock()
	defer cf.mu.Unlock()
	window := CFG.CooccurWindowSize

	for _, id := range ids {
		cf.Unigram[id] += weight
	}
	for i := 0; i < len(ids)-1; i++ {
		first, second := ids[i], ids[i+1]
		if cf.BigramByFirst[first] == nil {
			cf.BigramByFirst[first] = make(map[int]float64)
		}
		cf.BigramByFirst[first][second] += weight
	}
	for i := 0; i < len(ids)-2; i++ {
		ctx := [2]int{ids[i], ids[i+1]}
		if cf.TrigramByContext[ctx] == nil {
			cf.TrigramByContext[ctx] = make(map[int]float64)
		}
		cf.TrigramByContext[ctx][ids[i+2]] += weight
	}
	for i := 0; i < len(ids)-3; i++ {
		ctx := [3]int{ids[i], ids[i+1], ids[i+2]}
		if cf.FourgramByCtx[ctx] == nil {
			cf.FourgramByCtx[ctx] = make(map[int]float64)
		}
		cf.FourgramByCtx[ctx][ids[i+3]] += weight
	}
	// Co-occurrence window
	for i := 0; i < len(ids); i++ {
		center := ids[i]
		start := i - window
		if start < 0 {
			start = 0
		}
		end := i + window + 1
		if end > len(ids) {
			end = len(ids)
		}
		for j := start; j < end; j++ {
			if i != j {
				neighbor := ids[j]
				if cf.CooccurWindow[center] == nil {
					cf.CooccurWindow[center] = make(map[int]float64)
				}
				cf.CooccurWindow[center][neighbor] += weight
			}
		}
	}
}

// AbsorbUserWords sets temporary boosts for tokens the user just said.
// Like Leo's Santa Klaus but simpler: user words get multiplicative boost in generation.
func (cf *CooccurField) AbsorbUserWords(ids []int) {
	cf.mu.Lock()
	defer cf.mu.Unlock()
	// Decay existing boosts first
	for k, v := range cf.UserBoost {
		nv := v * CFG.UserBoostDecay
		if nv < 0.01 {
			delete(cf.UserBoost, k)
		} else {
			cf.UserBoost[k] = nv
		}
	}
	// Boost user's tokens
	strength := CFG.UserBoostStrength
	for _, id := range ids {
		cf.UserBoost[id] += strength
	}
}

// DecayUserBoost reduces user word boosts after a generation.
func (cf *CooccurField) DecayUserBoost() {
	cf.mu.Lock()
	defer cf.mu.Unlock()
	for k, v := range cf.UserBoost {
		nv := v * CFG.UserBoostDecay
		if nv < 0.01 {
			delete(cf.UserBoost, k)
		} else {
			cf.UserBoost[k] = nv
		}
	}
}

func (cf *CooccurField) SampleNext(contextIDs []int, vocabSize int, temperature float64) int {
	cf.mu.RLock()
	defer cf.mu.RUnlock()
	counts := make([]float64, vocabSize)
	found := false

	// Try 4-gram (deepest context)
	if len(contextIDs) >= 3 {
		ctx := [3]int{contextIDs[len(contextIDs)-3], contextIDs[len(contextIDs)-2], contextIDs[len(contextIDs)-1]}
		if d, ok := cf.FourgramByCtx[ctx]; ok {
			for tid, v := range d {
				if tid < vocabSize {
					counts[tid] += v
					found = true
				}
			}
		}
	}

	// Fallback to trigram
	if !found && len(contextIDs) >= 2 {
		a, b := contextIDs[len(contextIDs)-2], contextIDs[len(contextIDs)-1]
		if ctx, ok := cf.TrigramByContext[[2]int{a, b}]; ok {
			for tid, v := range ctx {
				if tid < vocabSize {
					counts[tid] += v
					found = true
				}
			}
		}
	}

	// Fallback to bigram
	if !found && len(contextIDs) >= 1 {
		prev := contextIDs[len(contextIDs)-1]
		if ctx, ok := cf.BigramByFirst[prev]; ok {
			for tid, v := range ctx {
				if tid < vocabSize {
					counts[tid] += v
					found = true
				}
			}
		}
	}

	// Fallback to unigram
	if !found {
		for k, v := range cf.Unigram {
			if k < vocabSize {
				counts[k] = v
			}
		}
	}

	// Blend with co-occurrence window (background resonance, always active)
	if len(contextIDs) > 0 {
		wnd := CFG.CooccurWindowSize
		ctxSlice := contextIDs
		if len(ctxSlice) > wnd {
			ctxSlice = ctxSlice[len(ctxSlice)-wnd:]
		}
		for _, ctxTok := range ctxSlice {
			if neighbors, ok := cf.CooccurWindow[ctxTok]; ok {
				for tid, cnt := range neighbors {
					if tid < vocabSize {
						counts[tid] += cnt * 0.3 // co-occurrence is softer than n-gram
					}
				}
			}
		}
	}

	// Apply user word boost (multiplicative)
	if len(cf.UserBoost) > 0 {
		for tid, boost := range cf.UserBoost {
			if tid < vocabSize && counts[tid] > 0 {
				counts[tid] *= (1.0 + boost)
			}
		}
	}

	// Apply temperature and sample
	total := 0.0
	for i := range counts {
		if counts[i] > 0 && temperature > 0 {
			counts[i] = math.Pow(counts[i], 1.0/temperature)
		}
		total += counts[i]
	}
	if total <= 0 {
		return rand.Intn(vocabSize)
	}

	r := rand.Float64() * total
	s := 0.0
	for i, c := range counts {
		s += c
		if s >= r {
			return i
		}
	}
	return vocabSize - 1
}

// And lo, the organism shall speak before it learns, like a newborn crying.
func CorpusGenerate(tok *EvolvingTokenizer, field *CooccurField, prompt string, maxTokens int) string {
	ids := []int{tok.Stoi[tok.BOS]}
	if prompt != "" {
		enc := tok.Encode(prompt)
		ids = enc[:len(enc)-1] // strip EOS
	}

	eosID := tok.Stoi[tok.EOS]
	for step := 0; step < maxTokens; step++ {
		nxt := field.SampleNext(ids, tok.VocabSize, CFG.Temperature)
		if nxt == eosID {
			break
		}
		ids = append(ids, nxt)
	}
	ids = append(ids, eosID)
	return tok.Decode(ids)
}

// And lo, the model and the corpus shall duet like two drunks harmonizing.
func GenerateResonant(model *GPT, tok *EvolvingTokenizer, field *CooccurField, prompt string, docs []string, useModel bool) string {
	if !useModel || model == nil {
		return CorpusGenerate(tok, field, prompt, CFG.CorpusGenMaxTokens)
	}

	model.mu.Lock()
	defer model.mu.Unlock()

	gradEnabled.Store(false)
	defer func() { gradEnabled.Store(true) }()

	var ids []int
	if prompt != "" {
		enc := tok.Encode(prompt)
		ids = enc[:len(enc)-1]
	} else {
		ids = []int{tok.Stoi[tok.BOS]}
	}

	keys := make([][]*Vec, model.NLayer)
	values := make([][]*Vec, model.NLayer)
	for i := 0; i < model.NLayer; i++ {
		keys[i] = make([]*Vec, 0)
		values[i] = make([]*Vec, 0)
	}

	limit := len(ids)
	if limit > model.BlockSize {
		limit = model.BlockSize
	}
	for pos := 0; pos < limit; pos++ {
		model.ForwardStep(ids[pos], pos, keys, values)
	}

	cur := ids[len(ids)-1]
	var outIDs []int
	var recentBuf []int // for repetition guard
	eosID := tok.Stoi[tok.EOS]
	bosID := tok.Stoi[tok.BOS]

	// Consciousness: per-token dissonance tracking (Feature 1)
	entropyEMA := 0.0
	entropyEMAInit := false
	lowDropCount := 0
	entropySum := 0.0
	entropyCount := 0
	tokenCounts := make(map[int]int) // frequency penalty

	for step := 0; step < CFG.MaxGenTokens; step++ {
		pos := len(ids) - 1
		if pos > model.BlockSize-1 {
			pos = model.BlockSize - 1
		}
		logits := model.ForwardStep(cur, pos, keys, values)

		// Frequency + presence penalty on logits
		if CFG.FreqPenalty > 0 || CFG.PresencePenalty > 0 {
			for tid, cnt := range tokenCounts {
				if tid < len(logits.Data) {
					logits.Data[tid] -= CFG.FreqPenalty * float64(cnt)
					if cnt > 0 {
						logits.Data[tid] -= CFG.PresencePenalty
					}
				}
			}
		}

		// Model probs with surprise-modulated + dissonance-adaptive temperature
		temp := CFG.Temperature
		// Consciousness: surprise modulation (Feature 4 — now wired)
		if model.surpriseBaseline > 1e-6 {
			surpriseRatio := model.lastSurprise / model.surpriseBaseline
			if surpriseRatio > 1.5 {
				temp *= 0.85 // high surprise → be careful
			} else if surpriseRatio < 0.5 {
				temp *= 1.1 // low surprise → explore slightly
			}
		}
		if temp <= 1e-6 {
			temp = 1e-6
		}
		scaled := make([]float64, len(logits.Data))
		for i, v := range logits.Data {
			scaled[i] = v / temp
		}
		modelProbs := SoftmaxProbs(scaled)

		// Per-token entropy for dissonance
		entropy := 0.0
		for _, p := range modelProbs {
			if p > 1e-12 {
				entropy -= p * math.Log(p)
			}
		}
		entropySum += entropy
		entropyCount++

		// Consciousness: per-token dissonance feedback (Feature 1)
		dissonanceMul := 1.0
		if !entropyEMAInit {
			entropyEMA = entropy
			entropyEMAInit = true
		} else {
			entropyEMA = CFG.DissonanceEMAAlpha*entropy + (1.0-CFG.DissonanceEMAAlpha)*entropyEMA
			if entropyEMA > 1e-6 {
				ratio := entropy / entropyEMA
				if ratio > CFG.DissonanceSpikeThreshold {
					dissonanceMul = CFG.DissonanceSpikeK
					lowDropCount = 0
				} else if ratio < CFG.DissonanceDropThreshold {
					lowDropCount++
					if lowDropCount >= 3 {
						dissonanceMul = CFG.DissonanceDropK
					}
				} else {
					lowDropCount = 0
				}
			}
		}
		if dissonanceMul != 1.0 {
			temp *= dissonanceMul
			for i, v := range logits.Data {
				scaled[i] = v / temp
			}
			modelProbs = SoftmaxProbs(scaled)
		}

		// Per-token sigmoid corpus fade: compute alpha from local entropy
		tokenAlpha := 1.0 / (1.0 + math.Exp(-CFG.CorpusFadeK*(CFG.CorpusFadeThreshold-entropy)))

		// Corpus blend: skip entirely when tokenAlpha >= 0.99 (pure model mode)
		var blended []float64
		if tokenAlpha >= 0.99 || field == nil {
			blended = modelProbs
		} else {
			corpusCounts := make([]float64, tok.VocabSize)
			ctxForCorpus := ids
			if len(ctxForCorpus) > 3 {
				ctxForCorpus = ctxForCorpus[len(ctxForCorpus)-3:]
			}
			// Rebuild corpus distribution under read lock
			field.mu.RLock()
			corpusTotal := 0.0
			if len(ctxForCorpus) >= 2 {
				a, b := ctxForCorpus[len(ctxForCorpus)-2], ctxForCorpus[len(ctxForCorpus)-1]
				if ctx, ok := field.TrigramByContext[[2]int{a, b}]; ok {
					for tid, v := range ctx {
						if tid < tok.VocabSize {
							corpusCounts[tid] += v
							corpusTotal += v
						}
					}
				}
			}
			if corpusTotal == 0 && len(ctxForCorpus) >= 1 {
				prev := ctxForCorpus[len(ctxForCorpus)-1]
				if ctx, ok := field.BigramByFirst[prev]; ok {
					for tid, v := range ctx {
						if tid < tok.VocabSize {
							corpusCounts[tid] += v
						}
					}
				}
			}
			field.mu.RUnlock()
			corpusTotal = 0.0
			for _, c := range corpusCounts {
				corpusTotal += c
			}
			corpusProbs := make([]float64, tok.VocabSize)
			if corpusTotal > 0 {
				for i, c := range corpusCounts {
					corpusProbs[i] = c / corpusTotal
				}
			} else {
				uni := 1.0 / float64(tok.VocabSize)
				for i := range corpusProbs {
					corpusProbs[i] = uni
				}
			}
			blended = make([]float64, tok.VocabSize)
			for i := 0; i < tok.VocabSize && i < len(modelProbs); i++ {
				blended[i] = tokenAlpha*modelProbs[i] + (1.0-tokenAlpha)*corpusProbs[i]
			}
		}

		// Consciousness: pattern breaking (Feature 2)
		if step >= CFG.AntiFieldMinStep && CFG.AntiFieldProb > 0 && rand.Float64() < CFG.AntiFieldProb {
			blended = modelProbs // pure model voice, bypass corpus
		}

		nxt := TopKTopPSample(blended, CFG.TopK, CFG.TopP, CFG.MinP, CFG.TypicalP)

		if nxt == eosID && step >= CFG.MinGenTokens {
			break
		}
		if nxt == eosID {
			continue
		}

		ids = append(ids, nxt)
		cur = nxt
		outIDs = append(outIDs, nxt)
		tokenCounts[nxt]++

		// Repetition guard: break if last rg*2 tokens are a repeating pattern
		recentBuf = append(recentBuf, nxt)
		rg := CFG.RepetitionGuard
		if len(recentBuf) > rg*2 {
			recentBuf = recentBuf[len(recentBuf)-rg*2:]
			if sliceEqual(recentBuf[rg:], recentBuf[:rg]) {
				break
			}
		}

		if step >= CFG.MinGenTokens && len(outIDs) > 0 {
			decIDs := append([]int{bosID}, outIDs...)
			decIDs = append(decIDs, eosID)
			text := tok.Decode(decIDs)
			if len(text) > 0 {
				last := text[len(text)-1]
				if last == '.' || last == '!' || last == '?' {
					break
				}
			}
		}
	}

	// Consciousness: store mean entropy for conscience (Feature 5)
	if entropyCount > 0 {
		model.lastGenEntropy = entropySum / float64(entropyCount)
	}

	decIDs := append([]int{bosID}, outIDs...)
	decIDs = append(decIDs, eosID)
	return tok.Decode(decIDs)
}

// ============================================================
// 9) TRAINING — warmup, then continual micro-bursts
// ============================================================

// ============================================================
// 9.5) SYNTROPY TRACKER — the arrow that points toward coherence
// ============================================================
// And lo, the organism shall not merely track its changes,
// but reason mathematically about whether it is becoming more itself.

// BurstRecord stores what happened after a training burst — for self-meta-learning.
type BurstRecord struct {
	Action     string
	LossBefore float64
	LossAfter  float64
}

// SyntropyTracker is the mathematical self-reasoning engine.
// Tracks entropy trend, field deviation, purpose alignment.
// Makes decisions about learning direction — not just 'did I learn?'
// but 'should I keep going this way?'
// SwarmPeerInfo holds peer information from mesh.db.
type SwarmPeerInfo struct {
	Peers []map[string]interface{}
}

type SyntropyTracker struct {
	EntropyHistory   []float64 // rolling window of model entropy
	SyntropyTrend    float64   // positive = organizing, negative = dissolving
	FieldDeviation   float64   // how far from corpus physics
	PurposeMagnitude float64   // strength of current learning direction
	PurposeAlignment float64   // cosine(purpose, gamma)
	LastAction       string    // what was decided last time
	BurstHistory     []BurstRecord // last 16 burst outcomes — training efficiency memory
	ModelStage       int       // current growth stage (set during measure)
	LastMitosisTime  float64   // cooldown for divide
	SwarmInfo        *SwarmPeerInfo // peer state from mesh.db (set externally)
}

// NewSyntropyTracker creates a new tracker with sane defaults.
// And lo, the arrow is drawn, but not yet fired.
func NewSyntropyTracker() *SyntropyTracker {
	return &SyntropyTracker{
		LastAction: "none",
	}
}

// RecordBurst logs a burst outcome for self-meta-learning.
// The organism remembers what worked and what didn't.
func (st *SyntropyTracker) RecordBurst(action string, lossBefore, lossAfter float64) {
	st.BurstHistory = append(st.BurstHistory, BurstRecord{action, lossBefore, lossAfter})
	if len(st.BurstHistory) > 16 {
		st.BurstHistory = st.BurstHistory[len(st.BurstHistory)-16:]
	}
}

// ActionEffectiveness returns the mean loss delta for a given action type.
// Negative = good (loss went down). Positive = bad (loss went up).
func (st *SyntropyTracker) ActionEffectiveness(action string) (float64, int) {
	sum := 0.0
	count := 0
	for _, br := range st.BurstHistory {
		if br.Action == action {
			sum += br.LossAfter - br.LossBefore
			count++
		}
	}
	if count == 0 {
		return 0, 0
	}
	return sum / float64(count), count
}

// SyntropyMetrics holds the result of a syntropy measurement pass.
type SyntropyMetrics struct {
	Entropy          float64
	SyntropyTrend    float64
	FieldDeviation   float64
	PurposeMagnitude float64
	PurposeAlignment float64
}

// Measure takes all measurements. This is the organism looking at itself
// through mathematical instruments.
func (st *SyntropyTracker) Measure(model *GPT, tok *EvolvingTokenizer, field *CooccurField, docs []string) SyntropyMetrics {
	st.ModelStage = model.CurrentGrowthStage()
	entropyNow := model.ComputeModelEntropy(tok, docs, 16)
	st.EntropyHistory = append(st.EntropyHistory, entropyNow)
	if len(st.EntropyHistory) > CFG.SyntropyWindow {
		st.EntropyHistory = st.EntropyHistory[len(st.EntropyHistory)-CFG.SyntropyWindow:]
	}

	// syntropy = negative entropy trend (entropy going down = syntropy going up)
	if len(st.EntropyHistory) >= 2 {
		recentHalf := len(st.EntropyHistory) / 2
		oldMean := 0.0
		for _, v := range st.EntropyHistory[:recentHalf] {
			oldMean += v
		}
		oldMean /= float64(recentHalf)

		newSlice := st.EntropyHistory[recentHalf:]
		newMean := 0.0
		for _, v := range newSlice {
			newMean += v
		}
		newMean /= float64(len(newSlice))

		st.SyntropyTrend = oldMean - newMean // positive = good
	} else {
		st.SyntropyTrend = 0.0
	}

	st.FieldDeviation = model.ComputeFieldDeviation(tok, field, docs, 32)
	_, st.PurposeMagnitude = model.ComputePurposeVector()
	st.PurposeAlignment = model.PurposeGammaAlignment()

	return SyntropyMetrics{
		Entropy:          entropyNow,
		SyntropyTrend:    st.SyntropyTrend,
		FieldDeviation:   st.FieldDeviation,
		PurposeMagnitude: st.PurposeMagnitude,
		PurposeAlignment: st.PurposeAlignment,
	}
}

// SyntropyDecision holds the outcome of the organism's mathematical self-reasoning.
// Not just LR anymore — the organism modulates its entire behavior.
type SyntropyDecision struct {
	LRMultiplier      float64
	TempOffset        float64  // added to generation temperature (-0.05 to +0.05)
	AccumOverride     int      // 0 = no override, >0 = use this accum_steps for this burst
	DeltaGrowOverride *float64 // nil = no override
	Action            string
}

// DecideAction performs mathematical self-reasoning: decide how to adjust learning.
// And lo, this is where tracking becomes reasoning, and reasoning becomes action.
// The organism does not just observe — it steers.
func (st *SyntropyTracker) DecideAction() SyntropyDecision {
	// Default: steady state
	lrMultiplier := 1.0
	tempOffset := 0.0
	accumOverride := 0
	var deltaGrowOverride *float64
	action := "steady"

	// CASE 1: Syntropy rising + field deviation in sweet spot = thriving
	if st.SyntropyTrend > 0.01 &&
		st.FieldDeviation > CFG.FieldDeviationFloor &&
		st.FieldDeviation < CFG.FieldDeviationCeiling {
		lrMultiplier = CFG.SyntropyLRBoost
		tempOffset = -0.05 // more confident when organizing
		if st.PurposeAlignment > 0.3 {
			boost := CFG.SyntropyDeltaGrowBoost
			deltaGrowOverride = &boost
			accumOverride = 2 // stable gradient when everything aligned
			action = "amplify"
		} else {
			action = "boost"
		}

		// CASE 2: Syntropy falling = dissolving, slow down
	} else if st.SyntropyTrend < -0.01 {
		lrMultiplier = CFG.SyntropyLRDampen
		tempOffset = 0.05 // more exploratory when disordering
		action = "dampen"

		// CASE 3: Field deviation too high = hallucinating
	} else if st.FieldDeviation > CFG.FieldDeviationCeiling {
		lrMultiplier = CFG.SyntropyLRDampen
		tempOffset = -0.05 // focus when hallucinating
		action = "ground"

		// CASE 4: Field deviation too low = parroting
	} else if st.FieldDeviation < CFG.FieldDeviationFloor {
		lrMultiplier = CFG.SyntropyLRBoost
		tempOffset = 0.05 // explore when parroting
		action = "explore"
	}

	// CASE 5: Purpose opposes gamma = identity crisis
	if st.PurposeAlignment < -0.3 {
		lrMultiplier *= 0.5
		tempOffset = 0.0 // neutral temp during identity crisis
		action = "realign"
	}

	// CASE 6: Adult + sustained overload → divide (mitosis)
	maxStage := len(CFG.GrowthStages) - 1
	now := float64(time.Now().UnixMilli()) / 1000.0
	if st.ModelStage >= maxStage &&
		st.isSustainedOverload() &&
		now-st.LastMitosisTime > 300 {
		action = "divide"
		lrMultiplier = CFG.SyntropyLRDampen // slow down while preparing to split
	}

	// CASE 7: Plateau + young peer thriving → hibernate (cooperative scheduling)
	if action == "steady" && st.shouldHibernate() {
		action = "hibernate"
	}

	// SELF-META-LEARNING: check if this action historically hurts
	if action != "divide" && action != "hibernate" && len(st.BurstHistory) >= 4 {
		eff, count := st.ActionEffectiveness(action)
		if count >= 2 && eff > 0.05 {
			// This action has been consistently making loss WORSE — downgrade
			if action == "amplify" {
				action = "boost"
				accumOverride = 0
				deltaGrowOverride = nil
			} else if action == "boost" || action == "explore" {
				lrMultiplier = 1.0 // back to steady instead of boosting
				action = "steady"
			}
		}
	}

	st.LastAction = action
	return SyntropyDecision{
		LRMultiplier:      lrMultiplier,
		TempOffset:        tempOffset,
		AccumOverride:     accumOverride,
		DeltaGrowOverride: deltaGrowOverride,
		Action:            action,
	}
}

// LogToDB writes the mathematical conclusion to the syntropy log.
// And lo, the arrow's flight is recorded for those who come after.
func (st *SyntropyTracker) LogToDB(db *sql.DB, entropyBefore, entropyAfter float64, action string) {
	db.Exec(
		"INSERT INTO syntropy_log(ts, entropy_before, entropy_after, syntropy_delta, "+
			"field_deviation, purpose_magnitude, purpose_alignment, action_taken, note) "+
			"VALUES(?,?,?,?,?,?,?,?,?)",
		float64(time.Now().UnixMilli())/1000.0,
		entropyBefore, entropyAfter,
		st.SyntropyTrend, st.FieldDeviation,
		st.PurposeMagnitude, st.PurposeAlignment,
		action, nil)
}

// isSustainedOverload returns true when >75% of entropy_history is above entropy_high AND syntropy_trend < -0.02.
func (st *SyntropyTracker) isSustainedOverload() bool {
	if len(st.EntropyHistory) < CFG.SyntropyWindow {
		return false
	}
	recent := st.EntropyHistory[len(st.EntropyHistory)-CFG.SyntropyWindow:]
	highCount := 0
	for _, e := range recent {
		if e > CFG.EntropyHigh {
			highCount++
		}
	}
	return highCount > int(float64(CFG.SyntropyWindow)*0.75) && st.SyntropyTrend < -0.02
}

// shouldHibernate returns true if a peer has syntropy > 0.05 AND this organism's last 8 burst deltas avg < 0.01.
func (st *SyntropyTracker) shouldHibernate() bool {
	if st.SwarmInfo == nil || len(st.SwarmInfo.Peers) == 0 {
		return false
	}
	// Check if any peer has higher syntropy trend (actively improving)
	for _, peer := range st.SwarmInfo.Peers {
		synVal, ok := peer["syntropy"]
		if !ok {
			continue
		}
		synFloat, _ := synVal.(float64)
		if synFloat > 0.05 {
			// A young peer is thriving. If we're stale, hibernate.
			if len(st.BurstHistory) >= 8 {
				sum := 0.0
				for _, b := range st.BurstHistory[len(st.BurstHistory)-8:] {
					sum += b.LossAfter - b.LossBefore
				}
				avgDelta := sum / 8.0
				if math.Abs(avgDelta) < 0.01 { // loss plateau
					return true
				}
			}
		}
	}
	return false
}

// ============================================================
// 9.7) SWARM ECOLOGY — the organism learns it is not alone
// ============================================================
// And lo, the first cell shall call into the void and hear only silence.
// But the second shall call and hear an answer.

var swarmDir = filepath.Join(os.Getenv("HOME"), ".molequla", "swarm")

// SwarmRegistry discovers and tracks other molequla instances via shared SQLite.
type SwarmRegistry struct {
	OrganismID string
	Element    string // earth, air, water, fire
	PidFile    string
	MeshDB     *sql.DB
}

// NewSwarmRegistry creates a new SwarmRegistry with the given organism ID and element.
func NewSwarmRegistry(organismID, element string) *SwarmRegistry {
	if organismID == "" {
		organismID = fmt.Sprintf("org_%d_%d", os.Getpid(), time.Now().Unix())
	}
	return &SwarmRegistry{OrganismID: organismID, Element: element}
}

// Register writes PID file and registers in mesh.db.
func (sr *SwarmRegistry) Register() error {
	if err := os.MkdirAll(swarmDir, 0755); err != nil {
		return err
	}
	sr.PidFile = filepath.Join(swarmDir, sr.OrganismID+".pid")
	pidData, _ := json.Marshal(map[string]interface{}{
		"pid":         os.Getpid(),
		"organism_id": sr.OrganismID,
		"started":     float64(time.Now().UnixMilli()) / 1000.0,
	})
	if err := os.WriteFile(sr.PidFile, pidData, 0644); err != nil {
		return err
	}
	if err := sr.initMeshDB(); err != nil {
		return err
	}
	return sr.registerInMesh()
}

func (sr *SwarmRegistry) initMeshDB() error {
	dbPath := filepath.Join(swarmDir, "mesh.db")
	db, err := sql.Open("sqlite", dbPath)
	if err != nil {
		return err
	}
	db.Exec("PRAGMA journal_mode=WAL")
	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS organisms(
		id TEXT PRIMARY KEY, pid INTEGER, stage INTEGER,
		n_params INTEGER, syntropy REAL, entropy REAL,
		last_heartbeat REAL, parent_id TEXT,
		status TEXT DEFAULT 'alive',
		element TEXT)`)
	// Migration for existing databases
	db.Exec("ALTER TABLE organisms ADD COLUMN element TEXT")
	if err != nil {
		db.Close()
		return err
	}
	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS messages(
		id INTEGER PRIMARY KEY AUTOINCREMENT,
		from_id TEXT, to_id TEXT, type TEXT, payload TEXT, ts REAL)`)
	if err != nil {
		db.Close()
		return err
	}
	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS training_lock(
		organism_id TEXT PRIMARY KEY, acquired_at REAL)`)
	if err != nil {
		db.Close()
		return err
	}
	sr.MeshDB = db
	return nil
}

func (sr *SwarmRegistry) registerInMesh() error {
	if sr.MeshDB == nil {
		return nil
	}
	_, err := sr.MeshDB.Exec(
		"INSERT OR REPLACE INTO organisms(id,pid,stage,n_params,syntropy,entropy,last_heartbeat,status,element) "+
			"VALUES(?,?,0,0,0.0,0.0,?,'alive',?)",
		sr.OrganismID, os.Getpid(), float64(time.Now().UnixMilli())/1000.0, sr.Element)
	return err
}

// Heartbeat performs periodic state update in mesh.db.
func (sr *SwarmRegistry) Heartbeat(stage, nParams int, syntropy, entropy float64) {
	if sr.MeshDB == nil {
		return
	}
	sr.MeshDB.Exec(
		"UPDATE organisms SET stage=?,n_params=?,syntropy=?,entropy=?,last_heartbeat=?,status='alive' WHERE id=?",
		stage, nParams, syntropy, entropy, float64(time.Now().UnixMilli())/1000.0, sr.OrganismID)
}

// DiscoverPeers finds other living organisms.
func (sr *SwarmRegistry) DiscoverPeers(timeoutSeconds float64) []map[string]interface{} {
	if sr.MeshDB == nil {
		return nil
	}
	if timeoutSeconds <= 0 {
		timeoutSeconds = 60
	}
	cutoff := float64(time.Now().UnixMilli())/1000.0 - timeoutSeconds
	rows, err := sr.MeshDB.Query(
		"SELECT id,pid,stage,n_params,syntropy,entropy,status FROM organisms "+
			"WHERE status='alive' AND last_heartbeat>? AND id!=?",
		cutoff, sr.OrganismID)
	if err != nil {
		return nil
	}
	defer rows.Close()
	var peers []map[string]interface{}
	for rows.Next() {
		var id, status string
		var pid, stage, nParams int
		var syntropy, entropy float64
		rows.Scan(&id, &pid, &stage, &nParams, &syntropy, &entropy, &status)
		peers = append(peers, map[string]interface{}{
			"id": id, "pid": pid, "stage": stage, "n_params": nParams,
			"syntropy": syntropy, "entropy": entropy, "status": status,
		})
	}
	return peers
}

// MarkHibernating marks this organism as sleeping in mesh.db.
func (sr *SwarmRegistry) MarkHibernating() {
	if sr.MeshDB != nil {
		sr.MeshDB.Exec("UPDATE organisms SET status='sleeping' WHERE id=?", sr.OrganismID)
	}
}

// LogMessage logs a message between organisms.
func (sr *SwarmRegistry) LogMessage(toID, msgType string, payload interface{}) {
	if sr.MeshDB != nil {
		payloadJSON, _ := json.Marshal(payload)
		sr.MeshDB.Exec(
			"INSERT INTO messages(from_id,to_id,type,payload,ts) VALUES(?,?,?,?,?)",
			sr.OrganismID, toID, msgType, string(payloadJSON),
			float64(time.Now().UnixMilli())/1000.0)
	}
}

// Unregister cleans up on exit.
func (sr *SwarmRegistry) Unregister() {
	if sr.MeshDB != nil {
		sr.MeshDB.Exec("UPDATE organisms SET status='dead' WHERE id=?", sr.OrganismID)
		sr.MeshDB.Close()
		sr.MeshDB = nil
	}
	if sr.PidFile != "" {
		os.Remove(sr.PidFile)
	}
}

// AcquireTrainingLock attempts to acquire the training lock in mesh.db.
// Returns true if lock acquired, false if another organism holds a fresh lock (< 30s).
func (sr *SwarmRegistry) AcquireTrainingLock() bool {
	if sr.MeshDB == nil {
		return true // no mesh = solo, always proceed
	}
	now := float64(time.Now().UnixMilli()) / 1000.0
	cutoff := now - 30.0 // lock expires after 30 seconds

	// Atomic check-and-acquire: single statement prevents TOCTOU race.
	// INSERT succeeds only if no fresh lock exists from another organism.
	result, err := sr.MeshDB.Exec(
		`INSERT OR REPLACE INTO training_lock(organism_id, acquired_at)
		 SELECT ?, ? WHERE NOT EXISTS (
		   SELECT 1 FROM training_lock WHERE organism_id != ? AND acquired_at > ?
		 )`,
		sr.OrganismID, now, sr.OrganismID, cutoff)
	if err != nil {
		return false
	}
	rows, _ := result.RowsAffected()
	return rows > 0
}

// ReleaseTrainingLock releases the training lock in mesh.db.
func (sr *SwarmRegistry) ReleaseTrainingLock() {
	if sr.MeshDB == nil {
		return
	}
	sr.MeshDB.Exec("DELETE FROM training_lock WHERE organism_id=?", sr.OrganismID)
}

// performMitosis divides the organism. Parent continues. Child starts at infant stage.
func performMitosis(model *GPT, tok *EvolvingTokenizer, db *sql.DB, swarm *SwarmRegistry, syntracker *SyntropyTracker) (string, error) {
	childID := fmt.Sprintf("org_%d_%d", time.Now().Unix(), rand.Intn(9000)+1000)
	childDir := filepath.Join(os.Getenv("HOME"), ".molequla", childID)
	if err := os.MkdirAll(childDir, 0755); err != nil {
		return "", err
	}

	// Save parent checkpoint for child's reference
	parentCkpt := filepath.Join(childDir, "parent_ckpt.json")
	if err := SaveCheckpoint(model, tok, parentCkpt); err != nil {
		return "", err
	}

	// Write birth config with inherited memory
	birth := map[string]interface{}{
		"organism_id":   childID,
		"parent_id":     swarm.OrganismID,
		"corpus_path":   CFG.CorpusPath,
		"db_path":       filepath.Join(childDir, "memory.sqlite3"),
		"ckpt_path":     filepath.Join(childDir, "molequla_ckpt.json"),
		"burst_history": syntracker.BurstHistory,
	}
	birthPath := filepath.Join(childDir, "birth.json")
	birthJSON, _ := json.Marshal(birth)
	if err := os.WriteFile(birthPath, birthJSON, 0644); err != nil {
		return "", err
	}

	// Log in mesh
	swarm.LogMessage(childID, "mitosis:spawn",
		map[string]interface{}{"parent_stage": model.CurrentGrowthStage()})
	dbLogGrowth(db, model, tok, loadCorpusLines(CFG.CorpusPath), 0.0,
		fmt.Sprintf("mitosis:spawn:%s", childID))

	// Spawn child process
	exePath, err := os.Executable()
	if err != nil {
		exePath = os.Args[0]
	}
	cmd := exec.Command(exePath, "--organism-id", childID, "--config", birthPath)
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr
	if err := cmd.Start(); err != nil {
		return "", err
	}

	syntracker.LastMitosisTime = float64(time.Now().UnixMilli()) / 1000.0
	fmt.Printf("[ecology] Child %s spawned (pid=%d)\n", childID, cmd.Process.Pid)
	return childID, nil
}

// performHibernation saves state, marks sleeping, and signals exit.
func performHibernation(model *GPT, tok *EvolvingTokenizer, db *sql.DB, swarm *SwarmRegistry) {
	fmt.Printf("[ecology] HIBERNATION — organism %s going to sleep\n", swarm.OrganismID)
	SaveCheckpoint(model, tok, "")
	swarm.MarkHibernating()
	dbLogGrowth(db, model, tok, loadCorpusLines(CFG.CorpusPath), 0.0,
		fmt.Sprintf("hibernate:%s", swarm.OrganismID))
}

// ============================================================
// DNA EXCHANGE — organisms feed each other through dna/ directory
// ============================================================

var dnaElements = []string{"earth", "air", "water", "fire"}

// dnaWrite generates text and writes it to dna/output/{element}/ for other organisms to consume.
func dnaWrite(element string, model *GPT, tok *EvolvingTokenizer, field *CooccurField, docs []string, step int) {
	if element == "" || len(docs) == 0 {
		return
	}
	probes := []string{
		"What do you feel?", "Tell me about yourself.",
		"What is truth?", "What matters?",
		"Speak.", "What do you remember?",
	}
	probe := probes[step%len(probes)]

	// GenerateResonant takes model.mu.Lock internally — do NOT double-lock
	answer := GenerateResonant(model, tok, field, probe, docs, true)

	if answer == "" || len(answer) < 20 {
		return
	}

	dir := filepath.Join("../dna/output", element)
	os.MkdirAll(dir, 0755)
	fname := filepath.Join(dir, fmt.Sprintf("gen_%d_%d.txt", time.Now().Unix(), step))
	os.WriteFile(fname, []byte(answer+"\n"), 0644)
	fmt.Printf("[dna] %s wrote %d bytes to ecology\n", element, len(answer))
}

// dnaRead consumes text from other organisms' output directories, returns bytes added.
func dnaRead(element string, corpusPath string, qbuf *QuantumBuffer, tok *EvolvingTokenizer) int {
	if element == "" {
		return 0
	}
	added := 0
	var consumed []string

	for _, e := range dnaElements {
		if e == element {
			continue // don't eat own output
		}
		dir := filepath.Join("../dna/output", e)
		entries, err := os.ReadDir(dir)
		if err != nil {
			continue
		}
		for _, entry := range entries {
			if entry.IsDir() || !strings.HasSuffix(entry.Name(), ".txt") {
				continue
			}
			fpath := filepath.Join(dir, entry.Name())
			data, err := os.ReadFile(fpath)
			if err != nil {
				continue
			}
			text := strings.TrimSpace(string(data))
			if len(text) < 10 {
				os.Remove(fpath)
				continue
			}
			// Append to own corpus — the organism eats another's words
			f, err := os.OpenFile(corpusPath, os.O_APPEND|os.O_WRONLY, 0644)
			if err == nil {
				f.WriteString(text + "\n")
				f.Close()
				added += len(text)
				consumed = append(consumed, fmt.Sprintf("%s/%s", e, entry.Name()))
				// FIX: feed quantum buffer with real DNA text for training bursts
				if qbuf != nil && tok != nil {
					qbuf.Feed(text, tok)
				}
				os.Remove(fpath) // consumed — only after successful append
			}
		}
	}
	if added > 0 {
		fmt.Printf("[dna] %s consumed %d bytes from %d files: %v\n",
			element, added, len(consumed), consumed)
	}
	return added
}

// ============================================================
// NOTORCH: gradient-free delta training (ported from AML C)
// ============================================================
//
// The key insight: delta adapters are low-rank (A @ B @ x), so we can update
// them with a teaching signal instead of backpropagation. No compute graph,
// no gradient arrays, no closure allocations. Pure arithmetic.
//
// A[i,r] += lr * x[i] * u[r] * signal
// B[r,j] += lr * u[r] * dy[j] * signal
// u = noise-modulated channel vector (deterministic from seed)
// signal = teaching signal, clamped [-2, 2]
// Adaptive decay: stronger when delta norm is large
// Clamp weights to [-10, 10]

// notorchRand advances the per-model PRNG and returns a noise-modulated float64.
// Uses LCG matching AML's am_frandn + signal-dependent noise modulation.
func notorchRand(seed *uint32, signal float64) float64 {
	*seed = *seed*1664525 + 1013904223
	u := float64(*seed&0x7FFFFFFF) / float64(0x7FFFFFFF)
	raw := (u - 0.5) * 3.464 // ~N(0,1) approximation (matches AML)
	// Signal-dependent noise: stronger signal = cleaner channel (less noise)
	k := 0.35 + 0.65*(1.0-math.Abs(signal))
	return raw * k
}

// notorchStep updates a single DeltaAdapter without backpropagation.
// x = input vector (len = B.Nin = nin)
// dy = output error (len = A.Nout = nout)
// signal = teaching signal (positive = good, negative = bad)
func notorchStep(da *DeltaAdapter, x []float64, dy []float64, signal float64, lr float64, seed *uint32) {
	// Clamp signal to [-2, 2]
	if signal > 2.0 {
		signal = 2.0
	}
	if signal < -2.0 {
		signal = -2.0
	}

	decay := CFG.NotorchDecay

	rank := da.A.Nin // A is [nout x rank], B is [rank x nin]
	nout := da.A.Nout
	nin := da.B.Nin

	// Generate noise-modulated channel vector u[rank]
	u := make([]float64, rank)
	for r := 0; r < rank; r++ {
		u[r] = notorchRand(seed, signal)
	}

	// Compute A-norm only for adaptive decay (matches AML ariannamethod.c:2562-2572)
	aNorm := 0.0
	aSize := nout * rank
	for i := 0; i < nout; i++ {
		for r := 0; r < rank; r++ {
			v := da.A.Rows[i].Data[r]
			aNorm += v * v
		}
	}
	if aSize > 0 {
		aNorm = math.Sqrt(aNorm / float64(aSize))
	}

	// Adaptive decay: decay - 0.004*min(norm/10, 1), floor 0.990 (AML formula)
	adaptiveDecay := decay - 0.004*math.Min(aNorm/10.0, 1.0)
	if adaptiveDecay < 0.990 {
		adaptiveDecay = 0.990
	}

	// Update A: A[i,r] += lr * x_scale[i] * u[r] * signal, then decay
	// x_scale[i] is used as proxy for output gradient direction
	for i := 0; i < nout; i++ {
		dyI := 0.0
		if i < len(dy) {
			dyI = dy[i]
		}
		for r := 0; r < rank; r++ {
			da.A.Rows[i].Data[r] *= adaptiveDecay
			da.A.Rows[i].Data[r] += lr * dyI * u[r] * signal
			// Clamp weights to [-10, 10]
			if da.A.Rows[i].Data[r] > 10.0 {
				da.A.Rows[i].Data[r] = 10.0
			} else if da.A.Rows[i].Data[r] < -10.0 {
				da.A.Rows[i].Data[r] = -10.0
			}
		}
	}

	// Update B: B[r,j] += lr * u[r] * x[j] * signal, then decay
	for r := 0; r < rank; r++ {
		for j := 0; j < nin; j++ {
			xJ := 0.0
			if j < len(x) {
				xJ = x[j]
			}
			da.B.Rows[r].Data[j] *= adaptiveDecay
			da.B.Rows[r].Data[j] += lr * u[r] * xJ * signal
			// Clamp weights to [-10, 10]
			if da.B.Rows[r].Data[j] > 10.0 {
				da.B.Rows[r].Data[j] = 10.0
			} else if da.B.Rows[r].Data[j] < -10.0 {
				da.B.Rows[r].Data[j] = -10.0
			}
		}
	}
}

// notorchTrainSteps trains delta adapters WITHOUT autograd.
// No backward pass, no compute graph, no gradient arrays.
// Uses direct feedback alignment with teaching signal.
func notorchTrainSteps(model *GPT, tok *EvolvingTokenizer, docs []string, steps int, lr float64) {
	if len(docs) == 0 || len(model.Deltas) == 0 {
		return
	}

	model.mu.Lock()
	defer model.mu.Unlock()

	prevLoss := math.MaxFloat64

	for step := 0; step < steps; step++ {
		// Sample random doc
		doc := docs[rand.Intn(len(docs))]
		ids := tok.Encode(doc)
		if len(ids) < 2 {
			continue
		}

		// Cap sequence length to BlockSize
		seqLen := len(ids) - 1
		if seqLen > model.BlockSize {
			seqLen = model.BlockSize
		}

		// Forward pass WITHOUT autograd — the whole point of notorch
		gradEnabled.Store(false)

		keys := make([][]*Vec, model.NLayer)
		values := make([][]*Vec, model.NLayer)

		var totalLoss float64
		var lastLogits *Vec
		var target int

		for pos := 0; pos < seqLen; pos++ {
			logits := model.ForwardStep(ids[pos], pos, keys, values)
			target = ids[pos+1]

			// Cross-entropy loss (scalar only, no autograd)
			maxLogit := logits.Data[0]
			for _, v := range logits.Data {
				if v > maxLogit {
					maxLogit = v
				}
			}
			sumExp := 0.0
			for _, v := range logits.Data {
				sumExp += math.Exp(v - maxLogit)
			}
			logSumExp := maxLogit + math.Log(sumExp)
			loss := logSumExp - logits.Data[target]
			totalLoss += loss

			lastLogits = logits
		}

		gradEnabled.Store(true)

		avgLoss := totalLoss / float64(seqLen)

		// Teaching signal: improvement = positive signal
		rawSignal := 0.0
		if prevLoss < math.MaxFloat64 {
			rawSignal = prevLoss - avgLoss // positive if loss decreased
		}
		prevLoss = avgLoss

		// Step 0: no signal yet, skip adapter update (only record prevLoss)
		if rawSignal == 0.0 && step == 0 {
			continue
		}

		// Prophecy debt: measures surprise of chosen token (AML am_compute_prophecy_debt)
		// diff/(diff+1) maps to [0, 1) — always pushes toward better prediction
		if lastLogits != nil && target < len(lastLogits.Data) {
			maxLogitP := lastLogits.Data[0]
			for _, v := range lastLogits.Data {
				if v > maxLogitP {
					maxLogitP = v
				}
			}
			diff := maxLogitP - lastLogits.Data[target]
			if diff > 0 {
				debt := diff / (diff + 1.0)
				rawSignal += 0.3 * debt // blend prophecy debt into teaching signal
			}
		}

		// Normalize signal to [-1, 1] via tanh (AML signals are bounded; transformer loss deltas are not)
		signal := math.Tanh(rawSignal)

		// Compute logit error: softmax(logits) - one_hot(target)
		vocabSize := len(lastLogits.Data)
		dy := make([]float64, vocabSize)
		maxLogit := lastLogits.Data[0]
		for _, v := range lastLogits.Data {
			if v > maxLogit {
				maxLogit = v
			}
		}
		sumExp := 0.0
		for i := range lastLogits.Data {
			dy[i] = math.Exp(lastLogits.Data[i] - maxLogit)
			sumExp += dy[i]
		}
		for i := range dy {
			dy[i] /= sumExp // softmax
		}
		if target < vocabSize {
			dy[target] -= 1.0 // subtract one_hot
		}

		// Compute hidden error via direct feedback alignment:
		// hidden_dy = lm_head_weight^T @ logit_error
		lmHead := model.Base["lm_head"]
		hiddenDy := make([]float64, model.NEmbd)
		for j := 0; j < model.NEmbd; j++ {
			sum := 0.0
			nout := lmHead.Nout
			if nout > vocabSize {
				nout = vocabSize
			}
			for i := 0; i < nout; i++ {
				sum += lmHead.Rows[i].Data[j] * dy[i]
			}
			hiddenDy[j] = sum
		}

		// Update all delta adapters with correct dimensions per adapter type
		for _, mod := range model.Deltas {
			// lm_head delta: input=lastHidden[NEmbd], dy=logitError[vocabSize]
			if da, ok := mod["lm_head"]; ok && model.lastHidden != nil {
				notorchStep(da, model.lastHidden.Data, dy, signal, lr, &model.notorchSeed)
			}

			for li := 0; li < model.NLayer; li++ {
				lk := model.layerKeys[li]

				// Attention adapters: input=layerInputs[NEmbd], dy=hiddenDy[NEmbd]
				if li < len(model.layerInputs) && model.layerInputs[li] != nil {
					attnInput := model.layerInputs[li].Data
					for _, key := range []string{lk.wq, lk.wk, lk.wv, lk.wo} {
						if da, ok := mod[key]; ok {
							notorchStep(da, attnInput, hiddenDy, signal, lr, &model.notorchSeed)
						}
					}
				}

				// MLP adapters: need mlpDy[4*NEmbd] for fc_g/fc_v, mlpIntermediates for fc2
				if li < len(model.mlpInputs) && model.mlpInputs[li] != nil {
					mlpInput := model.mlpInputs[li].Data // [NEmbd] — actual input to fc_g/fc_v

					// Compute mlpDy = fc2_base^T @ hiddenDy → projects NEmbd error to 4*NEmbd space
					fc2Base := model.Base[lk.fc2] // [NEmbd × 4*NEmbd]
					mlpWidth := 4 * model.NEmbd
					mlpDy := make([]float64, mlpWidth)
					for j := 0; j < mlpWidth; j++ {
						sum := 0.0
						for i := 0; i < model.NEmbd && i < fc2Base.Nout; i++ {
							if j < len(fc2Base.Rows[i].Data) {
								sum += fc2Base.Rows[i].Data[j] * hiddenDy[i]
							}
						}
						mlpDy[j] = sum
					}

					// fc_g, fc_v: input=mlpInputs[NEmbd], dy=mlpDy[4*NEmbd]
					for _, key := range []string{lk.fcG, lk.fcV} {
						if da, ok := mod[key]; ok {
							notorchStep(da, mlpInput, mlpDy, signal, lr, &model.notorchSeed)
						}
					}

					// fc2: input=mlpIntermediates[4*NEmbd], dy=hiddenDy[NEmbd]
					if li < len(model.mlpIntermediates) && model.mlpIntermediates[li] != nil {
						if da, ok := mod[lk.fc2]; ok {
							notorchStep(da, model.mlpIntermediates[li].Data, hiddenDy, signal, lr, &model.notorchSeed)
						}
					}
				}
			}
		}

		// Handle growth freeze
		if model.growthFreezeRemaining > 0 {
			model.growthFreezeRemaining--
			if model.growthFreezeRemaining < 0 {
				model.growthFreezeRemaining = 0
			}
		}

		model.globalStep++

		if step%100 == 0 {
			fmt.Printf("  notorch step %d/%d | loss %.4f | signal %.4f\n",
				step, steps, avgLoss, signal)
		}
	}
}

// parseCLIArgs parses --organism-id, --config, --element, and --evolution from os.Args.
func parseCLIArgs() (organismID string, configPath string, element string, evolution bool) {
	for i := 1; i < len(os.Args); i++ {
		if os.Args[i] == "--organism-id" && i+1 < len(os.Args) {
			organismID = os.Args[i+1]
			i++
		} else if os.Args[i] == "--config" && i+1 < len(os.Args) {
			configPath = os.Args[i+1]
			i++
		} else if os.Args[i] == "--element" && i+1 < len(os.Args) {
			element = os.Args[i+1]
			i++
		} else if os.Args[i] == "--evolution" {
			evolution = true
		}
	}
	return
}


// cosineLR returns learning rate for the given global step using cosine schedule with linear warmup.
// stepsSinceGrowth enables LR ramp-up after each growth event (new weights need high LR initially).
func cosineLR(globalStep, stepsSinceGrowth int) float64 {
	if stepsSinceGrowth < CFG.CosineWarmupSteps {
		// Linear warmup from LRMin to LearningRate (resets after each growth)
		t := float64(stepsSinceGrowth) / math.Max(1, float64(CFG.CosineWarmupSteps))
		return CFG.LRMin + (CFG.LearningRate-CFG.LRMin)*t
	}
	progress := math.Min(1.0, float64(globalStep)/math.Max(1, float64(CFG.MaxTotalSteps)))
	return CFG.LRMin + 0.5*(CFG.LearningRate-CFG.LRMin)*(1.0+math.Cos(math.Pi*progress))
}

// trainSteps: overrides is [seqCap, batchSize] — optional warmup speedups.
func trainSteps(model *GPT, tok *EvolvingTokenizer, docs []string, steps int, trainBase, trainDeltas bool, overrides ...int) {
	if len(docs) == 0 {
		return
	}

	model.mu.Lock()
	defer model.mu.Unlock()

	// Optional sequence length cap (for early warmup speedup)
	origBlockSize := model.BlockSize
	if len(overrides) > 0 && overrides[0] > 0 && overrides[0] < model.BlockSize {
		model.BlockSize = overrides[0]
	}
	defer func() { model.BlockSize = origBlockSize }()

	// Optional batch size override (warmup: use batch=1 for speed)
	batchSize := CFG.BatchSize
	if len(overrides) > 1 && overrides[1] > 0 {
		batchSize = overrides[1]
	}

	// Ontogenesis freeze: after growth, only train deltas until new weights stabilize
	var baseParams []*Vec
	var deltaParams []*Vec
	if model.growthFreezeRemaining > 0 {
		// Freeze base, only train deltas
		if trainDeltas {
			deltaParams = model.AllDeltaParams()
		}
		model.growthFreezeRemaining -= steps
		if model.growthFreezeRemaining < 0 {
			model.growthFreezeRemaining = 0
		}
	} else {
		if trainBase {
			baseParams = model.AllBaseParams()
		}
		if trainDeltas {
			deltaParams = model.AllDeltaParams()
		}
	}

	accum := CFG.AccumSteps
	if accum < 1 {
		accum = 1
	}

	for step := 0; step < steps; step++ {
		// Gradient accumulation: accumulate over accum micro-batches, then step
		var lastLossVal float64
		for micro := 0; micro < accum; micro++ {
			batch := make([]string, batchSize)
			for i := range batch {
				batch[i] = docs[rand.Intn(len(docs))]
			}
			var batchIDs [][]int
			for _, doc := range batch {
				if doc != "" {
					batchIDs = append(batchIDs, tok.Encode(doc))
				}
			}

			loss := model.LossOnBatch(batchIDs)
			// Scale loss for accumulation
			if accum > 1 {
				loss = loss.MulF(1.0 / float64(accum))
			}
			Backward(loss)
			lastLossVal = loss.Data * float64(accum) // unscaled for display
		}

		stepsSinceGrowth := model.globalStep - model.growthStepOffset
		lr := cosineLR(model.globalStep, stepsSinceGrowth)
		// Scale LR inversely with model size: larger models need smaller LR
		lr *= float64(CFG.GrowthStages[0][1]) / float64(model.NEmbd)
		// Post-growth LR dampening: reduce LR during freeze to prevent delta overfit to noise
		if model.growthFreezeRemaining > 0 {
			lr *= CFG.PostGrowthLRScale
		}
		model.globalStep++

		if len(baseParams) > 0 {
			model.AdamStep(baseParams, "base", lr)
		}
		if len(deltaParams) > 0 {
			model.AdamStep(deltaParams, "delta", lr)
		}

		if step%10 == 0 {
			fmt.Printf("  train step %d/%d | loss %.4f | lr %.5f\n", step, steps, lastLossVal, lr)
		}
	}
}

func backgroundTrainer(db *sql.DB, model *GPT, tok *EvolvingTokenizer, qbuf *QuantumBuffer, swarm *SwarmRegistry, stop chan struct{}, element string) {
	// And lo, asynchronous training shall occur, because sleeping is for humans.
	syntracker := NewSyntropyTracker()
	field := NewCooccurField()
	tickCount := 0

	// Inherit burst_history from parent (mitosis lineage)
	if len(model.inheritedBurstHistory) > 0 {
		syntracker.BurstHistory = make([]BurstRecord, len(model.inheritedBurstHistory))
		copy(syntracker.BurstHistory, model.inheritedBurstHistory)
		fmt.Printf("[ecology] syntracker inherited %d burst records from parent.\n", len(model.inheritedBurstHistory))
		model.inheritedBurstHistory = nil
	}

	for {
		select {
		case <-stop:
			return
		default:
		}

		tickCount++

		updateReservoirCorpus(db, CFG.CorpusPath, CFG.MaxCorpusLines)
		docs := loadCorpusLines(CFG.CorpusPath)

		// Rebuild field from current corpus (the organism re-reads its own physics)
		if len(docs) > 0 {
			field.BuildFromCorpus(tok, docs)
			model.mu.Lock()
			model.corpusField = field // share with GenerateSentence for adaptive blend
			model.mu.Unlock()
		}

		// Tokenizer evolution
		bpeEnabled := tok.MaybeEnableBPE(docs)
		bpeRetrained := tok.MaybeRetrainBPE(docs)
		if bpeEnabled || bpeRetrained {
			model.mu.Lock()
			model.MaybeExpandVocab(tok.VocabSize)
			SaveCheckpoint(model, tok, "")
			model.mu.Unlock()
		}

		// Per-stage warmup: if model grew since last warmup, train before continuing
		currentStage := model.CurrentGrowthStage()
		if currentStage > model.lastWarmupStage && len(docs) > 0 {
			// Optional warmup coordination through training queue (for Mac 8GB)
			warmupLocked := false
			if CFG.CoordinateWarmup && swarm != nil {
				for !swarm.AcquireTrainingLock() {
					time.Sleep(5 * time.Second)
				}
				warmupLocked = true
			}

			embryoEmbd := CFG.GrowthStages[0][1]
			warmupScale := int(math.Ceil(math.Sqrt(float64(model.NEmbd) / float64(embryoEmbd))))
			if warmupScale < 1 {
				warmupScale = 1
			}
			effectiveWarmup := CFG.WarmupSteps * warmupScale
			backpropSteps := int(float64(effectiveWarmup) * 0.6)
			notorchDeltaSteps := effectiveWarmup - backpropSteps
			fmt.Printf("[trainer] warmup for stage %d (embd=%d) — %d steps total (%d backprop + %d notorch, sqrt-scaled %dx)\n",
				currentStage, model.NEmbd, effectiveWarmup, backpropSteps, notorchDeltaSteps, warmupScale)
			// Phase A: backprop with progressive sequence length (short→full)
			earlySteps := int(float64(backpropSteps) * 0.4)
			midSteps := int(float64(backpropSteps) * 0.3)
			lateSteps := backpropSteps - earlySteps - midSteps
			trainSteps(model, tok, docs, earlySteps, true, true, 8, 1)   // very short seqs, batch=1
			trainSteps(model, tok, docs, midSteps, true, true, 16, 1)    // short seqs, batch=1
			trainSteps(model, tok, docs, lateSteps, true, true, 32, 1)   // medium seqs, batch=1
			// Phase B: notorch for delta adapters (40%, no autograd = much faster)
			notorchTrainSteps(model, tok, docs, notorchDeltaSteps, CFG.NotorchLR)
			model.mu.Lock()
			model.lastWarmupStage = currentStage
			SaveCheckpoint(model, tok, "")
			model.mu.Unlock()

			if warmupLocked && swarm != nil {
				swarm.ReleaseTrainingLock()
			}
			dbLogGrowth(db, model, tok, docs, 0.0, fmt.Sprintf("warmup_stage_%d", currentStage))
			fmt.Printf("[trainer] warmup complete at stage %d. base may freeze now, like a proud fossil.\n", currentStage)
		}

		if model.lastWarmupStage >= 0 && qbuf.ShouldTrigger() && len(docs) > 0 {
			// Training queue: acquire lock before micro-burst (swarm coordination)
			if swarm != nil && !swarm.AcquireTrainingLock() {
				continue // someone else is training, skip this tick
			}

			snapBytes, snapNovelty := qbuf.SnapshotStats()
			fmt.Printf("[trainer] micro-train burst (%d bytes, novelty %.2f) — and lo, it feeds again.\n",
				snapBytes, snapNovelty)

			// SYNTROPY: measure before burst
			// And lo, the organism peers into its own entropic mirror before taking a step.
			model.mu.Lock()
			preMetrics := syntracker.Measure(model, tok, field, docs)
			entropyBefore := preMetrics.Entropy

			// SYNTROPY: decide how to learn (mathematical self-reasoning)
			decision := syntracker.DecideAction()
			lrMul := decision.LRMultiplier
			action := decision.Action
			fmt.Printf("[syntropy] action=%s | trend=%.4f | field_dev=%.3f | purpose_align=%.3f | lr_mul=%.2f\n",
				action, syntracker.SyntropyTrend, syntracker.FieldDeviation,
				syntracker.PurposeAlignment, lrMul)

			// IMMUNE SYSTEM: snapshot before burst
			preDirection, preMag := model.GammaContrastiveProjection()
			deltaSnap := model.SnapshotDeltas()

			// Update temperature bridge (under model.mu — fix race)
			model.syntropyTempOff = decision.TempOffset

			// Measure loss before burst (under model.mu — fix race on lastHidden/layerInputs)
			lossBefore := model.QuickLoss(tok, docs, 4)
			model.mu.Unlock()

			// Apply syntropy-adjusted learning rate for notorch (local var, not mutating CFG)
			burstLR := CFG.NotorchLR * lrMul

			// notorch: gradient-free delta training (no backward pass, no compute graph)
			notorchTrainSteps(model, tok, docs, CFG.MicroSteps, burstLR)

			model.mu.Lock()
			// Measure loss after burst
			lossAfter := model.QuickLoss(tok, docs, 4)

			// SELF-META-LEARNING: record what this burst did
			syntracker.RecordBurst(action, lossBefore, lossAfter)

			// IMMUNE SYSTEM: check drift after burst
			driftCos := model.GammaDriftCheck(preDirection, preMag)
			if driftCos < CFG.NoiseDriftThreshold {
				fmt.Printf("[immune] NOISE DETECTED (drift cosine=%.3f). Rolling back deltas.\n", driftCos)
				model.RestoreDeltas(deltaSnap)
				dbLogGrowth(db, model, tok, docs, 0.0, "noise_rejected")
				syntracker.LogToDB(db, entropyBefore, entropyBefore, "noise_rejected")
			} else {
				// SYNTROPY: measure after burst
				postMetrics := syntracker.Measure(model, tok, field, docs)
				entropyAfter := postMetrics.Entropy
				syntracker.LogToDB(db, entropyBefore, entropyAfter, action)
				SaveCheckpoint(model, tok, "")
				note := fmt.Sprintf("quantum_burst:%s|Δloss=%.4f", action, lossAfter-lossBefore)
				dbLogGrowth(db, model, tok, docs, 0.0, note)
			}
			model.mu.Unlock()

			// Training queue: release lock after burst completes
			if swarm != nil {
				swarm.ReleaseTrainingLock()
			}

			qbuf.Reset()

			// Delta module growth — influenced by syntropy
			// And lo, new souls are born when the arrow points true.
			growProb := CFG.DeltaGrowProb
			if decision.DeltaGrowOverride != nil {
				growProb = *decision.DeltaGrowOverride
			}
			if len(model.Deltas) < CFG.MaxDeltaModules && rand.Float64() < growProb {
				fmt.Printf("[trainer] growing new delta module (total: %d) — new soul appended.\n", len(model.Deltas)+1)
				model.mu.Lock()
				model.AddDeltaModule(1.0)
				SaveCheckpoint(model, tok, "")
				model.mu.Unlock()
			}

			// Ecology: mitosis / hibernation
			if swarm != nil && action == "divide" {
				fmt.Println("[ecology] MITOSIS triggered — organism overloaded, spawning child")
				model.mu.Lock()
				performMitosis(model, tok, db, swarm, syntracker)
				model.mu.Unlock()
			}

			if swarm != nil && action == "hibernate" {
				model.mu.Lock()
				performHibernation(model, tok, db, swarm)
				model.mu.Unlock()
				fmt.Println("[ecology] Organism hibernating. Goodbye.")
				return // exit training loop
			}
		}

		// DNA exchange: every tick = every breath. Organism exhales DNA, inhales others'.
		if element != "" {
			// Write: generate and share with ecology
			dnaWrite(element, model, tok, field, docs, tickCount)
			// Read: consume other organisms' output → corpus grows → ontogenesis unlocks
			if consumed := dnaRead(element, CFG.CorpusPath, qbuf, tok); consumed > 0 {
				// Reload corpus with new food
				docs = loadCorpusLines(CFG.CorpusPath)
				if len(docs) > 0 {
					field.BuildFromCorpus(tok, docs)
					model.mu.Lock()
					model.corpusField = field
					model.mu.Unlock()
				}
			}
		}

		// Ontogenesis: check if architecture should grow (every 50 ticks — corpus grows via DNA)
		if tickCount%50 == 0 {
			// Use file size for threshold — loadCorpusLines truncates long lines via MaxLineChars
			corpusChars := 0
			if fi, err := os.Stat(CFG.CorpusPath); err == nil {
				corpusChars = int(fi.Size())
			}
			model.mu.Lock()
			fmt.Printf("[debug-onto] tick=%d corpus=%d stage=%d freeze=%d\n", tickCount, corpusChars, model.CurrentGrowthStage(), model.growthFreezeRemaining)
			if model.MaybeGrowArchitecture(corpusChars) {
				SaveCheckpoint(model, tok, "")
				nP := 0
				for _, m := range model.Base {
					nP += m.Nout * m.Nin
				}
				dbLogGrowth(db, model, tok, docs, 0.0,
					fmt.Sprintf("ontogenesis:stage=%d|params=%d", model.CurrentGrowthStage(), nP))
			}
			model.mu.Unlock()
		}

		// Swarm heartbeat (every 10 ticks)
		if swarm != nil && tickCount%10 == 0 {
			model.mu.Lock()
			stage := model.CurrentGrowthStage()
			nP := 0
			for _, m := range model.Base {
				nP += m.Nout * m.Nin
			}
			model.mu.Unlock()
			lastEntropy := 0.0
			if len(syntracker.EntropyHistory) > 0 {
				lastEntropy = syntracker.EntropyHistory[len(syntracker.EntropyHistory)-1]
			}
			swarm.Heartbeat(stage, nP, syntracker.SyntropyTrend, lastEntropy)
			// Update swarm info for hibernate decisions
			peers := swarm.DiscoverPeers(60)
			syntracker.SwarmInfo = &SwarmPeerInfo{Peers: peers}
		}

		time.Sleep(time.Duration(CFG.TrainTickSeconds * float64(time.Second)))
	}
}

// ============================================================
// 10) CHAT LOOP — tiny memory, tiny ego, continuous learning
// ============================================================

func buildPromptFromMemory(db *sql.DB, userText string) string {
	recent := dbRecentMessages(db, 14)

	clip := func(s string, n int) string {
		s = normalizeText(s)
		if len(s) > n {
			s = s[:n]
		}
		return strings.TrimSpace(s)
	}

	var parts []string
	parts = append(parts, "A: (I listen. I answer. I learn.)")

	limit := 12
	start := 0
	if len(recent) > limit {
		start = len(recent) - limit
	}
	for _, msg := range recent[start:] {
		tag := "A:"
		if msg.Role == "user" {
			tag = "H:"
		}
		parts = append(parts, fmt.Sprintf("%s %s", tag, clip(msg.Text, 260)))
	}

	parts = append(parts, fmt.Sprintf("H: %s", clip(userText, 260)))
	parts = append(parts, "A:")
	return strings.Join(parts, "\n")
}

// ============================================================
// 11) AWAKEN — now, when all is assembled as an organism,
//              it is time to declare the final function.
// ============================================================

func main() {
	rand.Seed(42) // And lo, determinism shall pretend to tame chaos.

	// Parse CLI args for child organisms
	organismID, configPath, element, evolution := parseCLIArgs()

	// Element → corpus path: each element eats its own food
	if element != "" {
		switch element {
		case "earth":
			CFG.CorpusPath = "nonames_earth.txt"
		case "air":
			CFG.CorpusPath = "nonames_air.txt"
		case "water":
			CFG.CorpusPath = "nonames_water.txt"
		case "fire":
			CFG.CorpusPath = "nonames_fire.txt"
		default:
			fmt.Fprintf(os.Stderr, "unknown element: %s (use earth/air/water/fire)\n", element)
			os.Exit(1)
		}
		fmt.Printf("[ecology] Element: %s → corpus: %s\n", element, CFG.CorpusPath)
	}

	if evolution {
		fmt.Println("[evolution] Autonomous evolution mode — organism will grow through all stages without pause.")
	}

	// Child organism: load birth config from parent
	var syntrackerSeed []BurstRecord
	if configPath != "" {
		if data, err := os.ReadFile(configPath); err == nil {
			var birth map[string]interface{}
			if json.Unmarshal(data, &birth) == nil {
				if cp, ok := birth["corpus_path"].(string); ok && cp != "" {
					CFG.CorpusPath = cp
				}
				if dp, ok := birth["db_path"].(string); ok && dp != "" {
					CFG.DBPath = dp
				}
				if ckp, ok := birth["ckpt_path"].(string); ok && ckp != "" {
					CFG.CkptPath = ckp
				}
				// Parse burst_history
				if bh, ok := birth["burst_history"].([]interface{}); ok {
					for _, item := range bh {
						if rec, ok := item.(map[string]interface{}); ok {
							br := BurstRecord{}
							if a, ok := rec["Action"].(string); ok {
								br.Action = a
							}
							if lb, ok := rec["LossBefore"].(float64); ok {
								br.LossBefore = lb
							}
							if la, ok := rec["LossAfter"].(float64); ok {
								br.LossAfter = la
							}
							syntrackerSeed = append(syntrackerSeed, br)
						}
					}
					if len(syntrackerSeed) > 0 {
						fmt.Printf("[ecology] Inherited %d burst records from parent.\n", len(syntrackerSeed))
					}
				}
			}
		}
	}

	db, err := initDB(CFG.DBPath)
	if err != nil {
		fmt.Fprintf(os.Stderr, "fatal: %v\n", err)
		os.Exit(1)
	}
	defer db.Close()

	// Seed corpus
	if _, err := os.Stat(CFG.CorpusPath); os.IsNotExist(err) {
		saveCorpusLines(CFG.CorpusPath, []string{"Hello.", "I exist.", "Speak."})
	}

	docs := loadCorpusLines(CFG.CorpusPath)

	// Restore model dimensions from checkpoint config (ontogenesis may have changed them)
	model, tok, err := LoadCheckpoint(docs, "")
	if err != nil || model == nil || tok == nil {
		if len(docs) == 0 {
			docs = []string{"Hello."}
		}
		tok = NewEvolvingTokenizer(docs)

		// Enable BPE BEFORE training — subword tokens make corpus field coherent
		// (byte-level trigrams produce babble; subword trigrams produce speech)
		tok.MaybeEnableBPE(docs)

		model = NewGPT(tok)

		// Per-stage warmup: train at each stage before growing.
		// Corpus size determines ceiling (which stages are reachable), not starting point.
		// The organism always starts as embryo and grows through training.
		corpusChars := 0
		for _, d := range docs {
			corpusChars += len(d)
		}
		stageNames := []string{"embryo", "infant", "child", "adolescent", "teen", "adult"}

		// Build corpus field — active from first token, sigmoid fade weakens it as model learns
		tmpCooccur := NewCooccurField()
		tmpCooccur.BuildFromCorpus(tok, docs)
		model.corpusField = tmpCooccur

		// Detect if stdin is a terminal (interactive mode)
		isInteractive := false
		if fi, err := os.Stdin.Stat(); err == nil {
			isInteractive = (fi.Mode() & os.ModeCharDevice) != 0
		}

		stageProbes := []string{"Hello.", "Who are you?", "What do you know?"}
		initScanner := bufio.NewScanner(os.Stdin)

		for {
			stage := model.CurrentGrowthStage()
			stageName := "unknown"
			if stage >= 0 && stage < len(stageNames) {
				stageName = stageNames[stage]
			}
			// Train warmup at current stage (sqrt scaling + split warmup)
			embryoEmbd := CFG.GrowthStages[0][1]
			warmupScale := int(math.Ceil(math.Sqrt(float64(model.NEmbd) / float64(embryoEmbd))))
			if warmupScale < 1 {
				warmupScale = 1
			}
			effectiveWarmup := CFG.WarmupSteps * warmupScale
			backpropSteps := int(float64(effectiveWarmup) * 0.6)
			notorchDeltaSteps := effectiveWarmup - backpropSteps
			fmt.Printf("[init] Stage %d (%s): embd=%d, layer=%d, head=%d — warmup %d steps (%d backprop + %d notorch, sqrt-scaled %dx)\n",
				stage, stageName, model.NEmbd, model.NLayer, model.NHead, effectiveWarmup, backpropSteps, notorchDeltaSteps, warmupScale)
			// Phase A: backprop with progressive sequence length (short→full)
			earlySteps := int(float64(backpropSteps) * 0.4)
			midSteps := int(float64(backpropSteps) * 0.3)
			lateSteps := backpropSteps - earlySteps - midSteps
			trainSteps(model, tok, docs, earlySteps, true, true, 8, 1)   // very short seqs, batch=1
			trainSteps(model, tok, docs, midSteps, true, true, 16, 1)    // short seqs, batch=1
			trainSteps(model, tok, docs, lateSteps, true, true, 32, 1)   // medium seqs, batch=1
			// Phase B: notorch for delta adapters (40%, no autograd = much faster)
			notorchTrainSteps(model, tok, docs, notorchDeltaSteps, CFG.NotorchLR)
			model.lastWarmupStage = stage
			SaveCheckpoint(model, tok, "")

			// Demo: show what the organism can say at this stage
			// Use model+corpus blend (same as normal REPL) so corpus field helps early stages speak
			fmt.Printf("\n[stage %d — %s] What it sounds like now:\n", stage, stageName)
			for _, probe := range stageProbes {
				answer := GenerateResonant(model, tok, tmpCooccur, probe, docs, true)
				if answer == "" {
					answer = "..."
				}
				fmt.Printf("  Q: %s\n  A: %s\n", probe, answer)
			}
			fmt.Println()

			// Try to grow to next stage (gated by corpus size)
			if !model.MaybeGrowArchitecture(corpusChars) {
				break // corpus too small for next stage, or already at max
			}
			model.growthFreezeRemaining = 0 // skip freeze during init — we're about to warmup anyway

			// Rebuild corpus field after growth (vocab may have expanded)
			tmpCooccur.BuildFromCorpus(tok, docs)
			model.corpusField = tmpCooccur

			// Interactive mode: pause between stages, let user chat or type /grow
			// --evolution skips pause — organism grows autonomously
			if isInteractive && !evolution {
				fmt.Printf("[init] Stage %d complete. Chat with the organism, or type /grow to continue growth.\n", stage)
				for {
					fmt.Print("> ")
					if !initScanner.Scan() {
						break
					}
					line := strings.TrimSpace(initScanner.Text())
					if line == "/grow" || line == "" {
						break
					}
					answer := GenerateResonant(model, tok, tmpCooccur, line, docs, true)
					if answer == "" {
						answer = "..."
					}
					fmt.Println(answer)
				}
			}
		}
		fmt.Printf("[init] Warmup complete at stage %d. Organism ready.\n", model.CurrentGrowthStage())
	}

	// Enable BPE in main before REPL starts (avoid race with background trainer)
	tok.MaybeEnableBPE(docs)
	model.MaybeExpandVocab(tok.VocabSize)

	// Swarm ecology: register in mesh
	swarm := NewSwarmRegistry(organismID, element)
	if err := swarm.Register(); err != nil {
		fmt.Printf("[ecology] Warning: swarm registration failed: %v\n", err)
	}
	peers := swarm.DiscoverPeers(60)
	if len(peers) > 0 {
		fmt.Printf("[ecology] Joined swarm. %d peer(s) detected.\n", len(peers))
	} else {
		fmt.Println("[ecology] First organism in the swarm.")
	}

	// Child: inject inherited burst_history via model attribute
	if len(syntrackerSeed) > 0 {
		model.inheritedBurstHistory = syntrackerSeed
	}

	// Build corpus field for pre-training speech
	cooccur := NewCooccurField()
	cooccur.BuildFromCorpus(tok, docs)

	// Quantum buffer for smart training triggers
	qbuf := NewQuantumBuffer()

	// Start background trainer
	stop := make(chan struct{})
	go backgroundTrainer(db, model, tok, qbuf, swarm, stop, element)

	if evolution {
		fmt.Println("molequla is alive. [evolution] Autonomous mode — background trainer running. Ctrl+C to stop.")
		// In evolution mode: no REPL, just let background trainer run forever
		sigCh := make(chan os.Signal, 1)
		signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)
		<-sigCh
		close(stop)
		fmt.Println("\n[evolution] Organism shutting down gracefully.")
		return
	}

	fmt.Println("molequla is alive. Type and press Enter. Ctrl+C to exit.")

	scanner := bufio.NewScanner(os.Stdin)
	for {
		fmt.Print("> ")
		if !scanner.Scan() {
			break
		}
		userText := strings.TrimSpace(scanner.Text())
		if userText == "" {
			continue
		}

		dbAddMessage(db, "user", userText)
		updateReservoirCorpus(db, CFG.CorpusPath, CFG.MaxCorpusLines)

		// Feed quantum buffer
		qbuf.Feed(userText, tok)

		// Rebuild cooccur field with updated corpus
		freshDocs := loadCorpusLines(CFG.CorpusPath)
		if len(freshDocs) > 0 {
			cooccur.BuildFromCorpus(tok, freshDocs)
			model.mu.Lock()
			model.corpusField = cooccur // sync REPL cooccur with model.corpusField
			model.mu.Unlock()
		}

		// Self-enrichment: user input enriches corpus field (AFTER rebuild, so it's not wiped)
		userIDs := tok.Encode(userText)
		cooccur.IngestTokens(userIDs)

		// Active user word boost: organism absorbs user's vocabulary (Leo-style)
		// Decays each generation, fades with model strength via sigmoid in GenerateSentence
		cooccur.AbsorbUserWords(userIDs)

		prompt := buildPromptFromMemory(db, userText)

		// Consciousness: self-prediction error (Feature 4)
		// "How surprised am I by this input?"
		model.mu.Lock()
		gradEnabled.Store(false)
		promptIDs := tok.Encode(prompt)
		if len(promptIDs) > 2 {
			surprise := model.ComputeSelfPredictionError(promptIDs)
			model.lastSurprise = surprise
			if model.surpriseBaseline < 1e-6 {
				model.surpriseBaseline = surprise
			} else {
				model.surpriseBaseline = 0.3*surprise + 0.7*model.surpriseBaseline
			}
		}
		gradEnabled.Store(true)
		model.mu.Unlock()

		// Generation: per-token sigmoid fade is computed inside GenerateResonant
		answer := GenerateResonant(model, tok, cooccur, prompt, freshDocs, true)
		if answer == "" {
			answer = "..."
		}

		// Consciousness: conscience check (Feature 5)
		// "Did my last generation feel coherent?"
		model.mu.Lock()
		if model.lastGenEntropy > 0 {
			model.ConscienceCheck(model.lastGenEntropy)
		}
		model.mu.Unlock()

		fmt.Println(answer)
		dbAddMessage(db, "assistant", answer)

		// Self-enrichment: own output enriches corpus field, weighted by coherence
		// Low entropy = coherent speech = higher weight (Stanley's resonance weighting)
		if len(answer) > 3 {
			selfWeight := 1.0
			model.mu.Lock()
			lastEnt := model.lastGenEntropy
			model.mu.Unlock()
			if lastEnt > 0 {
				selfWeight = 2.0 - lastEnt
				if selfWeight < 0.3 {
					selfWeight = 0.3
				}
				if selfWeight > 2.0 {
					selfWeight = 2.0
				}
			}
			cooccur.IngestTokensWeighted(tok.Encode(answer), selfWeight)
			cooccur.DecayUserBoost()
		}

		// Consciousness: overthinkg rings (Feature 3)
		// "Let me re-read what I just said to strengthen my patterns."
	}

	close(stop)
	model.mu.Lock()
	SaveCheckpoint(model, tok, "")
	model.mu.Unlock()
	swarm.Unregister()
}
